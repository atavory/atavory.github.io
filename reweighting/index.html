<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-US">
  <head>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 1st November 2003), see www.w3.org" />
    <title>
      Dynamically Reweighing Experts' Predictions
    </title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="copyright" content="Copyright &amp;#169; 2005 W3C (MIT, ERCIM, Keio)" />
    <meta name="font-size-adjustment" content="-2" />
    <link rel="stylesheet" href="slidy.css" type="text/css" media="screen, projection, print" />
    <link rel="stylesheet" href="w3c-blue.css" type="text/css" media="screen, projection, print" />
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML"></script>
</script>
    <script src="slidy.js" type="text/javascript">
</script>
  </head>
  <body>
    <div class="slide cover title">
      <!-- hidden style graphics to ensure they are saved with other content -->
      <h1>
        Dynamically Reweighting Experts' Advice
      </h1><br />
      <em>With A Recursive Least Squares Example</em>
      <p></p>
    </div>
    <div class="background"></div>
    <div class = "slide">
    <h1>Outline</h1>
    <ul>
    <li><u><b>Introduction</b></u></li>
    <li>An RLS Example</li>
    <li>Reweighing</li>
    <li>Tracking The Best Expert</li>
    <li>Conclusions</li>
    </ul>
    </div>
    <div class="slide">
      <h1>
        What Is This Talk About?
      </h1>
      <ul>
        <li>Suppose we have a number of different "experts"
        </li>
        <li>
          <ul>
            <li>Parameterized algorithms (<i>e.g.</i>, RLS, an example that will follow).
            </li>
            <li>Prediction algorithms suitable for different settings (<i>e.g.</i>, low/high volatility)
            </li>
            <li>Different prediction algorithms
            </li>
          </ul>
        </li>
        <li>How can we dynamically combine the advice?
        </li>
        <li>Currently we:
          <ul>
            <li>Use testing (perhaps using x-validation).
            </li>
            <li>Use state variables
            </li>
            <li>Guess
            </li>
          </ul>
        </li>
      </ul>
    </div>
    <div class="slide">
      <h1>
        Sources
      </h1>
      <ul>
        <li>
          <ul>
            <li>Universal Linear Prediction By Model Order Weighting, A. Singer and M. Feder, IEEE-TIT, 1999
            </li>
            <li>Tracking the Best Expert, M. Herbster and M. K. Warmuth, Machine Learning, 1998
            </li>
          </ul>
        </li>
      </ul>
    </div>
    <div class="slide">
      <h1>
        Prediction Definitions
      </h1>
      <ul>
        <li>Batch Linear Prediction `y[t] ~ \hat{y}[t] = \sum_{k = 1}^pc_k x[t - k]`
        </li>
        <li>Sequential Linear Prediction `y[t] ~ \hat{y}[t] = \sum_{k = 1}^pc_k^{t-1} x[t - k]`
        </li>
        <li>Square Loss `l_n(y, \hat{y} ) = \sum_{t = 1}^n (y[t] - \hat{y}[t] )^2`
        </li>
        <li>Experts `\hat{y}_1, \ldots, \hat{y}_m`
        </li>
      </ul>
    </div>
    <div class = "slide">
    <h1>Outline</h1>
    <ul>
    <li>Introduction</li>
    <li><u><b>An RLS Example</b></u></li>
    <li>Reweighing</li>
    <li>Tracking The Best Expert</li>
    <li>Conclusions</li>
    </ul>
    </div>
    <div class = "slide">
    <h1>RLS Problem</h1>
      <ul>
        <li>Reminder
      <ul>
        <li>Batch Linear Prediction `y[t] ~ \hat{y}[t] = \sum_{k = 1}^pc_k x[t - k]`
        </li>
        <li>Sequential Linear Prediction `y[t] ~ \hat{y}[t] = \sum_{k = 1}^pc_k^{t-1} x[t - k]`
        </li>
      </ul>
	</li>
	<li>Loss function `\sum_{t = 1}^{n - 1} \lambda^{n - 1 - t} (y[t] - \hat{y}[t])^2`</li>
<li>How can the sequential linear prediction coefficients (`c_k^{t - 1}`) be found efficiently?</li>
	</ul>
    </div>
	<div class = "slide">
	<h1>Loss Function Simplification</h1>
	<p>
	The loss function is
	`\sum_{t = 1}^{n - 1} \lambda^{n - 1 - t} (y[t] - \hat{y}[t])^2 =`
	</p>
<p>

	`\sum_{t = 1}^{n - 1} \lambda^{n - 1 - t} (y[t] - \sum_{k = 1}^pc_k^{t-1} x[t - k])^2.`
</p>
<p>
	Set `y'[t] = \sqrt{\lambda^{n - 1 - t}} y[t]`, `x'[t] = \sqrt{\lambda^{n - 1 - t}} x[t]`. The loss function is</p>
<p>
	`\sum_{t = 1}^{n - 1} (y'[t] - \sum_{k = 1}^pc_k^{t-1} x'[t - k])^2.`
</p>
	</div>
    <div class = "slide">
    <h1>Outline</h1>
    <ul>
    <li>Introduction</li>
    <li>An RLS Example</li>
    <li><u><b>Reweighing</b></u></li>
    <li>Tracking The Best Expert</li>
    <li>Conclusions</li>
    </ul>
    </div>
    <div class="slide">
      <h1>
        Exponential Update Rule
      </h1>
      <ul>
        Form a single decision from the experts' decisions using
		<p>
			`\hat{y}_u[t] = \sum_{k = 1}^m u_k[t] \hat{y}_k[t]`
		</p>
			where  
		<p>
			`u_k[t] = \frac{ \mbox{exp}(-\frac{1}{2c}l_{t - 1}(y, \hat{y}_k)) }{ \sum_{j = 1}^m \mbox{exp}(-\frac{1}{2c}l_{t - 1}(y, \hat{y}_j)) }`
		</p>
			and `c` is some parameter.

		<p>
			Experts become "less important" proportionally to the exponent of their loss.
		</p>
        </li>
      </ul>
    </div>
    <div class="slide">
      <h1>
        Universality
      </h1>
	<p>
	The exponential-update universal predictor is <i>universal</i>
	<p>
	<p>
	<b>Theorem</b>
	`\frac{1}{n} l_n(y, \hat{y}_u) \leq \min_k \frac{1}{n} l_n(y, \hat{y}_k) + O(\frac{\ln (m) }{n})`
	</p>
	<p>
		Interesting coding interpretation - the regret is the overhead of "describing" the best expert.
	</p>
    </div>
    <div class="slide">
      <h1>
        Expert's "Probability"
      </h1>
      <ul>
        <li>An expert's probability is `P_k(y^n) = B \mbox{exp}(-\frac{1}{2c}l_n(y, \hat{y}_k))`
          <ul>
            <li>high-loss sequences are <u>defined</u> as "unlikely"
            </li>
          </ul>
        </li>
        <li>Conditional probability is analogously `P_k(y[n]|y^{n - 1}) = \frac{P_k(y^n)}{P_k(y^{n - 1})} = \mbox{exp}(-\frac{1}{2c}l_n(y[n], \hat{y}_k[n]))`
        </li>
      </ul>
    </div>
    <div class="slide">
      <h1>
        Universal "Probability"
      </h1>
	<p>
	The universal probability (<i>i.e.</i>, the universal predictor's probability) is 
	`P_u(y^n) = \frac{1}{m} \sum_{i = 1}^m P_i(y^n)`.
	</p>
	<p>
	Implication:
	`P_u(y^n) \geq \max_i \frac{1}{m} P_i(y^n)`, and so
	`-\ln(P_u(y^n)) \leq \min_i\{ln(M) - \ln(P_i(y^n))\} \leq \min_i\{ln(m) - \ln(B) + \frac{1}{2c} \ln(y, \hat{y}_i)\}`.

	The universal "coding length" is related to the best expert's penalty.
	</p>
    </div>
	<div class = "slide">
		<h1>Universal Performance "Probability"</h1>
	<p>
		Define a probability in terms of the performance:
	</p>
	<p>
		`\hat{P}_u(y^n) = 
		B \mbox{exp}(-\frac{1}{2c}l_n(y[n], \hat{y}_k[n])) =`

		`B \mbox{exp}(-\frac{1}{2c} \sum_{t = 1}^n (y[t] - \sum_{i = 1}^m u_i[t]\hat{y}_i[t])^2) =`
		`\Pi_{t = 1}^n B \mbox{exp}(-\frac{1}{2c} ( y[t] - \sum_{i = 1}^m u_i[t]\hat{y}_i[t])^2) =`
		`\Pi_{t = 1}^n f(\sum_{i = 1}^m u_i[t]\hat{y}_i[t])`
	</p>
	<p>
	where
	`f_t(z) = B \mbox{exp}(-\frac{1}{2c}(y[t] - z)^2)`.
	</p>
	</div>
	<div class = "slide">
	<h1>Relating The Two "Probabilities"</h1>
	<p>Reminder</p>
	<ul>
		<li>
		`P_u(y^n) = \Pi_{t = 1}^n \sum_{i = 1}^m f(u_i[t]\hat{y}_i[t])`
		</li>
		<li>
		`\hat{P}_u(y^n) = \Pi_{t = 1}^n f(\sum_{i = 1}^m u_i[t]\hat{y}_i[t])`
		</li>
	</ul>
	<p>
	If `y` is bounded, then `f` can be made concave, and so, by Jensen's inequality,
	`-\ln(P_u(y^n)) \geq -\ln(\hat{p}_u(y^n)) = \frac{1}{2c} l_n(y, \hat{y}_i)`.
	</p>
	</div>
    <div class = "slide">
    <h1>Outline</h1>
    <ul>
    <li>Introduction</li>
    <li>An RLS Example</li>
    <li>Reweighing</li>
    <li><u><b>Tracking The Best Expert</b></u></li>
    <li>Conclusions</li>
    </ul>
    </div>
	<div class = "slide">
		<h1>Problem</h1>
		<ul>
			<li>`m` experts</li>
			<li>The time segment has a $k$-partition.
			<ul>
				<li>There are `k + 1` segments with unknown start/end times</li>
				<li>Each segment has a "best" expert</li>
				<li>$k$ is unknown</li>
			</ul>
		</ul>
	</div>
	<div class = "slide">
	<h1>Master Algorithm</h1>
	<ul>
		<li><b>Init</b> `w_1^s[1] = \cdots = w_m^s[1] = 1 / m`</li>
		<li><b>Predict</b> Use `\hat{y}_u[t] = \sum_{i = 1}^m v_i[t] \hat{y}_i[t] `, where `v_i[t] = {w_i^s[t]} / {\sum_{j = 1}^m w_j^s[t]}`</li>
		<li><b>Update</b> `w_i^a[t] = w_i^s[t] e^{-1/2 l(y[t], \hat{y})}`</li>
		<li><b>Redistribute</b> Use some rule  s.t. `\sum_{j = 1}^a w_j^s[t + 1] = \sum_{j = 1}^m w_i^a[t]`</li>
	</ul>
	</div>
	<h1>Specific Algorithms</h1>
	<p>Consider two specific redistribution rules:</p>
	<ul>
		<li><b>Init</b> `w_1^s[1] = \cdots = w_m^s[1] = 1 / m`</li>
		<li><b>Predict</b> Use `\sum_{i = 1}^m v_i[t] \hat{y}_i[t] `, where `v_i[t] = {w_i^s[t]} / {\sum_{j = 1}^m w_j^s[t]}`</li>
		<li><b>Update</b> `w_i^a[t] = w_i^s[t] e^{-1/2 l(y[t], \hat{y})}`</li>
		<li><b>Redistribute</b> 
		<ul>
			<li><b>Static Expert</b> `w_i^s[t + 1] = w_i^a[t]`</li>
			<li><b>Fixed Shared</b> `w_i^s[t + 1] = (1 - \alpha) w_i^a[t] + \alpha / ( m - 1 ) \sum_{j \ne i}w_j^a[t]`</li>
		</ul>
		</li>
	</ul>
	</div>
	<div class = "slide">
	<h1>Relating Loss and Weights</h1>
	<p>
	<b>Lemma</b> The Master Algorithm's regret is bounded from above by `-2 \ln(w_{i}^s[n + 1]`.
	</p>
	<p>Proof:
		
		`l(\hat{y}_u[t], y[t]) \le -2 \ln( \sum_{i = 1}^m v_i[t]e^{-1/2l(\hat{y}_i[t], y[t])})` [Vovk], so
		`\sum_{t = 1}^n l(\hat{y}_u[t], y[t]) \le \sum_{t = 1}^n -2 \ln( \sum_{i = 1}^m v_i[t]e^{-1/2l(\hat{y}_i[t], y[t])}) = 
			-2\ln({\sum_{i = 1}^mw_i^a[t]} / {\sum_{i = 1}^mw_i^s[t]})`. But since redistribution does not change the total weights, 
		`\sum_{t = 1}^n l(\hat{y}_u[t], y[t]) \le -2 \sum_{t = 1}^n \ln({\sum_{i = 1}^mw_i^s[t + 1]} / {\sum_{i = 1}^mw_i^s[t]})
			= -2 \ln( \sum_{i = 1}^mw_i^s[t + 1])`.
	</p>
	</div>
	<div class = "slide">
	<h1>Static Expert Regret</h1>
	<p>
	<b>Lemma</b> The Master Algorithm's regret is bounded from above by `-2 \ln(w_{i}^s[n + 1]`.
	</p>
	<p>
	<b>Theorem</b> `l_n(\hat{y}_u, y) \le l_n(\hat{y}_i, y) + 2 \ln(m)`
	</p>
	<p>Proof:
	`\le w_i^s[n + 1] = w_i^s[n] e^{-l(\hat{y}_i[n], y[n])} = \cdots = {w_i^s[1] e^{-l_n(\hat{y}_i, y)} } / m `.
	</p>
	<p>
		The regret is the overhead of "describing" the best expert.
	</p>
	</div>
	<div class = "slide">
	<h1>Fixed Share Analysis - 1/3 </h1>
	<p>
	<b>Singe Expert Lemma</b> `{w_i^a[t']} / {w_i^a[t]} \ge e^{-1/2l_{t, t'}(\hat{y}, \hat{y}_i)} ( 1 - \alpha)^{t' -t }`.
	</p>
	<p>Proof:
 	`w_i^s[t + 1] = (1 - \alpha) e^{-1/2l(\hat{y}_i[t], y[t])}w_i^s[t] + {\alpha}/{n - 1} \sum_{j \ne i}w_j^a[t] \ge `
	`(1 - \alpha) e^{-1/2l(\hat{y}_i[t], y[t])}w_i^s[t]`
	</p>
	<p>The weight of an expert cannot drop too sharply if her loss is small.</p>
	</div>
	<h1>Fixed Share Analysis - 2/3 </h1>
	<p>
	<b>Cross Expert Lemma</b> `{w_i^s[t + 1]} / {w_j^a[t]} \ge {\alpha} / {n - 1}`.
	</p>
	<p>
	`\Leftarrow w_i^s[t + 1] = (1 - \alpha) w_i^a[t] + {\alpha}/{n - 1} \sum_{j \ne i}w_j^a[t] \ge `
	`{\alpha}/{n - 1} \sum_{j \ne i}w_j^a[t]`
	</p>
	<p>The weight of an expert does not drop too low relative to a different expert's.</p>
	</div>
	<div class = "slide">
	<h1>Fixed Share Analysis - 3/3 </h1>
	<p>
	Let the experts' weights be `w_{e_0}, \ldots, w_{e_k}`.
	</p>
	<p>
		<b>Final Expert Weight Lemma</b> 
	`w_{e_k}^s[n + 1] \geq 1 / n e^{-1/2 (l_{1, i_1 - 1}(\hat{y}, \hat{y}_{e_0}) + l_{i_1, i_2 - 1}(\hat{y}, \hat{y}_{e_1}) + \cdots )}(1 - \alpha)^{n - m - 1}({\alpha} / {m - 1})^k`.
	</p>
	<p>Proof:
	`w_{e_k}^s[n + 1] = w_{e_0}^s[t_0] \frac{w_{e_0}^a[t_1 - 1]}{w_{e_0}^s[t_0]} \Pi_{i = 1}^k(\frac{w_{e_i}^s[t_i]}{w_{e_i - 1}^s[t_i - 1]} \frac{w_{e_i}^a[t_{i + 1} - 1]}{w_{e_i}^s[t_i]}) \frac{w_{e_k}^s[n + 1]}{w_{e_k}^s[t_{k + 1} - 1]}`
	Each of these terms is covered by either the Single Expert Lemma or the Cross Expert Lemma; the rest follows by simplification.
	</p>
	</div>
	<div class = "slide">
		<h1>Fixed-Share Regret</h1>
	<p>
	<b>Theorem</b> `l_n(\hat{y}_u, y) \le [l_{1, i_1 - 1}(\hat{y}, \hat{y}_{e_0}) + l_{i_1, i_2 - 1}(\hat{y}, \hat{y}_{e_1}) + \cdots] + `
	` 2(\ln(m) + k \ln(m - 1)) + (H(k / {n - 1}) + D(k / {m - 1} | \alpha))`
	</p>
	<p>
		The regret is the overhead of "describing" the best expert<b>s</b>.
	</p>
	</div>
  </body>
</html>
