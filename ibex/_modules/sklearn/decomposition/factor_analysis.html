
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.decomposition.factor_analysis &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.decomposition.factor_analysis</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Factor Analysis.</span>

<span class="sd">A latent linear variable model.</span>

<span class="sd">FactorAnalysis is similar to probabilistic PCA implemented by PCA.score</span>
<span class="sd">While PCA assumes Gaussian noise with the same variance for each</span>
<span class="sd">feature, the FactorAnalysis model assumes different variances for</span>
<span class="sd">each of them.</span>

<span class="sd">This implementation is based on David Barber&#39;s Book,</span>
<span class="sd">Bayesian Reasoning and Machine Learning,</span>
<span class="sd">http://www.cs.ucl.ac.uk/staff/d.barber/brml,</span>
<span class="sd">Algorithm 21.1</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Author: Christian Osendorfer &lt;osendorf@gmail.com&gt;</span>
<span class="c1">#         Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="c1">#         Denis A. Engemann &lt;denis-alexander.engemann@inria.fr&gt;</span>

<span class="c1"># License: BSD3</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="k">import</span> <span class="n">sqrt</span><span class="p">,</span> <span class="n">log</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">linalg</span>


<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span>
<span class="kn">from</span> <span class="nn">..externals.six.moves</span> <span class="k">import</span> <span class="n">xrange</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="k">import</span> <span class="n">fast_logdet</span><span class="p">,</span> <span class="n">randomized_svd</span><span class="p">,</span> <span class="n">squared_norm</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="k">import</span> <span class="n">ConvergenceWarning</span>


<span class="k">class</span> <span class="nc">FactorAnalysis</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Factor Analysis (FA)</span>

<span class="sd">    A simple linear generative model with Gaussian latent variables.</span>

<span class="sd">    The observations are assumed to be caused by a linear transformation of</span>
<span class="sd">    lower dimensional latent factors and added Gaussian noise.</span>
<span class="sd">    Without loss of generality the factors are distributed according to a</span>
<span class="sd">    Gaussian with zero mean and unit covariance. The noise is also zero mean</span>
<span class="sd">    and has an arbitrary diagonal covariance matrix.</span>

<span class="sd">    If we would restrict the model further, by assuming that the Gaussian</span>
<span class="sd">    noise is even isotropic (all diagonal entries are the same) we would obtain</span>
<span class="sd">    :class:`PPCA`.</span>

<span class="sd">    FactorAnalysis performs a maximum likelihood estimate of the so-called</span>
<span class="sd">    `loading` matrix, the transformation of the latent variables to the</span>
<span class="sd">    observed ones, using expectation-maximization (EM).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;FA&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_components : int | None</span>
<span class="sd">        Dimensionality of latent space, the number of components</span>
<span class="sd">        of ``X`` that are obtained after ``transform``.</span>
<span class="sd">        If None, n_components is set to the number of features.</span>

<span class="sd">    tol : float</span>
<span class="sd">        Stopping tolerance for EM algorithm.</span>

<span class="sd">    copy : bool</span>
<span class="sd">        Whether to make a copy of X. If ``False``, the input X gets overwritten</span>
<span class="sd">        during fitting.</span>

<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    noise_variance_init : None | array, shape=(n_features,)</span>
<span class="sd">        The initial guess of the noise variance for each feature.</span>
<span class="sd">        If None, it defaults to np.ones(n_features)</span>

<span class="sd">    svd_method : {&#39;lapack&#39;, &#39;randomized&#39;}</span>
<span class="sd">        Which SVD method to use. If &#39;lapack&#39; use standard SVD from</span>
<span class="sd">        scipy.linalg, if &#39;randomized&#39; use fast ``randomized_svd`` function.</span>
<span class="sd">        Defaults to &#39;randomized&#39;. For most applications &#39;randomized&#39; will</span>
<span class="sd">        be sufficiently precise while providing significant speed gains.</span>
<span class="sd">        Accuracy can also be improved by setting higher values for</span>
<span class="sd">        `iterated_power`. If this is not sufficient, for maximum precision</span>
<span class="sd">        you should choose &#39;lapack&#39;.</span>

<span class="sd">    iterated_power : int, optional</span>
<span class="sd">        Number of iterations for the power method. 3 by default. Only used</span>
<span class="sd">        if ``svd_method`` equals &#39;randomized&#39;</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=0)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`. Only used when ``svd_method`` equals &#39;randomized&#39;.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    components_ : array, [n_components, n_features]</span>
<span class="sd">        Components with maximum variance.</span>

<span class="sd">    loglike_ : list, [n_iterations]</span>
<span class="sd">        The log likelihood at each iteration.</span>

<span class="sd">    noise_variance_ : array, shape=(n_features,)</span>
<span class="sd">        The estimated noise variance for each feature.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        Number of iterations run.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. David Barber, Bayesian Reasoning and Machine Learning,</span>
<span class="sd">        Algorithm 21.1</span>

<span class="sd">    .. Christopher M. Bishop: Pattern Recognition and Machine Learning,</span>
<span class="sd">        Chapter 12.2.4</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    PCA: Principal component analysis is also a latent linear variable model</span>
<span class="sd">        which however assumes equal noise variance for each feature.</span>
<span class="sd">        This extra assumption makes probabilistic PCA faster as it can be</span>
<span class="sd">        computed in closed form.</span>
<span class="sd">    FastICA: Independent component analysis, a latent variable model with</span>
<span class="sd">        non-Gaussian latent variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">noise_variance_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">svd_method</span><span class="o">=</span><span class="s1">&#39;randomized&#39;</span><span class="p">,</span>
                 <span class="n">iterated_power</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy</span> <span class="o">=</span> <span class="n">copy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="k">if</span> <span class="n">svd_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;lapack&#39;</span><span class="p">,</span> <span class="s1">&#39;randomized&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;SVD method </span><span class="si">%s</span><span class="s1"> is not supported. Please consider&#39;</span>
                             <span class="s1">&#39; the documentation&#39;</span> <span class="o">%</span> <span class="n">svd_method</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">svd_method</span> <span class="o">=</span> <span class="n">svd_method</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_init</span> <span class="o">=</span> <span class="n">noise_variance_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iterated_power</span> <span class="o">=</span> <span class="n">iterated_power</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

<div class="viewcode-block" id="FactorAnalysis.fit"><a class="viewcode-back" href="../../../tmp/api_ibex_sklearn_decomposition_factoranalysis.html#ibex.sklearn.decomposition.FactorAnalysis.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the FactorAnalysis model to X using EM</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_features)</span>
<span class="sd">            Training data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span>
        <span class="k">if</span> <span class="n">n_components</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_components</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>

        <span class="c1"># some constant terms</span>
        <span class="n">nsqrt</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">llconst</span> <span class="o">=</span> <span class="n">n_features</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> <span class="n">n_components</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_init</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_features</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;noise_variance_init dimension does not &quot;</span>
                                 <span class="s2">&quot;with number of features : </span><span class="si">%d</span><span class="s2"> != </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_init</span><span class="p">),</span> <span class="n">n_features</span><span class="p">))</span>
            <span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_init</span><span class="p">)</span>

        <span class="n">loglike</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">old_ll</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">SMALL</span> <span class="o">=</span> <span class="mf">1e-12</span>

        <span class="c1"># we&#39;ll modify svd outputs to return unexplained variance</span>
        <span class="c1"># to allow for unified computation of loglikelihood</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">svd_method</span> <span class="o">==</span> <span class="s1">&#39;lapack&#39;</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">my_svd</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">n_components</span><span class="p">],</span> <span class="n">V</span><span class="p">[:</span><span class="n">n_components</span><span class="p">],</span>
                        <span class="n">squared_norm</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">n_components</span><span class="p">:]))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">svd_method</span> <span class="o">==</span> <span class="s1">&#39;randomized&#39;</span><span class="p">:</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">my_svd</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">randomized_svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span>
                                         <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                         <span class="n">n_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">iterated_power</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">s</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">squared_norm</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">squared_norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;SVD method </span><span class="si">%s</span><span class="s1"> is not supported. Please consider&#39;</span>
                             <span class="s1">&#39; the documentation&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">svd_method</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="c1"># SMALL helps numerics</span>
            <span class="n">sqrt_psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">psi</span><span class="p">)</span> <span class="o">+</span> <span class="n">SMALL</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">unexp_var</span> <span class="o">=</span> <span class="n">my_svd</span><span class="p">(</span><span class="n">X</span> <span class="o">/</span> <span class="p">(</span><span class="n">sqrt_psi</span> <span class="o">*</span> <span class="n">nsqrt</span><span class="p">))</span>
            <span class="n">s</span> <span class="o">**=</span> <span class="mi">2</span>
            <span class="c1"># Use &#39;maximum&#39; here to avoid sqrt problems.</span>
            <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">))[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">V</span>
            <span class="k">del</span> <span class="n">V</span>
            <span class="n">W</span> <span class="o">*=</span> <span class="n">sqrt_psi</span>

            <span class="c1"># loglikelihood</span>
            <span class="n">ll</span> <span class="o">=</span> <span class="n">llconst</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
            <span class="n">ll</span> <span class="o">+=</span> <span class="n">unexp_var</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">psi</span><span class="p">))</span>
            <span class="n">ll</span> <span class="o">*=</span> <span class="o">-</span><span class="n">n_samples</span> <span class="o">/</span> <span class="mf">2.</span>
            <span class="n">loglike</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">ll</span> <span class="o">-</span> <span class="n">old_ll</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">old_ll</span> <span class="o">=</span> <span class="n">ll</span>

            <span class="n">psi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">var</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">SMALL</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;FactorAnalysis did not converge.&#39;</span> <span class="o">+</span>
                          <span class="s1">&#39; You might want&#39;</span> <span class="o">+</span>
                          <span class="s1">&#39; to increase the number of iterations.&#39;</span><span class="p">,</span>
                          <span class="n">ConvergenceWarning</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">W</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span> <span class="o">=</span> <span class="n">psi</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loglike_</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="FactorAnalysis.transform"><a class="viewcode-back" href="../../../tmp/api_ibex_sklearn_decomposition_factoranalysis.html#ibex.sklearn.decomposition.FactorAnalysis.transform">[docs]</a>    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply dimensionality reduction to X using the model.</span>

<span class="sd">        Compute the expected mean of the latent variables.</span>
<span class="sd">        See Barber, 21.2.33 (or Bishop, 12.66).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_features)</span>
<span class="sd">            Training data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_new : array-like, shape (n_samples, n_components)</span>
<span class="sd">            The latent variables of X.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;components_&#39;</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">Ih</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="p">))</span>

        <span class="n">X_transformed</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>

        <span class="n">Wpsi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span>
        <span class="n">cov_z</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Ih</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wpsi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">Wpsi</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">X_transformed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">cov_z</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">X_transformed</span></div>

    <span class="k">def</span> <span class="nf">get_covariance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute data covariance with the FactorAnalysis model.</span>

<span class="sd">        ``cov = components_.T * components_ + diag(noise_variance)``</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        cov : array, shape (n_features, n_features)</span>
<span class="sd">            Estimated covariance of data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;components_&#39;</span><span class="p">)</span>

        <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
        <span class="n">cov</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span><span class="nb">len</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span>  <span class="c1"># modify diag inplace</span>
        <span class="k">return</span> <span class="n">cov</span>

    <span class="k">def</span> <span class="nf">get_precision</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute data precision matrix with the FactorAnalysis model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        precision : array, shape (n_features, n_features)</span>
<span class="sd">            Estimated precision of data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;components_&#39;</span><span class="p">)</span>

        <span class="n">n_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># handle corner cases first</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">==</span> <span class="n">n_features</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_covariance</span><span class="p">())</span>

        <span class="c1"># Get precision using matrix inversion lemma</span>
        <span class="n">components_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">components_</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">components_</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span><span class="p">,</span> <span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">precision</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span><span class="nb">len</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">precision</span><span class="p">),</span> <span class="n">components_</span><span class="p">))</span>
        <span class="n">precision</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">precision</span> <span class="o">/=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">precision</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span><span class="nb">len</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise_variance_</span>
        <span class="k">return</span> <span class="n">precision</span>

<div class="viewcode-block" id="FactorAnalysis.score_samples"><a class="viewcode-back" href="../../../tmp/api_ibex_sklearn_decomposition_factoranalysis.html#ibex.sklearn.decomposition.FactorAnalysis.score_samples">[docs]</a>    <span class="k">def</span> <span class="nf">score_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the log-likelihood of each sample</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array, shape (n_samples, n_features)</span>
<span class="sd">            The data</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ll : array, shape (n_samples,)</span>
<span class="sd">            Log-likelihood of each sample under the current model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;components_&#39;</span><span class="p">)</span>

        <span class="n">Xr</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_precision</span><span class="p">()</span>
        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">log_like</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">log_like</span> <span class="o">=</span> <span class="o">-.</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">Xr</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xr</span><span class="p">,</span> <span class="n">precision</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_like</span> <span class="o">-=</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
                          <span class="o">-</span> <span class="n">fast_logdet</span><span class="p">(</span><span class="n">precision</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">log_like</span></div>

<div class="viewcode-block" id="FactorAnalysis.score"><a class="viewcode-back" href="../../../tmp/api_ibex_sklearn_decomposition_factoranalysis.html#ibex.sklearn.decomposition.FactorAnalysis.score">[docs]</a>    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the average log-likelihood of the samples</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array, shape (n_samples, n_features)</span>
<span class="sd">            The data</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ll : float</span>
<span class="sd">            Average log-likelihood of the samples under the current model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">))</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>