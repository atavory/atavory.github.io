
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.gaussian_process.gaussian_process &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.gaussian_process.gaussian_process</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>

<span class="c1"># Author: Vincent Dubourg &lt;vincent.dubourg@gmail.com&gt;</span>
<span class="c1">#         (mostly translation, see implementation details)</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">linalg</span><span class="p">,</span> <span class="n">optimize</span>

<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..metrics.pairwise</span> <span class="k">import</span> <span class="n">manhattan_distances</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">regression_models</span> <span class="k">as</span> <span class="n">regression</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">correlation_models</span> <span class="k">as</span> <span class="n">correlation</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">deprecated</span>

<span class="n">MACHINE_EPSILON</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>


<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;l1_cross_distances was deprecated in version 0.18 &quot;</span>
            <span class="s2">&quot;and will be removed in 0.20.&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">l1_cross_distances</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the nonzero componentwise L1 cross-distances between the vectors</span>
<span class="sd">    in X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    X : array_like</span>
<span class="sd">        An array with shape (n_samples, n_features)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    D : array with shape (n_samples * (n_samples - 1) / 2, n_features)</span>
<span class="sd">        The array of componentwise L1 cross-distances.</span>

<span class="sd">    ij : arrays with shape (n_samples * (n_samples - 1) / 2, 2)</span>
<span class="sd">        The indices i and j of the vectors in X associated to the cross-</span>
<span class="sd">        distances in D: D[k] = np.abs(X[ij[k, 0]] - Y[ij[k, 1]]).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_nonzero_cross_dist</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">ij</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_nonzero_cross_dist</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_nonzero_cross_dist</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
    <span class="n">ll_1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ll_0</span> <span class="o">=</span> <span class="n">ll_1</span>
        <span class="n">ll_1</span> <span class="o">=</span> <span class="n">ll_0</span> <span class="o">+</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">ij</span><span class="p">[</span><span class="n">ll_0</span><span class="p">:</span><span class="n">ll_1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">ij</span><span class="p">[</span><span class="n">ll_0</span><span class="p">:</span><span class="n">ll_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">D</span><span class="p">[</span><span class="n">ll_0</span><span class="p">:</span><span class="n">ll_1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">X</span><span class="p">[(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span><span class="n">n_samples</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">D</span><span class="p">,</span> <span class="n">ij</span>


<span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;GaussianProcess was deprecated in version 0.18 and will be &quot;</span>
            <span class="s2">&quot;removed in 0.20. Use the GaussianProcessRegressor instead.&quot;</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">GaussianProcess</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The legacy Gaussian Process model class.</span>

<span class="sd">    .. deprecated:: 0.18</span>
<span class="sd">        This class will be removed in 0.20.</span>
<span class="sd">        Use the :class:`GaussianProcessRegressor` instead.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;gaussian_process&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    regr : string or callable, optional</span>
<span class="sd">        A regression function returning an array of outputs of the linear</span>
<span class="sd">        regression functional basis. The number of observations n_samples</span>
<span class="sd">        should be greater than the size p of this basis.</span>
<span class="sd">        Default assumes a simple constant regression trend.</span>
<span class="sd">        Available built-in regression models are::</span>

<span class="sd">            &#39;constant&#39;, &#39;linear&#39;, &#39;quadratic&#39;</span>

<span class="sd">    corr : string or callable, optional</span>
<span class="sd">        A stationary autocorrelation function returning the autocorrelation</span>
<span class="sd">        between two points x and x&#39;.</span>
<span class="sd">        Default assumes a squared-exponential autocorrelation model.</span>
<span class="sd">        Built-in correlation models are::</span>

<span class="sd">            &#39;absolute_exponential&#39;, &#39;squared_exponential&#39;,</span>
<span class="sd">            &#39;generalized_exponential&#39;, &#39;cubic&#39;, &#39;linear&#39;</span>

<span class="sd">    beta0 : double array_like, optional</span>
<span class="sd">        The regression weight vector to perform Ordinary Kriging (OK).</span>
<span class="sd">        Default assumes Universal Kriging (UK) so that the vector beta of</span>
<span class="sd">        regression weights is estimated using the maximum likelihood</span>
<span class="sd">        principle.</span>

<span class="sd">    storage_mode : string, optional</span>
<span class="sd">        A string specifying whether the Cholesky decomposition of the</span>
<span class="sd">        correlation matrix should be stored in the class (storage_mode =</span>
<span class="sd">        &#39;full&#39;) or not (storage_mode = &#39;light&#39;).</span>
<span class="sd">        Default assumes storage_mode = &#39;full&#39;, so that the</span>
<span class="sd">        Cholesky decomposition of the correlation matrix is stored.</span>
<span class="sd">        This might be a useful parameter when one is not interested in the</span>
<span class="sd">        MSE and only plan to estimate the BLUP, for which the correlation</span>
<span class="sd">        matrix is not required.</span>

<span class="sd">    verbose : boolean, optional</span>
<span class="sd">        A boolean specifying the verbose level.</span>
<span class="sd">        Default is verbose = False.</span>

<span class="sd">    theta0 : double array_like, optional</span>
<span class="sd">        An array with shape (n_features, ) or (1, ).</span>
<span class="sd">        The parameters in the autocorrelation model.</span>
<span class="sd">        If thetaL and thetaU are also specified, theta0 is considered as</span>
<span class="sd">        the starting point for the maximum likelihood estimation of the</span>
<span class="sd">        best set of parameters.</span>
<span class="sd">        Default assumes isotropic autocorrelation model with theta0 = 1e-1.</span>

<span class="sd">    thetaL : double array_like, optional</span>
<span class="sd">        An array with shape matching theta0&#39;s.</span>
<span class="sd">        Lower bound on the autocorrelation parameters for maximum</span>
<span class="sd">        likelihood estimation.</span>
<span class="sd">        Default is None, so that it skips maximum likelihood estimation and</span>
<span class="sd">        it uses theta0.</span>

<span class="sd">    thetaU : double array_like, optional</span>
<span class="sd">        An array with shape matching theta0&#39;s.</span>
<span class="sd">        Upper bound on the autocorrelation parameters for maximum</span>
<span class="sd">        likelihood estimation.</span>
<span class="sd">        Default is None, so that it skips maximum likelihood estimation and</span>
<span class="sd">        it uses theta0.</span>

<span class="sd">    normalize : boolean, optional</span>
<span class="sd">        Input X and observations y are centered and reduced wrt</span>
<span class="sd">        means and standard deviations estimated from the n_samples</span>
<span class="sd">        observations provided.</span>
<span class="sd">        Default is normalize = True so that data is normalized to ease</span>
<span class="sd">        maximum likelihood estimation.</span>

<span class="sd">    nugget : double or ndarray, optional</span>
<span class="sd">        Introduce a nugget effect to allow smooth predictions from noisy</span>
<span class="sd">        data.  If nugget is an ndarray, it must be the same length as the</span>
<span class="sd">        number of data points used for the fit.</span>
<span class="sd">        The nugget is added to the diagonal of the assumed training covariance;</span>
<span class="sd">        in this way it acts as a Tikhonov regularization in the problem.  In</span>
<span class="sd">        the special case of the squared exponential correlation function, the</span>
<span class="sd">        nugget mathematically represents the variance of the input values.</span>
<span class="sd">        Default assumes a nugget close to machine precision for the sake of</span>
<span class="sd">        robustness (nugget = 10. * MACHINE_EPSILON).</span>

<span class="sd">    optimizer : string, optional</span>
<span class="sd">        A string specifying the optimization algorithm to be used.</span>
<span class="sd">        Default uses &#39;fmin_cobyla&#39; algorithm from scipy.optimize.</span>
<span class="sd">        Available optimizers are::</span>

<span class="sd">            &#39;fmin_cobyla&#39;, &#39;Welch&#39;</span>

<span class="sd">        &#39;Welch&#39; optimizer is dued to Welch et al., see reference [WBSWM1992]_.</span>
<span class="sd">        It consists in iterating over several one-dimensional optimizations</span>
<span class="sd">        instead of running one single multi-dimensional optimization.</span>

<span class="sd">    random_start : int, optional</span>
<span class="sd">        The number of times the Maximum Likelihood Estimation should be</span>
<span class="sd">        performed from a random starting point.</span>
<span class="sd">        The first MLE always uses the specified starting point (theta0),</span>
<span class="sd">        the next starting points are picked at random according to an</span>
<span class="sd">        exponential distribution (log-uniform on [thetaL, thetaU]).</span>
<span class="sd">        Default does not use random starting point (random_start = 1).</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The generator used to shuffle the sequence of coordinates of theta in</span>
<span class="sd">        the Welch optimizer. If int, random_state is the seed used by the</span>
<span class="sd">        random number generator; If RandomState instance, random_state is the</span>
<span class="sd">        random number generator; If None, the random number generator is the</span>
<span class="sd">        RandomState instance used by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    theta_ : array</span>
<span class="sd">        Specified theta OR the best set of autocorrelation parameters (the \</span>
<span class="sd">        sought maximizer of the reduced likelihood function).</span>

<span class="sd">    reduced_likelihood_function_value_ : array</span>
<span class="sd">        The optimal reduced likelihood function value.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcess</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[1., 3., 5., 6., 7., 8.]]).T</span>
<span class="sd">    &gt;&gt;&gt; y = (X * np.sin(X)).ravel()</span>
<span class="sd">    &gt;&gt;&gt; gp = GaussianProcess(theta0=0.1, thetaL=.001, thetaU=1.)</span>
<span class="sd">    &gt;&gt;&gt; gp.fit(X, y)                                      # doctest: +ELLIPSIS</span>
<span class="sd">    GaussianProcess(beta0=None...</span>
<span class="sd">            ...</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The presentation implementation is based on a translation of the DACE</span>
<span class="sd">    Matlab toolbox, see reference [NLNS2002]_.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [NLNS2002] `H.B. Nielsen, S.N. Lophaven, H. B. Nielsen and J.</span>
<span class="sd">        Sondergaard.  DACE - A MATLAB Kriging Toolbox.` (2002)</span>
<span class="sd">        http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Lab/lab_maps/krigging/DACE-krigingsoft/dace/dace.pdf</span>

<span class="sd">    .. [WBSWM1992] `W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell,</span>
<span class="sd">        and M.D.  Morris (1992). Screening, predicting, and computer</span>
<span class="sd">        experiments.  Technometrics, 34(1) 15--25.`</span>
<span class="sd">        http://www.jstor.org/stable/1269548</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_regression_types</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;constant&#39;</span><span class="p">:</span> <span class="n">regression</span><span class="o">.</span><span class="n">constant</span><span class="p">,</span>
        <span class="s1">&#39;linear&#39;</span><span class="p">:</span> <span class="n">regression</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span>
        <span class="s1">&#39;quadratic&#39;</span><span class="p">:</span> <span class="n">regression</span><span class="o">.</span><span class="n">quadratic</span><span class="p">}</span>

    <span class="n">_correlation_types</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;absolute_exponential&#39;</span><span class="p">:</span> <span class="n">correlation</span><span class="o">.</span><span class="n">absolute_exponential</span><span class="p">,</span>
        <span class="s1">&#39;squared_exponential&#39;</span><span class="p">:</span> <span class="n">correlation</span><span class="o">.</span><span class="n">squared_exponential</span><span class="p">,</span>
        <span class="s1">&#39;generalized_exponential&#39;</span><span class="p">:</span> <span class="n">correlation</span><span class="o">.</span><span class="n">generalized_exponential</span><span class="p">,</span>
        <span class="s1">&#39;cubic&#39;</span><span class="p">:</span> <span class="n">correlation</span><span class="o">.</span><span class="n">cubic</span><span class="p">,</span>
        <span class="s1">&#39;linear&#39;</span><span class="p">:</span> <span class="n">correlation</span><span class="o">.</span><span class="n">linear</span><span class="p">}</span>

    <span class="n">_optimizer_types</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;fmin_cobyla&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Welch&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">regr</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">corr</span><span class="o">=</span><span class="s1">&#39;squared_exponential&#39;</span><span class="p">,</span> <span class="n">beta0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">storage_mode</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">theta0</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">,</span>
                 <span class="n">thetaL</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">thetaU</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;fmin_cobyla&#39;</span><span class="p">,</span>
                 <span class="n">random_start</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">nugget</span><span class="o">=</span><span class="mf">10.</span> <span class="o">*</span> <span class="n">MACHINE_EPSILON</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">regr</span> <span class="o">=</span> <span class="n">regr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="o">=</span> <span class="n">corr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="o">=</span> <span class="n">beta0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">storage_mode</span> <span class="o">=</span> <span class="n">storage_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span> <span class="o">=</span> <span class="n">theta0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="o">=</span> <span class="n">thetaL</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">=</span> <span class="n">thetaU</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">normalize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nugget</span> <span class="o">=</span> <span class="n">nugget</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_start</span> <span class="o">=</span> <span class="n">random_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

<div class="viewcode-block" id="GaussianProcess.fit"><a class="viewcode-back" href="../../../api_ibex_sklearn_gaussian_process_gaussianprocess.html#ibex.sklearn.gaussian_process.GaussianProcess.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The Gaussian Process model fitting method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : double array_like</span>
<span class="sd">            An array with shape (n_samples, n_features) with the input at which</span>
<span class="sd">            observations were made.</span>

<span class="sd">        y : double array_like</span>
<span class="sd">            An array with shape (n_samples, ) or shape (n_samples, n_targets)</span>
<span class="sd">            with the observations of the output to be predicted.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        gp : self</span>
<span class="sd">            A fitted Gaussian Process model object awaiting data to perform</span>
<span class="sd">            predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Run input checks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Force data to 2D numpy.array</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_numeric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_ndim_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

        <span class="c1"># Check shapes of DOE &amp; observations</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Run input checks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="c1"># Normalize data or don&#39;t</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">:</span>
            <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">X_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">X_std</span><span class="p">[</span><span class="n">X_std</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
            <span class="n">y_std</span><span class="p">[</span><span class="n">y_std</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
            <span class="c1"># center and scale X if necessary</span>
            <span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">X_std</span>
            <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">y_std</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">X_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Calculate matrix of distances D between samples</span>
        <span class="n">D</span><span class="p">,</span> <span class="n">ij</span> <span class="o">=</span> <span class="n">l1_cross_distances</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">==</span> <span class="mf">0.</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="o">!=</span> <span class="n">correlation</span><span class="o">.</span><span class="n">pure_nugget</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Multiple input features cannot have the same&quot;</span>
                            <span class="s2">&quot; target value.&quot;</span><span class="p">)</span>

        <span class="c1"># Regression matrix and parameters</span>
        <span class="n">F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">n_samples_F</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">n_samples_F</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Number of rows in F and X do not match. Most &quot;</span>
                            <span class="s2">&quot;likely something is going wrong with the &quot;</span>
                            <span class="s2">&quot;regression model.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="n">n_samples_F</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">((</span><span class="s2">&quot;Ordinary least squares problem is undetermined &quot;</span>
                             <span class="s2">&quot;n_samples=</span><span class="si">%d</span><span class="s2"> must be greater than the &quot;</span>
                             <span class="s2">&quot;regression model size p=</span><span class="si">%d</span><span class="s2">.&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">p</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Shapes of beta0 and F do not match.&quot;</span><span class="p">)</span>

        <span class="c1"># Set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ij</span> <span class="o">=</span> <span class="n">ij</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">F</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_std</span> <span class="o">=</span> <span class="n">X_mean</span><span class="p">,</span> <span class="n">X_std</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_std</span> <span class="o">=</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span>

        <span class="c1"># Determine Gaussian Process model parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Maximum Likelihood Estimation of the parameters</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Performing Maximum Likelihood Estimation of the &quot;</span>
                      <span class="s2">&quot;autocorrelation parameters...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function_value_</span><span class="p">,</span> <span class="n">par</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">_arg_max_reduced_likelihood_function</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function_value_</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Bad parameter region. &quot;</span>
                                <span class="s2">&quot;Try increasing upper bound&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Given parameters</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Given autocorrelation parameters. &quot;</span>
                      <span class="s2">&quot;Computing Gaussian Process model parameters...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function_value_</span><span class="p">,</span> <span class="n">par</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function_value_</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Bad point. Try increasing theta0.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;sigma2&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ft</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;Ft&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;G&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_mode</span> <span class="o">==</span> <span class="s1">&#39;light&#39;</span><span class="p">:</span>
            <span class="c1"># Delete heavy data (it will be computed again if required)</span>
            <span class="c1"># (it is required only when MSE is wanted in self.predict)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Light storage mode specified. &quot;</span>
                      <span class="s2">&quot;Flushing autocorrelation matrix...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ij</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Ft</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="GaussianProcess.predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_gaussian_process_gaussianprocess.html#ibex.sklearn.gaussian_process.GaussianProcess.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">eval_MSE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function evaluates the Gaussian Process model at x.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array_like</span>
<span class="sd">            An array with shape (n_eval, n_features) giving the point(s) at</span>
<span class="sd">            which the prediction(s) should be made.</span>

<span class="sd">        eval_MSE : boolean, optional</span>
<span class="sd">            A boolean specifying whether the Mean Squared Error should be</span>
<span class="sd">            evaluated or not.</span>
<span class="sd">            Default assumes evalMSE = False and evaluates only the BLUP (mean</span>
<span class="sd">            prediction).</span>

<span class="sd">        batch_size : integer, optional</span>
<span class="sd">            An integer giving the maximum number of points that can be</span>
<span class="sd">            evaluated simultaneously (depending on the available memory).</span>
<span class="sd">            Default is None so that all given points are evaluated at the same</span>
<span class="sd">            time.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array_like, shape (n_samples, ) or (n_samples, n_targets)</span>
<span class="sd">            An array with shape (n_eval, ) if the Gaussian Process was trained</span>
<span class="sd">            on an array of shape (n_samples, ) or an array with shape</span>
<span class="sd">            (n_eval, n_targets) if the Gaussian Process was trained on an array</span>
<span class="sd">            of shape (n_samples, n_targets) with the Best Linear Unbiased</span>
<span class="sd">            Prediction at x.</span>

<span class="sd">        MSE : array_like, optional (if eval_MSE == True)</span>
<span class="sd">            An array with shape (n_eval, ) or (n_eval, n_targets) as with y,</span>
<span class="sd">            with the Mean Squared Error at x.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">)</span>

        <span class="c1"># Check input shapes</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">n_eval</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n_samples_y</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Run input checks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_features</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s2">&quot;The number of features in X (X.shape[1] = </span><span class="si">%d</span><span class="s2">) &quot;</span>
                              <span class="s2">&quot;should match the number of features used &quot;</span>
                              <span class="s2">&quot;for fit() &quot;</span>
                              <span class="s2">&quot;which is </span><span class="si">%d</span><span class="s2">.&quot;</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_features</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># No memory management</span>
            <span class="c1"># (evaluates all given points in a single batch run)</span>

            <span class="c1"># Normalize input</span>
            <span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_std</span>

            <span class="c1"># Initialize output</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eval</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">eval_MSE</span><span class="p">:</span>
                <span class="n">MSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eval</span><span class="p">)</span>

            <span class="c1"># Get pairwise componentwise L1-distances to the input training set</span>
            <span class="n">dx</span> <span class="o">=</span> <span class="n">manhattan_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">sum_over_features</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Get regression function and correlation</span>
            <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta_</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_eval</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

            <span class="c1"># Scaled predictor</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>

            <span class="c1"># Predictor</span>
            <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_std</span> <span class="o">*</span> <span class="n">y_</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_eval</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_ndim_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

            <span class="c1"># Mean Squared Error</span>
            <span class="k">if</span> <span class="n">eval_MSE</span><span class="p">:</span>
                <span class="n">C</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span>
                <span class="k">if</span> <span class="n">C</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Light storage mode (need to recompute C, F, Ft and G)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;This GaussianProcess used &#39;light&#39; storage mode &quot;</span>
                              <span class="s2">&quot;at instantiation. Need to recompute &quot;</span>
                              <span class="s2">&quot;autocorrelation matrix...&quot;</span><span class="p">)</span>
                    <span class="n">reduced_likelihood_function_value</span><span class="p">,</span> <span class="n">par</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">Ft</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;Ft&#39;</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">par</span><span class="p">[</span><span class="s1">&#39;G&#39;</span><span class="p">]</span>

                <span class="n">rt</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># Universal Kriging</span>
                    <span class="n">u</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                                <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ft</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">rt</span><span class="p">)</span> <span class="o">-</span> <span class="n">f</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                                <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Ordinary Kriging</span>
                    <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">n_eval</span><span class="p">))</span>

                <span class="n">MSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                             <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="p">(</span><span class="n">rt</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                              <span class="o">+</span> <span class="p">(</span><span class="n">u</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:])</span>
                <span class="n">MSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">MSE</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_targets</span><span class="p">)</span>

                <span class="c1"># Mean Squared Error might be slightly negative depending on</span>
                <span class="c1"># machine precision: force to zero!</span>
                <span class="n">MSE</span><span class="p">[</span><span class="n">MSE</span> <span class="o">&lt;</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_ndim_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">MSE</span> <span class="o">=</span> <span class="n">MSE</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

                <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">MSE</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="k">return</span> <span class="n">y</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Memory management</span>

            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span> <span class="ow">or</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;batch_size must be a positive integer&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">eval_MSE</span><span class="p">:</span>

                <span class="n">y</span><span class="p">,</span> <span class="n">MSE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eval</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eval</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_eval</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">))):</span>
                    <span class="n">batch_from</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">batch_size</span>
                    <span class="n">batch_to</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_eval</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">batch_from</span><span class="p">:</span><span class="n">batch_to</span><span class="p">],</span> <span class="n">MSE</span><span class="p">[</span><span class="n">batch_from</span><span class="p">:</span><span class="n">batch_to</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch_from</span><span class="p">:</span><span class="n">batch_to</span><span class="p">],</span>
                                     <span class="n">eval_MSE</span><span class="o">=</span><span class="n">eval_MSE</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">MSE</span>

            <span class="k">else</span><span class="p">:</span>

                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eval</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_eval</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">))):</span>
                    <span class="n">batch_from</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">batch_size</span>
                    <span class="n">batch_to</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([(</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_eval</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">batch_from</span><span class="p">:</span><span class="n">batch_to</span><span class="p">]</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch_from</span><span class="p">:</span><span class="n">batch_to</span><span class="p">],</span>
                                     <span class="n">eval_MSE</span><span class="o">=</span><span class="n">eval_MSE</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">y</span></div>

    <span class="k">def</span> <span class="nf">reduced_likelihood_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function determines the BLUP parameters and evaluates the reduced</span>
<span class="sd">        likelihood function for the given autocorrelation parameters theta.</span>

<span class="sd">        Maximizing this function wrt the autocorrelation parameters theta is</span>
<span class="sd">        equivalent to maximizing the likelihood of the assumed joint Gaussian</span>
<span class="sd">        distribution of the observations y evaluated onto the design of</span>
<span class="sd">        experiments X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        theta : array_like, optional</span>
<span class="sd">            An array containing the autocorrelation parameters at which the</span>
<span class="sd">            Gaussian Process model parameters should be determined.</span>
<span class="sd">            Default uses the built-in autocorrelation parameters</span>
<span class="sd">            (ie ``theta = self.theta_``).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        reduced_likelihood_function_value : double</span>
<span class="sd">            The value of the reduced likelihood function associated to the</span>
<span class="sd">            given autocorrelation parameters theta.</span>

<span class="sd">        par : dict</span>
<span class="sd">            A dictionary containing the requested Gaussian Process model</span>
<span class="sd">            parameters:</span>

<span class="sd">            - ``sigma2`` is the Gaussian Process variance.</span>
<span class="sd">            - ``beta`` is the generalized least-squares regression weights for</span>
<span class="sd">              Universal Kriging or given beta0 for Ordinary Kriging.</span>
<span class="sd">            - ``gamma`` is the Gaussian Process weights.</span>
<span class="sd">            - ``C`` is the Cholesky decomposition of the correlation</span>
<span class="sd">              matrix [R].</span>
<span class="sd">            - ``Ft`` is the solution of the linear equation system</span>
<span class="sd">              [R] x Ft = F</span>
<span class="sd">            - ``G`` is the QR decomposition of the matrix Ft.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;X&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">theta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use built-in autocorrelation parameters</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_</span>

        <span class="c1"># Initialize output</span>
        <span class="n">reduced_likelihood_function_value</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">par</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Retrieve data</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">D</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span>
        <span class="n">ij</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ij</span>
        <span class="n">F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">F</span>

        <span class="k">if</span> <span class="n">D</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Light storage mode (need to recompute D, ij and F)</span>
            <span class="n">D</span><span class="p">,</span> <span class="n">ij</span> <span class="o">=</span> <span class="n">l1_cross_distances</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="o">==</span> <span class="mf">0.</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="o">!=</span> <span class="n">correlation</span><span class="o">.</span><span class="n">pure_nugget</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Multiple X are not allowed&quot;</span><span class="p">)</span>
            <span class="n">F</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Set up R</span>
        <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">nugget</span><span class="p">)</span>
        <span class="n">R</span><span class="p">[</span><span class="n">ij</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">ij</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">r</span>
        <span class="n">R</span><span class="p">[</span><span class="n">ij</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">ij</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">r</span>

        <span class="c1"># Cholesky decomposition of R</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">C</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">reduced_likelihood_function_value</span><span class="p">,</span> <span class="n">par</span>

        <span class="c1"># Get generalized least squares solution</span>
        <span class="n">Ft</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">Q</span><span class="p">,</span> <span class="n">G</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">Ft</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;economic&#39;</span><span class="p">)</span>

        <span class="n">sv</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">rcondG</span> <span class="o">=</span> <span class="n">sv</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">sv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">rcondG</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">:</span>
            <span class="c1"># Check F</span>
            <span class="n">sv</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">condF</span> <span class="o">=</span> <span class="n">sv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">sv</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">condF</span> <span class="o">&gt;</span> <span class="mf">1e15</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;F is too ill conditioned. Poor combination &quot;</span>
                                <span class="s2">&quot;of regression model and observations.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Ft is too ill conditioned, get out (try different theta)</span>
                <span class="k">return</span> <span class="n">reduced_likelihood_function_value</span><span class="p">,</span> <span class="n">par</span>

        <span class="n">Yt</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Universal Kriging</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Yt</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Ordinary Kriging</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta0</span><span class="p">)</span>

        <span class="n">rho</span> <span class="o">=</span> <span class="n">Yt</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ft</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        <span class="n">sigma2</span> <span class="o">=</span> <span class="p">(</span><span class="n">rho</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="c1"># The determinant of R is equal to the squared product of the diagonal</span>
        <span class="c1"># elements of its Cholesky decomposition C</span>
        <span class="n">detR</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">C</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">))</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span>

        <span class="c1"># Compute/Organize output</span>
        <span class="n">reduced_likelihood_function_value</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sigma2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">detR</span>
        <span class="n">par</span><span class="p">[</span><span class="s1">&#39;sigma2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_std</span> <span class="o">**</span> <span class="mf">2.</span>
        <span class="n">par</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="n">par</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">par</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span>
        <span class="n">par</span><span class="p">[</span><span class="s1">&#39;Ft&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Ft</span>
        <span class="n">par</span><span class="p">[</span><span class="s1">&#39;G&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">G</span>

        <span class="k">return</span> <span class="n">reduced_likelihood_function_value</span><span class="p">,</span> <span class="n">par</span>

    <span class="k">def</span> <span class="nf">_arg_max_reduced_likelihood_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function estimates the autocorrelation parameters theta as the</span>
<span class="sd">        maximizer of the reduced likelihood function.</span>
<span class="sd">        (Minimization of the opposite reduced likelihood function is used for</span>
<span class="sd">        convenience)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        self : All parameters are stored in the Gaussian Process model object.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        optimal_theta : array_like</span>
<span class="sd">            The best set of autocorrelation parameters (the sought maximizer of</span>
<span class="sd">            the reduced likelihood function).</span>

<span class="sd">        optimal_reduced_likelihood_function_value : double</span>
<span class="sd">            The optimal reduced likelihood function value.</span>

<span class="sd">        optimal_par : dict</span>
<span class="sd">            The BLUP parameters associated to thetaOpt.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Initialize output</span>
        <span class="n">best_optimal_theta</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_optimal_rlf_value</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_optimal_par</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The chosen optimizer is: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_start</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_start</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; random starts are required.&quot;</span><span class="p">)</span>

        <span class="n">percent_completed</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="c1"># Force optimizer to fmin_cobyla if the model is meant to be isotropic</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;Welch&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;fmin_cobyla&#39;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;fmin_cobyla&#39;</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">minus_reduced_likelihood_function</span><span class="p">(</span><span class="n">log10t</span><span class="p">):</span>
                <span class="k">return</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function</span><span class="p">(</span>
                    <span class="n">theta</span><span class="o">=</span><span class="mf">10.</span> <span class="o">**</span> <span class="n">log10t</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">constraints</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
                <span class="n">constraints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="k">lambda</span> <span class="n">log10t</span><span class="p">,</span> <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">:</span>
                                   <span class="n">log10t</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]))</span>
                <span class="n">constraints</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="k">lambda</span> <span class="n">log10t</span><span class="p">,</span> <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="p">:</span>
                                   <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="n">log10t</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_start</span><span class="p">):</span>

                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># Use specified starting point as first guess</span>
                    <span class="n">theta0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Generate a random starting point log10-uniformly</span>
                    <span class="c1"># distributed between bounds</span>
                    <span class="n">log10theta0</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">)</span>
                                   <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                                   <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">))</span>
                    <span class="n">theta0</span> <span class="o">=</span> <span class="mf">10.</span> <span class="o">**</span> <span class="n">log10theta0</span>

                <span class="c1"># Run Cobyla</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">log10_optimal_theta</span> <span class="o">=</span> \
                        <span class="n">optimize</span><span class="o">.</span><span class="n">fmin_cobyla</span><span class="p">(</span><span class="n">minus_reduced_likelihood_function</span><span class="p">,</span>
                                             <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">theta0</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">constraints</span><span class="p">,</span>
                                             <span class="n">iprint</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">ve</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Optimization failed. Try increasing the ``nugget``&quot;</span><span class="p">)</span>
                    <span class="k">raise</span> <span class="n">ve</span>

                <span class="n">optimal_theta</span> <span class="o">=</span> <span class="mf">10.</span> <span class="o">**</span> <span class="n">log10_optimal_theta</span>
                <span class="n">optimal_rlf_value</span><span class="p">,</span> <span class="n">optimal_par</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">reduced_likelihood_function</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">optimal_theta</span><span class="p">)</span>

                <span class="c1"># Compare the new optimizer to the best previous one</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">optimal_rlf_value</span> <span class="o">&gt;</span> <span class="n">best_optimal_rlf_value</span><span class="p">:</span>
                        <span class="n">best_optimal_rlf_value</span> <span class="o">=</span> <span class="n">optimal_rlf_value</span>
                        <span class="n">best_optimal_par</span> <span class="o">=</span> <span class="n">optimal_par</span>
                        <span class="n">best_optimal_theta</span> <span class="o">=</span> <span class="n">optimal_theta</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">best_optimal_rlf_value</span> <span class="o">=</span> <span class="n">optimal_rlf_value</span>
                    <span class="n">best_optimal_par</span> <span class="o">=</span> <span class="n">optimal_par</span>
                    <span class="n">best_optimal_theta</span> <span class="o">=</span> <span class="n">optimal_theta</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_start</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_start</span> <span class="o">&gt;</span> <span class="n">percent_completed</span><span class="p">:</span>
                        <span class="n">percent_completed</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="n">k</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_start</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> completed&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">percent_completed</span><span class="p">))</span>

            <span class="n">optimal_rlf_value</span> <span class="o">=</span> <span class="n">best_optimal_rlf_value</span>
            <span class="n">optimal_par</span> <span class="o">=</span> <span class="n">best_optimal_par</span>
            <span class="n">optimal_theta</span> <span class="o">=</span> <span class="n">best_optimal_theta</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;Welch&#39;</span><span class="p">:</span>

            <span class="c1"># Backup of the given attributes</span>
            <span class="n">theta0</span><span class="p">,</span> <span class="n">thetaL</span><span class="p">,</span> <span class="n">thetaU</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span>
            <span class="n">corr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span>
            <span class="n">verbose</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span>

            <span class="c1"># This will iterate over fmin_cobyla optimizer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;fmin_cobyla&#39;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># Initialize under isotropy assumption</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initialize under isotropy assumption...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
            <span class="n">theta_iso</span><span class="p">,</span> <span class="n">optimal_rlf_value_iso</span><span class="p">,</span> <span class="n">par_iso</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">_arg_max_reduced_likelihood_function</span><span class="p">()</span>
            <span class="n">optimal_theta</span> <span class="o">=</span> <span class="n">theta_iso</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">theta0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="c1"># Iterate over all dimensions of theta allowing for anisotropy</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Now improving allowing for anisotropy...&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">theta0</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Proceeding along dimension </span><span class="si">%d</span><span class="s2">...&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">theta_iso</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">thetaL</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">thetaU</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>

                <span class="k">def</span> <span class="nf">corr_cut</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">corr</span><span class="p">(</span><span class="n">check_array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">optimal_theta</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">i</span><span class="p">],</span>
                                                       <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                       <span class="n">optimal_theta</span><span class="p">[</span><span class="mi">0</span><span class="p">][(</span><span class="n">i</span> <span class="o">+</span>
                                                                         <span class="mi">1</span><span class="p">)::]])),</span>
                                <span class="n">d</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="o">=</span> <span class="n">corr_cut</span>
                <span class="n">optimal_theta</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">optimal_rlf_value</span><span class="p">,</span> <span class="n">optimal_par</span> <span class="o">=</span> \
                    <span class="bp">self</span><span class="o">.</span><span class="n">_arg_max_reduced_likelihood_function</span><span class="p">()</span>

            <span class="c1"># Restore the given attributes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">=</span> <span class="n">theta0</span><span class="p">,</span> <span class="n">thetaL</span><span class="p">,</span> <span class="n">thetaU</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="o">=</span> <span class="n">corr</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;Welch&#39;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

        <span class="k">else</span><span class="p">:</span>

            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;This optimizer (&#39;</span><span class="si">%s</span><span class="s2">&#39;) is not &quot;</span>
                                      <span class="s2">&quot;implemented yet. Please contribute!&quot;</span>
                                      <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">optimal_theta</span><span class="p">,</span> <span class="n">optimal_rlf_value</span><span class="p">,</span> <span class="n">optimal_par</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="c1"># Check regression model</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">regr</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">regr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regression_types</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">regr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regression_types</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">regr</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;regr should be one of </span><span class="si">%s</span><span class="s2"> or callable, &quot;</span>
                                 <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> was given.&quot;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_regression_types</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">regr</span><span class="p">))</span>

        <span class="c1"># Check regression weights if given (Ordinary Kriging)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta0</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Force to column vector</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta0</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Check correlation model</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corr</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_correlation_types</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">corr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_correlation_types</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">corr</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;corr should be one of </span><span class="si">%s</span><span class="s2"> or callable, &quot;</span>
                                 <span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> was given.&quot;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_correlation_types</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">corr</span><span class="p">))</span>

        <span class="c1"># Check storage mode</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_mode</span> <span class="o">!=</span> <span class="s1">&#39;full&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_mode</span> <span class="o">!=</span> <span class="s1">&#39;light&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Storage mode should either be &#39;full&#39; or &quot;</span>
                             <span class="s2">&quot;&#39;light&#39;, </span><span class="si">%s</span><span class="s2"> was given.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_mode</span><span class="p">)</span>

        <span class="c1"># Check correlation parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="p">)</span>
        <span class="n">lth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta0</span><span class="o">.</span><span class="n">size</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">lth</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="n">lth</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;theta0, thetaL and thetaU must have the &quot;</span>
                                 <span class="s2">&quot;same length.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The bounds must satisfy O &lt; thetaL &lt;= &quot;</span>
                                 <span class="s2">&quot;thetaU.&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta0</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;theta0 must be strictly positive.&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaL</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">thetaU</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;thetaL and thetaU should either be both or &quot;</span>
                             <span class="s2">&quot;neither specified.&quot;</span><span class="p">)</span>

        <span class="c1"># Force verbose type to bool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>

        <span class="c1"># Force normalize type to bool</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">)</span>

        <span class="c1"># Check nugget value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nugget</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nugget</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nugget</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;nugget must be positive or zero.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">n_samples</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">nugget</span><span class="o">.</span><span class="n">shape</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[(),</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,)]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;nugget must be either a scalar &quot;</span>
                             <span class="s2">&quot;or array of length n_samples.&quot;</span><span class="p">)</span>

        <span class="c1"># Check optimizer</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_types</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;optimizer should be one of </span><span class="si">%s</span><span class="s2">&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_types</span><span class="p">)</span>

        <span class="c1"># Force random_start type to int</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_start</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_start</span><span class="p">)</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>