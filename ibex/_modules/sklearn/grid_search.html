
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.grid_search &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.grid_search</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The :mod:`sklearn.grid_search` includes utilities to fine-tune the parameters</span>
<span class="sd">of an estimator.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="c1"># Author: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;,</span>
<span class="c1">#         Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="c1">#         Andreas Mueller &lt;amueller@ais.uni-bonn.de&gt;</span>
<span class="c1">#         Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">namedtuple</span><span class="p">,</span> <span class="n">Sized</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="k">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">reduce</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">is_classifier</span><span class="p">,</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">MetaEstimatorMixin</span>
<span class="kn">from</span> <span class="nn">.cross_validation</span> <span class="k">import</span> <span class="n">check_cv</span>
<span class="kn">from</span> <span class="nn">.cross_validation</span> <span class="k">import</span> <span class="n">_fit_and_score</span>
<span class="kn">from</span> <span class="nn">.externals.joblib</span> <span class="k">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">.externals</span> <span class="k">import</span> <span class="n">six</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">.utils.random</span> <span class="k">import</span> <span class="n">sample_without_replacement</span>
<span class="kn">from</span> <span class="nn">.utils.validation</span> <span class="k">import</span> <span class="n">_num_samples</span><span class="p">,</span> <span class="n">indexable</span>
<span class="kn">from</span> <span class="nn">.utils.metaestimators</span> <span class="k">import</span> <span class="n">if_delegate_has_method</span>
<span class="kn">from</span> <span class="nn">.metrics.scorer</span> <span class="k">import</span> <span class="n">check_scoring</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GridSearchCV&#39;</span><span class="p">,</span> <span class="s1">&#39;ParameterGrid&#39;</span><span class="p">,</span> <span class="s1">&#39;fit_grid_point&#39;</span><span class="p">,</span>
           <span class="s1">&#39;ParameterSampler&#39;</span><span class="p">,</span> <span class="s1">&#39;RandomizedSearchCV&#39;</span><span class="p">]</span>


<span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;This module was deprecated in version 0.18 in favor of the &quot;</span>
              <span class="s2">&quot;model_selection module into which all the refactored classes &quot;</span>
              <span class="s2">&quot;and functions are moved. This module will be removed in 0.20.&quot;</span><span class="p">,</span>
              <span class="ne">DeprecationWarning</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ParameterGrid</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Grid of parameters with a discrete number of values for each.</span>

<span class="sd">    .. deprecated:: 0.18</span>
<span class="sd">        This module will be removed in 0.20.</span>
<span class="sd">        Use :class:`sklearn.model_selection.ParameterGrid` instead.</span>

<span class="sd">    Can be used to iterate over parameter value combinations with the</span>
<span class="sd">    Python built-in function iter.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;grid_search&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    param_grid : dict of string to sequence, or sequence of such</span>
<span class="sd">        The parameter grid to explore, as a dictionary mapping estimator</span>
<span class="sd">        parameters to sequences of allowed values.</span>

<span class="sd">        An empty dict signifies default parameters.</span>

<span class="sd">        A sequence of dicts signifies a sequence of grids to search, and is</span>
<span class="sd">        useful to avoid exploring parameter combinations that make no sense</span>
<span class="sd">        or have no effect. See the examples below.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.grid_search import ParameterGrid</span>
<span class="sd">    &gt;&gt;&gt; param_grid = {&#39;a&#39;: [1, 2], &#39;b&#39;: [True, False]}</span>
<span class="sd">    &gt;&gt;&gt; list(ParameterGrid(param_grid)) == (</span>
<span class="sd">    ...    [{&#39;a&#39;: 1, &#39;b&#39;: True}, {&#39;a&#39;: 1, &#39;b&#39;: False},</span>
<span class="sd">    ...     {&#39;a&#39;: 2, &#39;b&#39;: True}, {&#39;a&#39;: 2, &#39;b&#39;: False}])</span>
<span class="sd">    True</span>

<span class="sd">    &gt;&gt;&gt; grid = [{&#39;kernel&#39;: [&#39;linear&#39;]}, {&#39;kernel&#39;: [&#39;rbf&#39;], &#39;gamma&#39;: [1, 10]}]</span>
<span class="sd">    &gt;&gt;&gt; list(ParameterGrid(grid)) == [{&#39;kernel&#39;: &#39;linear&#39;},</span>
<span class="sd">    ...                               {&#39;kernel&#39;: &#39;rbf&#39;, &#39;gamma&#39;: 1},</span>
<span class="sd">    ...                               {&#39;kernel&#39;: &#39;rbf&#39;, &#39;gamma&#39;: 10}]</span>
<span class="sd">    True</span>
<span class="sd">    &gt;&gt;&gt; ParameterGrid(grid)[1] == {&#39;kernel&#39;: &#39;rbf&#39;, &#39;gamma&#39;: 1}</span>
<span class="sd">    True</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    :class:`GridSearchCV`:</span>
<span class="sd">        uses ``ParameterGrid`` to perform a full parallelized parameter search.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
            <span class="c1"># wrap dictionary in a singleton list to support either dict</span>
            <span class="c1"># or list of dicts</span>
            <span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_grid</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterate over the points in the grid.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params : iterator over dict of string to any</span>
<span class="sd">            Yields dictionaries mapping each estimator parameter to one of its</span>
<span class="sd">            allowed values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span><span class="p">:</span>
            <span class="c1"># Always sort the keys of a dictionary, for reproducibility</span>
            <span class="n">items</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">items</span><span class="p">:</span>
                <span class="k">yield</span> <span class="p">{}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">keys</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">items</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">values</span><span class="p">):</span>
                    <span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
                    <span class="k">yield</span> <span class="n">params</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Number of points on the grid.&quot;&quot;&quot;</span>
        <span class="c1"># Product function that can handle iterables (np.product can&#39;t).</span>
        <span class="n">product</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">reduce</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="k">if</span> <span class="n">p</span> <span class="k">else</span> <span class="mi">1</span>
                   <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ind</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get the parameters that would be ``ind``th in iteration</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ind : int</span>
<span class="sd">            The iteration index</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params : dict of string to any</span>
<span class="sd">            Equal to list(self)[ind]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># This is used to make discrete sampling without replacement memory</span>
        <span class="c1"># efficient.</span>
        <span class="k">for</span> <span class="n">sub_grid</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span><span class="p">:</span>
            <span class="c1"># XXX: could memoize information used here</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">sub_grid</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">ind</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">{}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ind</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

            <span class="c1"># Reverse so most frequent cycling parameter comes first</span>
            <span class="n">keys</span><span class="p">,</span> <span class="n">values_lists</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="nb">sorted</span><span class="p">(</span><span class="n">sub_grid</span><span class="o">.</span><span class="n">items</span><span class="p">())[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">v_list</span><span class="p">)</span> <span class="k">for</span> <span class="n">v_list</span> <span class="ow">in</span> <span class="n">values_lists</span><span class="p">]</span>
            <span class="n">total</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="n">total</span><span class="p">:</span>
                <span class="c1"># Try the next grid</span>
                <span class="n">ind</span> <span class="o">-=</span> <span class="n">total</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">v_list</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">values_lists</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
                    <span class="n">ind</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
                    <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_list</span><span class="p">[</span><span class="n">offset</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">out</span>

        <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="s1">&#39;ParameterGrid index out of range&#39;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ParameterSampler</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generator on parameters sampled from given distributions.</span>

<span class="sd">    .. deprecated:: 0.18</span>
<span class="sd">        This module will be removed in 0.20.</span>
<span class="sd">        Use :class:`sklearn.model_selection.ParameterSampler` instead.</span>

<span class="sd">    Non-deterministic iterable over random candidate combinations for hyper-</span>
<span class="sd">    parameter search. If all parameters are presented as a list,</span>
<span class="sd">    sampling without replacement is performed. If at least one parameter</span>
<span class="sd">    is given as a distribution, sampling with replacement is used.</span>
<span class="sd">    It is highly recommended to use continuous distributions for continuous</span>
<span class="sd">    parameters.</span>

<span class="sd">    Note that as of SciPy 0.12, the ``scipy.stats.distributions`` do not accept</span>
<span class="sd">    a custom RNG instance and always use the singleton RNG from</span>
<span class="sd">    ``numpy.random``. Hence setting ``random_state`` will not guarantee a</span>
<span class="sd">    deterministic iteration whenever ``scipy.stats`` distributions are used to</span>
<span class="sd">    define the parameter search space.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;grid_search&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    param_distributions : dict</span>
<span class="sd">        Dictionary where the keys are parameters and values</span>
<span class="sd">        are distributions from which a parameter is to be sampled.</span>
<span class="sd">        Distributions either have to provide a ``rvs`` function</span>
<span class="sd">        to sample from them, or can be given as a list of values,</span>
<span class="sd">        where a uniform distribution is assumed.</span>

<span class="sd">    n_iter : integer</span>
<span class="sd">        Number of parameter settings that are produced.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        Pseudo random number generator state used for random uniform sampling</span>
<span class="sd">        from lists of possible values instead of scipy.stats distributions.</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : dict of string to any</span>
<span class="sd">        **Yields** dictionaries mapping each estimator parameter to</span>
<span class="sd">        as sampled value.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.grid_search import ParameterSampler</span>
<span class="sd">    &gt;&gt;&gt; from scipy.stats.distributions import expon</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">    &gt;&gt;&gt; param_grid = {&#39;a&#39;:[1, 2], &#39;b&#39;: expon()}</span>
<span class="sd">    &gt;&gt;&gt; param_list = list(ParameterSampler(param_grid, n_iter=4))</span>
<span class="sd">    &gt;&gt;&gt; rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())</span>
<span class="sd">    ...                 for d in param_list]</span>
<span class="sd">    &gt;&gt;&gt; rounded_list == [{&#39;b&#39;: 0.89856, &#39;a&#39;: 1},</span>
<span class="sd">    ...                  {&#39;b&#39;: 0.923223, &#39;a&#39;: 1},</span>
<span class="sd">    ...                  {&#39;b&#39;: 1.878964, &#39;a&#39;: 2},</span>
<span class="sd">    ...                  {&#39;b&#39;: 1.038159, &#39;a&#39;: 2}]</span>
<span class="sd">    True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_distributions</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span> <span class="o">=</span> <span class="n">param_distributions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># check if all distributions are given as lists</span>
        <span class="c1"># in this case we want to sample without replacement</span>
        <span class="n">all_lists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">([</span><span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;rvs&quot;</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
        <span class="n">rnd</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">all_lists</span><span class="p">:</span>
            <span class="c1"># look up sampled parameter settings in parameter grid</span>
            <span class="n">param_grid</span> <span class="o">=</span> <span class="n">ParameterGrid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span><span class="p">)</span>
            <span class="n">grid_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">grid_size</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The total space of parameters </span><span class="si">%d</span><span class="s2"> is smaller &quot;</span>
                    <span class="s2">&quot;than n_iter=</span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot; For exhaustive searches, use GridSearchCV.&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sample_without_replacement</span><span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">,</span>
                                                <span class="n">random_state</span><span class="o">=</span><span class="n">rnd</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">param_grid</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Always sort the keys of a dictionary, for reproducibility</span>
            <span class="n">items</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">moves</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
                <span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;rvs&quot;</span><span class="p">):</span>
                        <span class="n">params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">params</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="n">rnd</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">))]</span>
                <span class="k">yield</span> <span class="n">params</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Number of points that will be sampled.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span>


<span class="k">def</span> <span class="nf">fit_grid_point</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">scorer</span><span class="p">,</span>
                   <span class="n">verbose</span><span class="p">,</span> <span class="n">error_score</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run fit on one set of parameters.</span>

<span class="sd">    .. deprecated:: 0.18</span>
<span class="sd">        This module will be removed in 0.20.</span>
<span class="sd">        Use :func:`sklearn.model_selection.fit_grid_point` instead.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like, sparse matrix or list</span>
<span class="sd">        Input data.</span>

<span class="sd">    y : array-like or None</span>
<span class="sd">        Targets for input data.</span>

<span class="sd">    estimator : estimator object</span>
<span class="sd">        A object of that type is instantiated for each grid point.</span>
<span class="sd">        This is assumed to implement the scikit-learn estimator interface.</span>
<span class="sd">        Either estimator needs to provide a ``score`` function,</span>
<span class="sd">        or ``scoring`` must be passed.</span>

<span class="sd">    parameters : dict</span>
<span class="sd">        Parameters to be set on estimator for this grid point.</span>

<span class="sd">    train : ndarray, dtype int or bool</span>
<span class="sd">        Boolean mask or indices for training set.</span>

<span class="sd">    test : ndarray, dtype int or bool</span>
<span class="sd">        Boolean mask or indices for test set.</span>

<span class="sd">    scorer : callable or None.</span>
<span class="sd">        If provided must be a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        Verbosity level.</span>

<span class="sd">    **fit_params : kwargs</span>
<span class="sd">        Additional parameter passed to the fit function of the estimator.</span>

<span class="sd">    error_score : &#39;raise&#39; (default) or numeric</span>
<span class="sd">        Value to assign to the score if an error occurs in estimator fitting.</span>
<span class="sd">        If set to &#39;raise&#39;, the error is raised. If a numeric value is given,</span>
<span class="sd">        FitFailedWarning is raised. This parameter does not affect the refit</span>
<span class="sd">        step, which will always raise the error.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        Score of this parameter setting on given training / test split.</span>

<span class="sd">    parameters : dict</span>
<span class="sd">        The parameters that have been evaluated.</span>

<span class="sd">    n_samples_test : int</span>
<span class="sd">        Number of test samples in this split.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">score</span><span class="p">,</span> <span class="n">n_samples_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_fit_and_score</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scorer</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span>
                                              <span class="n">test</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span>
                                              <span class="n">fit_params</span><span class="p">,</span> <span class="n">error_score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">n_samples_test</span>


<span class="k">def</span> <span class="nf">_check_param_grid</span><span class="p">(</span><span class="n">param_grid</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param_grid</span><span class="p">,</span> <span class="s1">&#39;items&#39;</span><span class="p">):</span>
        <span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_grid</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">and</span> <span class="n">v</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Parameter array should be one-dimensional.&quot;</span><span class="p">)</span>

            <span class="n">check</span> <span class="o">=</span> <span class="p">[</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)]</span>
            <span class="k">if</span> <span class="kc">True</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">check</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Parameter values for parameter (</span><span class="si">{0}</span><span class="s2">) need &quot;</span>
                                 <span class="s2">&quot;to be a sequence.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Parameter values for parameter (</span><span class="si">{0}</span><span class="s2">) need &quot;</span>
                                 <span class="s2">&quot;to be a non-empty sequence.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">_CVScoreTuple</span> <span class="p">(</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;_CVScoreTuple&#39;</span><span class="p">,</span>
                                <span class="p">(</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span>
                                 <span class="s1">&#39;mean_validation_score&#39;</span><span class="p">,</span>
                                 <span class="s1">&#39;cv_validation_scores&#39;</span><span class="p">))):</span>
    <span class="c1"># A raw namedtuple is very memory efficient as it packs the attributes</span>
    <span class="c1"># in a struct to get rid of the __dict__ of attributes in particular it</span>
    <span class="c1"># does not copy the string for the keys on each instance.</span>
    <span class="c1"># By deriving a namedtuple class just to introduce the __repr__ method we</span>
    <span class="c1"># would also reintroduce the __dict__ on the instance. By telling the</span>
    <span class="c1"># Python interpreter that this subclass uses static __slots__ instead of</span>
    <span class="c1"># dynamic attributes. Furthermore we don&#39;t need any additional slot in the</span>
    <span class="c1"># subclass so we set __slots__ to the empty tuple.</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">()</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Simple custom repr to summarize the main info&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;mean: </span><span class="si">{0:.5f}</span><span class="s2">, std: </span><span class="si">{1:.5f}</span><span class="s2">, params: </span><span class="si">{2}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean_validation_score</span><span class="p">,</span>
            <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv_validation_scores</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BaseSearchCV</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">,</span>
                                      <span class="n">MetaEstimatorMixin</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for hyper parameter search with cross-validation.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">fit_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="s1">&#39;2*n_jobs&#39;</span><span class="p">,</span>
                 <span class="n">error_score</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span> <span class="o">=</span> <span class="n">fit_params</span> <span class="k">if</span> <span class="n">fit_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iid</span> <span class="o">=</span> <span class="n">iid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">refit</span> <span class="o">=</span> <span class="n">refit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span> <span class="o">=</span> <span class="n">pre_dispatch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">error_score</span> <span class="o">=</span> <span class="n">error_score</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_estimator_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">_estimator_type</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">classes_</span>

    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the score on the given data, if the estimator has been refit.</span>

<span class="sd">        This uses the score defined by ``scoring`` where provided, and the</span>
<span class="sd">        ``best_estimator_.score`` method otherwise.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Input data, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_output], optional</span>
<span class="sd">            Target relative to X for classification or regression;</span>
<span class="sd">            None for unsupervised learning.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : float</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">         * The long-standing behavior of this method changed in version 0.16.</span>
<span class="sd">         * It no longer uses the metric provided by ``estimator.score`` if the</span>
<span class="sd">           ``scoring`` parameter was set when fitting.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No score function explicitly defined, &quot;</span>
                             <span class="s2">&quot;and the estimator doesn&#39;t provide one </span><span class="si">%s</span><span class="s2">&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;best_estimator_&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Call predict on the estimator with the best found parameters.</span>

<span class="sd">        Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">        ``predict``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        X : indexable, length n_samples</span>
<span class="sd">            Must fulfill the input assumptions of the</span>
<span class="sd">            underlying estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;best_estimator_&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Call predict_proba on the estimator with the best found parameters.</span>

<span class="sd">        Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">        ``predict_proba``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        X : indexable, length n_samples</span>
<span class="sd">            Must fulfill the input assumptions of the</span>
<span class="sd">            underlying estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;best_estimator_&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Call predict_log_proba on the estimator with the best found parameters.</span>

<span class="sd">        Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">        ``predict_log_proba``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        X : indexable, length n_samples</span>
<span class="sd">            Must fulfill the input assumptions of the</span>
<span class="sd">            underlying estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict_log_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;best_estimator_&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Call decision_function on the estimator with the best found parameters.</span>

<span class="sd">        Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">        ``decision_function``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        X : indexable, length n_samples</span>
<span class="sd">            Must fulfill the input assumptions of the</span>
<span class="sd">            underlying estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;best_estimator_&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Call transform on the estimator with the best found parameters.</span>

<span class="sd">        Only available if the underlying estimator supports ``transform`` and</span>
<span class="sd">        ``refit=True``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        X : indexable, length n_samples</span>
<span class="sd">            Must fulfill the input assumptions of the</span>
<span class="sd">            underlying estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;best_estimator_&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">))</span>
    <span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Call inverse_transform on the estimator with the best found parameters.</span>

<span class="sd">        Only available if the underlying estimator implements ``inverse_transform`` and</span>
<span class="sd">        ``refit=True``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        -----------</span>
<span class="sd">        Xt : indexable, length n_samples</span>
<span class="sd">            Must fulfill the input assumptions of the</span>
<span class="sd">            underlying estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">parameter_iterable</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Actual fitting,  performing the search over parameters.&quot;&quot;&quot;</span>

        <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span> <span class="o">=</span> <span class="n">check_scoring</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">)</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">_num_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">indexable</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Target variable (y) has a different number &#39;</span>
                                 <span class="s1">&#39;of samples (</span><span class="si">%i</span><span class="s1">) than data (X: </span><span class="si">%i</span><span class="s1"> samples)&#39;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">n_samples</span><span class="p">))</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="n">cv</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="n">estimator</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parameter_iterable</span><span class="p">,</span> <span class="n">Sized</span><span class="p">):</span>
                <span class="n">n_candidates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameter_iterable</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitting </span><span class="si">{0}</span><span class="s2"> folds for each of </span><span class="si">{1}</span><span class="s2"> candidates, totalling&quot;</span>
                      <span class="s2">&quot; </span><span class="si">{2}</span><span class="s2"> fits&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="p">),</span> <span class="n">n_candidates</span><span class="p">,</span>
                                         <span class="n">n_candidates</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="p">)))</span>

        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span>

        <span class="n">pre_dispatch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span>
        <span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_and_score</span><span class="p">)(</span><span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span><span class="p">,</span>
                                    <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span><span class="p">,</span> <span class="n">return_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                    <span class="n">error_score</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">error_score</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">parameter_iterable</span>
                <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="p">)</span>

        <span class="c1"># Out is a list of triplet: score, estimator, n_test_samples</span>
        <span class="n">n_fits</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">n_folds</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="n">grid_scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">grid_start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_fits</span><span class="p">,</span> <span class="n">n_folds</span><span class="p">):</span>
            <span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">all_scores</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">this_score</span><span class="p">,</span> <span class="n">this_n_test_samples</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">parameters</span> <span class="ow">in</span> \
                    <span class="n">out</span><span class="p">[</span><span class="n">grid_start</span><span class="p">:</span><span class="n">grid_start</span> <span class="o">+</span> <span class="n">n_folds</span><span class="p">]:</span>
                <span class="n">all_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">this_score</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iid</span><span class="p">:</span>
                    <span class="n">this_score</span> <span class="o">*=</span> <span class="n">this_n_test_samples</span>
                    <span class="n">n_test_samples</span> <span class="o">+=</span> <span class="n">this_n_test_samples</span>
                <span class="n">score</span> <span class="o">+=</span> <span class="n">this_score</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iid</span><span class="p">:</span>
                <span class="n">score</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_test_samples</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">score</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_folds</span><span class="p">)</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">score</span><span class="p">,</span> <span class="n">parameters</span><span class="p">))</span>
            <span class="c1"># TODO: shall we also store the test_fold_sizes?</span>
            <span class="n">grid_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_CVScoreTuple</span><span class="p">(</span>
                <span class="n">parameters</span><span class="p">,</span>
                <span class="n">score</span><span class="p">,</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">all_scores</span><span class="p">)))</span>
        <span class="c1"># Store the computed scores</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_scores_</span> <span class="o">=</span> <span class="n">grid_scores</span>

        <span class="c1"># Find the best parameters by comparing on the mean validation score:</span>
        <span class="c1"># note that `sorted` is deterministic in the way it breaks ties</span>
        <span class="n">best</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">grid_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">mean_validation_score</span><span class="p">,</span>
                      <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_params_</span> <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_score_</span> <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">mean_validation_score</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
            <span class="c1"># fit the best estimator using the entire dataset</span>
            <span class="c1"># clone first to work around broken estimators</span>
            <span class="n">best_estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
                <span class="o">**</span><span class="n">best</span><span class="o">.</span><span class="n">parameters</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">best_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">best_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span> <span class="o">=</span> <span class="n">best_estimator</span>
        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">BaseSearchCV</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Exhaustive search over specified parameter values for an estimator.</span>

<span class="sd">    .. deprecated:: 0.18</span>
<span class="sd">        This module will be removed in 0.20.</span>
<span class="sd">        Use :class:`sklearn.model_selection.GridSearchCV` instead.</span>

<span class="sd">    Important members are fit, predict.</span>

<span class="sd">    GridSearchCV implements a &quot;fit&quot; and a &quot;score&quot; method.</span>
<span class="sd">    It also implements &quot;predict&quot;, &quot;predict_proba&quot;, &quot;decision_function&quot;,</span>
<span class="sd">    &quot;transform&quot; and &quot;inverse_transform&quot; if they are implemented in the</span>
<span class="sd">    estimator used.</span>

<span class="sd">    The parameters of the estimator used to apply these methods are optimized</span>
<span class="sd">    by cross-validated grid-search over a parameter grid.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;grid_search&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator object.</span>
<span class="sd">        A object of that type is instantiated for each grid point.</span>
<span class="sd">        This is assumed to implement the scikit-learn estimator interface.</span>
<span class="sd">        Either estimator needs to provide a ``score`` function,</span>
<span class="sd">        or ``scoring`` must be passed.</span>

<span class="sd">    param_grid : dict or list of dictionaries</span>
<span class="sd">        Dictionary with parameters names (string) as keys and lists of</span>
<span class="sd">        parameter settings to try as values, or a list of such</span>
<span class="sd">        dictionaries, in which case the grids spanned by each dictionary</span>
<span class="sd">        in the list are explored. This enables searching over any sequence</span>
<span class="sd">        of parameter settings.</span>

<span class="sd">    scoring : string, callable or None, default=None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``.</span>
<span class="sd">        If ``None``, the ``score`` method of the estimator is used.</span>

<span class="sd">    fit_params : dict, optional</span>
<span class="sd">        Parameters to pass to the fit method.</span>

<span class="sd">    n_jobs: int, default: 1 :</span>
<span class="sd">        The maximum number of estimators fit in parallel.</span>

<span class="sd">            - If -1 all CPUs are used.</span>

<span class="sd">            - If 1 is given, no parallel computing code is used at all,</span>
<span class="sd">              which is useful for debugging.</span>

<span class="sd">            - For ``n_jobs`` below -1, ``(n_cpus + n_jobs + 1)`` are used.</span>
<span class="sd">              For example, with ``n_jobs = -2`` all CPUs but one are used.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           Upgraded to joblib 0.9.3.</span>

<span class="sd">    pre_dispatch : int, or string, optional</span>
<span class="sd">        Controls the number of jobs that get dispatched during parallel</span>
<span class="sd">        execution. Reducing this number can be useful to avoid an</span>
<span class="sd">        explosion of memory consumption when more jobs get dispatched</span>
<span class="sd">        than CPUs can process. This parameter can be:</span>

<span class="sd">            - None, in which case all the jobs are immediately</span>
<span class="sd">              created and spawned. Use this for lightweight and</span>
<span class="sd">              fast-running jobs, to avoid delays due to on-demand</span>
<span class="sd">              spawning of the jobs</span>

<span class="sd">            - An int, giving the exact number of total jobs that are</span>
<span class="sd">              spawned</span>

<span class="sd">            - A string, giving an expression as a function of n_jobs,</span>
<span class="sd">              as in &#39;2*n_jobs&#39;</span>

<span class="sd">    iid : boolean, default=True</span>
<span class="sd">        If True, the data is assumed to be identically distributed across</span>
<span class="sd">        the folds, and the loss minimized is the total loss per sample,</span>
<span class="sd">        and not the mean loss across the folds.</span>

<span class="sd">    cv : int, cross-validation generator or an iterable, optional</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>
<span class="sd">        Possible inputs for cv are:</span>

<span class="sd">        - None, to use the default 3-fold cross-validation,</span>
<span class="sd">        - integer, to specify the number of folds.</span>
<span class="sd">        - An object to be used as a cross-validation generator.</span>
<span class="sd">        - An iterable yielding train/test splits.</span>

<span class="sd">        For integer/None inputs, if the estimator is a classifier and ``y`` is</span>
<span class="sd">        either binary or multiclass,</span>
<span class="sd">        :class:`sklearn.model_selection.StratifiedKFold` is used. In all</span>
<span class="sd">        other cases, :class:`sklearn.model_selection.KFold` is used.</span>

<span class="sd">        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various</span>
<span class="sd">        cross-validation strategies that can be used here.</span>

<span class="sd">    refit : boolean, default=True</span>
<span class="sd">        Refit the best estimator with the entire dataset.</span>
<span class="sd">        If &quot;False&quot;, it is impossible to make predictions using</span>
<span class="sd">        this GridSearchCV instance after fitting.</span>

<span class="sd">    verbose : integer</span>
<span class="sd">        Controls the verbosity: the higher, the more messages.</span>

<span class="sd">    error_score : &#39;raise&#39; (default) or numeric</span>
<span class="sd">        Value to assign to the score if an error occurs in estimator fitting.</span>
<span class="sd">        If set to &#39;raise&#39;, the error is raised. If a numeric value is given,</span>
<span class="sd">        FitFailedWarning is raised. This parameter does not affect the refit</span>
<span class="sd">        step, which will always raise the error.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn import svm, grid_search, datasets</span>
<span class="sd">    &gt;&gt;&gt; iris = datasets.load_iris()</span>
<span class="sd">    &gt;&gt;&gt; parameters = {&#39;kernel&#39;:(&#39;linear&#39;, &#39;rbf&#39;), &#39;C&#39;:[1, 10]}</span>
<span class="sd">    &gt;&gt;&gt; svr = svm.SVC()</span>
<span class="sd">    &gt;&gt;&gt; clf = grid_search.GridSearchCV(svr, parameters)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(iris.data, iris.target)</span>
<span class="sd">    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS</span>
<span class="sd">    GridSearchCV(cv=None, error_score=...,</span>
<span class="sd">           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,</span>
<span class="sd">                         decision_function_shape=&#39;ovr&#39;, degree=..., gamma=...,</span>
<span class="sd">                         kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="sd">                         random_state=None, shrinking=True, tol=...,</span>
<span class="sd">                         verbose=False),</span>
<span class="sd">           fit_params={}, iid=..., n_jobs=1,</span>
<span class="sd">           param_grid=..., pre_dispatch=..., refit=...,</span>
<span class="sd">           scoring=..., verbose=...)</span>


<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    grid_scores_ : list of named tuples</span>
<span class="sd">        Contains scores for all parameter combinations in param_grid.</span>
<span class="sd">        Each entry corresponds to one parameter setting.</span>
<span class="sd">        Each named tuple has the attributes:</span>

<span class="sd">            * ``parameters``, a dict of parameter settings</span>
<span class="sd">            * ``mean_validation_score``, the mean score over the</span>
<span class="sd">              cross-validation folds</span>
<span class="sd">            * ``cv_validation_scores``, the list of scores for each fold</span>

<span class="sd">    best_estimator_ : estimator</span>
<span class="sd">        Estimator that was chosen by the search, i.e. estimator</span>
<span class="sd">        which gave highest score (or smallest loss if specified)</span>
<span class="sd">        on the left out data. Not available if refit=False.</span>

<span class="sd">    best_score_ : float</span>
<span class="sd">        Score of best_estimator on the left out data.</span>

<span class="sd">    best_params_ : dict</span>
<span class="sd">        Parameter setting that gave the best results on the hold out data.</span>

<span class="sd">    scorer_ : function</span>
<span class="sd">        Scorer function used on the held out data to choose the best</span>
<span class="sd">        parameters for the model.</span>

<span class="sd">    Notes</span>
<span class="sd">    ------</span>
<span class="sd">    The parameters selected are those that maximize the score of the left out</span>
<span class="sd">    data, unless an explicit score is passed in which case it is used instead.</span>

<span class="sd">    If `n_jobs` was set to a value higher than one, the data is copied for each</span>
<span class="sd">    point in the grid (and not `n_jobs` times). This is done for efficiency</span>
<span class="sd">    reasons if individual jobs take very little time, but may raise errors if</span>
<span class="sd">    the dataset is large and not enough memory is available.  A workaround in</span>
<span class="sd">    this case is to set `pre_dispatch`. Then, the memory is copied only</span>
<span class="sd">    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *</span>
<span class="sd">    n_jobs`.</span>

<span class="sd">    See Also</span>
<span class="sd">    ---------</span>
<span class="sd">    :class:`ParameterGrid`:</span>
<span class="sd">        generates all the combinations of a hyperparameter grid.</span>

<span class="sd">    :func:`sklearn.cross_validation.train_test_split`:</span>
<span class="sd">        utility function to split the data into a development set usable</span>
<span class="sd">        for fitting a GridSearchCV instance and an evaluation set for</span>
<span class="sd">        its final evaluation.</span>

<span class="sd">    :func:`sklearn.metrics.make_scorer`:</span>
<span class="sd">        Make a scorer from a performance metric or loss function.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pre_dispatch</span><span class="o">=</span><span class="s1">&#39;2*n_jobs&#39;</span><span class="p">,</span> <span class="n">error_score</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">GridSearchCV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">fit_params</span><span class="p">,</span> <span class="n">n_jobs</span><span class="p">,</span> <span class="n">iid</span><span class="p">,</span>
            <span class="n">refit</span><span class="p">,</span> <span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="p">,</span> <span class="n">error_score</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid</span>
        <span class="n">_check_param_grid</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)</span>

<div class="viewcode-block" id="GridSearchCV.fit"><a class="viewcode-back" href="../../tmp/api_ibex_sklearn_grid_search_gridsearchcv.html#ibex.sklearn.grid_search.GridSearchCV.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run fit with all sets of parameters.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Training vector, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_output], optional</span>
<span class="sd">            Target relative to X for classification or regression;</span>
<span class="sd">            None for unsupervised learning.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ParameterGrid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_grid</span><span class="p">))</span></div>


<span class="k">class</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span><span class="n">BaseSearchCV</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Randomized search on hyper parameters.</span>

<span class="sd">    .. deprecated:: 0.18</span>
<span class="sd">        This module will be removed in 0.20.</span>
<span class="sd">        Use :class:`sklearn.model_selection.RandomizedSearchCV` instead.</span>

<span class="sd">    RandomizedSearchCV implements a &quot;fit&quot; and a &quot;score&quot; method.</span>
<span class="sd">    It also implements &quot;predict&quot;, &quot;predict_proba&quot;, &quot;decision_function&quot;,</span>
<span class="sd">    &quot;transform&quot; and &quot;inverse_transform&quot; if they are implemented in the</span>
<span class="sd">    estimator used.</span>

<span class="sd">    The parameters of the estimator used to apply these methods are optimized</span>
<span class="sd">    by cross-validated search over parameter settings.</span>

<span class="sd">    In contrast to GridSearchCV, not all parameter values are tried out, but</span>
<span class="sd">    rather a fixed number of parameter settings is sampled from the specified</span>
<span class="sd">    distributions. The number of parameter settings that are tried is</span>
<span class="sd">    given by n_iter.</span>

<span class="sd">    If all parameters are presented as a list,</span>
<span class="sd">    sampling without replacement is performed. If at least one parameter</span>
<span class="sd">    is given as a distribution, sampling with replacement is used.</span>
<span class="sd">    It is highly recommended to use continuous distributions for continuous</span>
<span class="sd">    parameters.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;randomized_parameter_search&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator object.</span>
<span class="sd">        A object of that type is instantiated for each grid point.</span>
<span class="sd">        This is assumed to implement the scikit-learn estimator interface.</span>
<span class="sd">        Either estimator needs to provide a ``score`` function,</span>
<span class="sd">        or ``scoring`` must be passed.</span>

<span class="sd">    param_distributions : dict</span>
<span class="sd">        Dictionary with parameters names (string) as keys and distributions</span>
<span class="sd">        or lists of parameters to try. Distributions must provide a ``rvs``</span>
<span class="sd">        method for sampling (such as those from scipy.stats.distributions).</span>
<span class="sd">        If a list is given, it is sampled uniformly.</span>

<span class="sd">    n_iter : int, default=10</span>
<span class="sd">        Number of parameter settings that are sampled. n_iter trades</span>
<span class="sd">        off runtime vs quality of the solution.</span>

<span class="sd">    scoring : string, callable or None, default=None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``.</span>
<span class="sd">        If ``None``, the ``score`` method of the estimator is used.</span>

<span class="sd">    fit_params : dict, optional</span>
<span class="sd">        Parameters to pass to the fit method.</span>

<span class="sd">    n_jobs: int, default: 1 :</span>
<span class="sd">        The maximum number of estimators fit in parallel.</span>

<span class="sd">            - If -1 all CPUs are used.</span>

<span class="sd">            - If 1 is given, no parallel computing code is used at all,</span>
<span class="sd">              which is useful for debugging.</span>

<span class="sd">            - For ``n_jobs`` below -1, ``(n_cpus + n_jobs + 1)`` are used.</span>
<span class="sd">              For example, with ``n_jobs = -2`` all CPUs but one are used.</span>

<span class="sd">    pre_dispatch : int, or string, optional</span>
<span class="sd">        Controls the number of jobs that get dispatched during parallel</span>
<span class="sd">        execution. Reducing this number can be useful to avoid an</span>
<span class="sd">        explosion of memory consumption when more jobs get dispatched</span>
<span class="sd">        than CPUs can process. This parameter can be:</span>

<span class="sd">            - None, in which case all the jobs are immediately</span>
<span class="sd">              created and spawned. Use this for lightweight and</span>
<span class="sd">              fast-running jobs, to avoid delays due to on-demand</span>
<span class="sd">              spawning of the jobs</span>

<span class="sd">            - An int, giving the exact number of total jobs that are</span>
<span class="sd">              spawned</span>

<span class="sd">            - A string, giving an expression as a function of n_jobs,</span>
<span class="sd">              as in &#39;2*n_jobs&#39;</span>

<span class="sd">    iid : boolean, default=True</span>
<span class="sd">        If True, the data is assumed to be identically distributed across</span>
<span class="sd">        the folds, and the loss minimized is the total loss per sample,</span>
<span class="sd">        and not the mean loss across the folds.</span>

<span class="sd">    cv : int, cross-validation generator or an iterable, optional</span>
<span class="sd">        Determines the cross-validation splitting strategy.</span>
<span class="sd">        Possible inputs for cv are:</span>

<span class="sd">        - None, to use the default 3-fold cross-validation,</span>
<span class="sd">        - integer, to specify the number of folds.</span>
<span class="sd">        - An object to be used as a cross-validation generator.</span>
<span class="sd">        - An iterable yielding train/test splits.</span>

<span class="sd">        For integer/None inputs, if the estimator is a classifier and ``y`` is</span>
<span class="sd">        either binary or multiclass,</span>
<span class="sd">        :class:`sklearn.model_selection.StratifiedKFold` is used. In all</span>
<span class="sd">        other cases, :class:`sklearn.model_selection.KFold` is used.</span>

<span class="sd">        Refer :ref:`User Guide &lt;cross_validation&gt;` for the various</span>
<span class="sd">        cross-validation strategies that can be used here.</span>

<span class="sd">    refit : boolean, default=True</span>
<span class="sd">        Refit the best estimator with the entire dataset.</span>
<span class="sd">        If &quot;False&quot;, it is impossible to make predictions using</span>
<span class="sd">        this RandomizedSearchCV instance after fitting.</span>

<span class="sd">    verbose : integer</span>
<span class="sd">        Controls the verbosity: the higher, the more messages.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional, default=None</span>
<span class="sd">        Pseudo random number generator state used for random uniform sampling</span>
<span class="sd">        from lists of possible values instead of scipy.stats distributions.</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    error_score : &#39;raise&#39; (default) or numeric</span>
<span class="sd">        Value to assign to the score if an error occurs in estimator fitting.</span>
<span class="sd">        If set to &#39;raise&#39;, the error is raised. If a numeric value is given,</span>
<span class="sd">        FitFailedWarning is raised. This parameter does not affect the refit</span>
<span class="sd">        step, which will always raise the error.</span>


<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    grid_scores_ : list of named tuples</span>
<span class="sd">        Contains scores for all parameter combinations in param_grid.</span>
<span class="sd">        Each entry corresponds to one parameter setting.</span>
<span class="sd">        Each named tuple has the attributes:</span>

<span class="sd">            * ``parameters``, a dict of parameter settings</span>
<span class="sd">            * ``mean_validation_score``, the mean score over the</span>
<span class="sd">              cross-validation folds</span>
<span class="sd">            * ``cv_validation_scores``, the list of scores for each fold</span>

<span class="sd">    best_estimator_ : estimator</span>
<span class="sd">        Estimator that was chosen by the search, i.e. estimator</span>
<span class="sd">        which gave highest score (or smallest loss if specified)</span>
<span class="sd">        on the left out data. Not available if refit=False.</span>

<span class="sd">    best_score_ : float</span>
<span class="sd">        Score of best_estimator on the left out data.</span>

<span class="sd">    best_params_ : dict</span>
<span class="sd">        Parameter setting that gave the best results on the hold out data.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The parameters selected are those that maximize the score of the held-out</span>
<span class="sd">    data, according to the scoring parameter.</span>

<span class="sd">    If `n_jobs` was set to a value higher than one, the data is copied for each</span>
<span class="sd">    parameter setting(and not `n_jobs` times). This is done for efficiency</span>
<span class="sd">    reasons if individual jobs take very little time, but may raise errors if</span>
<span class="sd">    the dataset is large and not enough memory is available.  A workaround in</span>
<span class="sd">    this case is to set `pre_dispatch`. Then, the memory is copied only</span>
<span class="sd">    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *</span>
<span class="sd">    n_jobs`.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    :class:`GridSearchCV`:</span>
<span class="sd">        Does exhaustive search over a grid of parameters.</span>

<span class="sd">    :class:`ParameterSampler`:</span>
<span class="sd">        A generator over parameter settings, constructed from</span>
<span class="sd">        param_distributions.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">param_distributions</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">fit_params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="s1">&#39;2*n_jobs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">error_score</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span> <span class="o">=</span> <span class="n">param_distributions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RandomizedSearchCV</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span> <span class="n">fit_params</span><span class="o">=</span><span class="n">fit_params</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">iid</span><span class="o">=</span><span class="n">iid</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="n">refit</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">pre_dispatch</span><span class="o">=</span><span class="n">pre_dispatch</span><span class="p">,</span> <span class="n">error_score</span><span class="o">=</span><span class="n">error_score</span><span class="p">)</span>

<div class="viewcode-block" id="RandomizedSearchCV.fit"><a class="viewcode-back" href="../../tmp/api_ibex_sklearn_grid_search_randomizedsearchcv.html#ibex.sklearn.grid_search.RandomizedSearchCV.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run fit on the estimator with randomly drawn parameters.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Training vector, where n_samples in the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples] or [n_samples, n_output], optional</span>
<span class="sd">            Target relative to X for classification or regression;</span>
<span class="sd">            None for unsupervised learning.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sampled_params</span> <span class="o">=</span> <span class="n">ParameterSampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span><span class="p">,</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">,</span>
                                          <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sampled_params</span><span class="p">)</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../index.html">
              <img class="logo" src="../../_static/logo.jpeg" alt="Logo"/>
            </a></p><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>