
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.manifold.t_sne &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.manifold.t_sne</h1><div class="highlight"><pre>
<span></span><span class="c1"># Author: Alexander Fabisch  -- &lt;afabisch@informatik.uni-bremen.de&gt;</span>
<span class="c1"># Author: Christopher Moody &lt;chrisemoody@gmail.com&gt;</span>
<span class="c1"># Author: Nick Travers &lt;nickt@squareup.com&gt;</span>
<span class="c1"># License: BSD 3 clause (C) 2014</span>

<span class="c1"># This is the exact and Barnes-Hut t-SNE implementation. There are other</span>
<span class="c1"># modifications of the algorithm:</span>
<span class="c1"># * Fast Optimization for t-SNE:</span>
<span class="c1">#   http://cseweb.ucsd.edu/~lvdmaaten/workshops/nips2010/papers/vandermaaten.pdf</span>

<span class="kn">from</span> <span class="nn">time</span> <span class="k">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">linalg</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="k">import</span> <span class="n">pdist</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="k">import</span> <span class="n">squareform</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="k">import</span> <span class="n">csr_matrix</span>
<span class="kn">from</span> <span class="nn">..neighbors</span> <span class="k">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..decomposition</span> <span class="k">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">..metrics.pairwise</span> <span class="k">import</span> <span class="n">pairwise_distances</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_utils</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_barnes_hut_tsne</span>
<span class="kn">from</span> <span class="nn">..externals.six</span> <span class="k">import</span> <span class="n">string_types</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">deprecated</span>


<span class="n">MACHINE_EPSILON</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>


<span class="k">def</span> <span class="nf">_joint_probabilities</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">desired_perplexity</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute joint probabilities p_ij from distances.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    distances : array, shape (n_samples * (n_samples-1) / 2,)</span>
<span class="sd">        Distances of samples are stored as condensed matrices, i.e.</span>
<span class="sd">        we omit the diagonal and duplicate entries and store everything</span>
<span class="sd">        in a one-dimensional array.</span>

<span class="sd">    desired_perplexity : float</span>
<span class="sd">        Desired perplexity of the joint probability distributions.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        Verbosity level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    P : array, shape (n_samples * (n_samples-1) / 2,)</span>
<span class="sd">        Condensed joint probability matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute conditional probabilities such that they approximately match</span>
    <span class="c1"># the desired perplexity</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">distances</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">conditional_P</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">_binary_search_perplexity</span><span class="p">(</span>
        <span class="n">distances</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">desired_perplexity</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">conditional_P</span> <span class="o">+</span> <span class="n">conditional_P</span><span class="o">.</span><span class="n">T</span>
    <span class="n">sum_P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">P</span><span class="p">),</span> <span class="n">MACHINE_EPSILON</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">squareform</span><span class="p">(</span><span class="n">P</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_P</span><span class="p">,</span> <span class="n">MACHINE_EPSILON</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span>


<span class="k">def</span> <span class="nf">_joint_probabilities_nn</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">desired_perplexity</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute joint probabilities p_ij from distances using just nearest</span>
<span class="sd">    neighbors.</span>

<span class="sd">    This method is approximately equal to _joint_probabilities. The latter</span>
<span class="sd">    is O(N), but limiting the joint probability to nearest neighbors improves</span>
<span class="sd">    this substantially to O(uN).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    distances : array, shape (n_samples, k)</span>
<span class="sd">        Distances of samples to its k nearest neighbors.</span>

<span class="sd">    neighbors : array, shape (n_samples, k)</span>
<span class="sd">        Indices of the k nearest-neighbors for each samples.</span>

<span class="sd">    desired_perplexity : float</span>
<span class="sd">        Desired perplexity of the joint probability distributions.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        Verbosity level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    P : csr sparse matrix, shape (n_samples, n_samples)</span>
<span class="sd">        Condensed joint probability matrix with only nearest neighbors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="c1"># Compute conditional probabilities such that they approximately match</span>
    <span class="c1"># the desired perplexity</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">distances</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">conditional_P</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">_binary_search_perplexity</span><span class="p">(</span>
        <span class="n">distances</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">desired_perplexity</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">conditional_P</span><span class="p">)),</span> \
        <span class="s2">&quot;All probabilities should be finite&quot;</span>

    <span class="c1"># Symmetrize the joint probability distribution using sparse operations</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">((</span><span class="n">conditional_P</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
                    <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">)),</span>
                   <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="n">P</span> <span class="o">=</span> <span class="n">P</span> <span class="o">+</span> <span class="n">P</span><span class="o">.</span><span class="n">T</span>

    <span class="c1"># Normalize the joint probability distribution</span>
    <span class="n">sum_P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">MACHINE_EPSILON</span><span class="p">)</span>
    <span class="n">P</span> <span class="o">/=</span> <span class="n">sum_P</span>

    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Computed conditional probabilities in </span><span class="si">{:.3f}</span><span class="s2">s&quot;</span>
              <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">duration</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">P</span>


<span class="k">def</span> <span class="nf">_kl_divergence</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span>
                   <span class="n">skip_num_points</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;t-SNE objective function: gradient of the KL divergence</span>
<span class="sd">    of p_ijs and q_ijs and the absolute error.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    params : array, shape (n_params,)</span>
<span class="sd">        Unraveled embedding.</span>

<span class="sd">    P : array, shape (n_samples * (n_samples-1) / 2,)</span>
<span class="sd">        Condensed joint probability matrix.</span>

<span class="sd">    degrees_of_freedom : float</span>
<span class="sd">        Degrees of freedom of the Student&#39;s-t distribution.</span>

<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>

<span class="sd">    n_components : int</span>
<span class="sd">        Dimension of the embedded space.</span>

<span class="sd">    skip_num_points : int (optional, default:0)</span>
<span class="sd">        This does not compute the gradient for points with indices below</span>
<span class="sd">        `skip_num_points`. This is useful when computing transforms of new</span>
<span class="sd">        data where you&#39;d like to keep the old data fixed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    kl_divergence : float</span>
<span class="sd">        Kullback-Leibler divergence of p_ij and q_ij.</span>

<span class="sd">    grad : array, shape (n_params,)</span>
<span class="sd">        Unraveled gradient of the Kullback-Leibler divergence with respect to</span>
<span class="sd">        the embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X_embedded</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>

    <span class="c1"># Q is a heavy-tailed distribution: Student&#39;s t-distribution</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">pdist</span><span class="p">(</span><span class="n">X_embedded</span><span class="p">,</span> <span class="s2">&quot;sqeuclidean&quot;</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">+=</span> <span class="mf">1.</span>
    <span class="n">dist</span> <span class="o">/=</span> <span class="n">degrees_of_freedom</span>
    <span class="n">dist</span> <span class="o">**=</span> <span class="p">(</span><span class="n">degrees_of_freedom</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">/</span> <span class="o">-</span><span class="mf">2.0</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dist</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dist</span><span class="p">)),</span> <span class="n">MACHINE_EPSILON</span><span class="p">)</span>

    <span class="c1"># Optimization trick below: np.dot(x, y) is faster than</span>
    <span class="c1"># np.sum(x * y) because it calls BLAS</span>

    <span class="c1"># Objective: C (Kullback-Leibler divergence of P and Q)</span>
    <span class="n">kl_divergence</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">MACHINE_EPSILON</span><span class="p">)</span> <span class="o">/</span> <span class="n">Q</span><span class="p">))</span>

    <span class="c1"># Gradient: dC/dY</span>
    <span class="c1"># pdist always returns double precision distances. Thus we need to take</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_components</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">params</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">PQd</span> <span class="o">=</span> <span class="n">squareform</span><span class="p">((</span><span class="n">P</span> <span class="o">-</span> <span class="n">Q</span><span class="p">)</span> <span class="o">*</span> <span class="n">dist</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">skip_num_points</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">PQd</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;K&#39;</span><span class="p">),</span>
                         <span class="n">X_embedded</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">X_embedded</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">degrees_of_freedom</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">degrees_of_freedom</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">c</span>

    <span class="k">return</span> <span class="n">kl_divergence</span><span class="p">,</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">_kl_divergence_bh</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span>
                      <span class="n">angle</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">skip_num_points</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;t-SNE objective function: KL divergence of p_ijs and q_ijs.</span>

<span class="sd">    Uses Barnes-Hut tree methods to calculate the gradient that</span>
<span class="sd">    runs in O(NlogN) instead of O(N^2)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    params : array, shape (n_params,)</span>
<span class="sd">        Unraveled embedding.</span>

<span class="sd">    P : csr sparse matrix, shape (n_samples, n_sample)</span>
<span class="sd">        Sparse approximate joint probability matrix, computed only for the</span>
<span class="sd">        k nearest-neighbors and symmetrized.</span>

<span class="sd">    degrees_of_freedom : float</span>
<span class="sd">        Degrees of freedom of the Student&#39;s-t distribution.</span>

<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples.</span>

<span class="sd">    n_components : int</span>
<span class="sd">        Dimension of the embedded space.</span>

<span class="sd">    angle : float (default: 0.5)</span>
<span class="sd">        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.</span>
<span class="sd">        &#39;angle&#39; is the angular size (referred to as theta in [3]) of a distant</span>
<span class="sd">        node as measured from a point. If this size is below &#39;angle&#39; then it is</span>
<span class="sd">        used as a summary node of all points contained within it.</span>
<span class="sd">        This method is not very sensitive to changes in this parameter</span>
<span class="sd">        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing</span>
<span class="sd">        computation time and angle greater 0.8 has quickly increasing error.</span>

<span class="sd">    skip_num_points : int (optional, default:0)</span>
<span class="sd">        This does not compute the gradient for points with indices below</span>
<span class="sd">        `skip_num_points`. This is useful when computing transforms of new</span>
<span class="sd">        data where you&#39;d like to keep the old data fixed.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        Verbosity level.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    kl_divergence : float</span>
<span class="sd">        Kullback-Leibler divergence of p_ij and q_ij.</span>

<span class="sd">    grad : array, shape (n_params,)</span>
<span class="sd">        Unraveled gradient of the Kullback-Leibler divergence with respect to</span>
<span class="sd">        the embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">X_embedded</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_components</span><span class="p">)</span>

    <span class="n">val_P</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">indptr</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_embedded</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">_barnes_hut_tsne</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">val_P</span><span class="p">,</span> <span class="n">X_embedded</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">indptr</span><span class="p">,</span>
                                      <span class="n">grad</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span>
                                      <span class="n">dof</span><span class="o">=</span><span class="n">degrees_of_freedom</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">degrees_of_freedom</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">degrees_of_freedom</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">c</span>

    <span class="k">return</span> <span class="n">error</span><span class="p">,</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">_gradient_descent</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">it</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">,</span>
                      <span class="n">n_iter_check</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iter_without_progress</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
                      <span class="n">momentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">200.0</span><span class="p">,</span> <span class="n">min_gain</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                      <span class="n">min_grad_norm</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Batch gradient descent with momentum and individual gains.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    objective : function or callable</span>
<span class="sd">        Should return a tuple of cost and gradient for a given parameter</span>
<span class="sd">        vector. When expensive to compute, the cost can optionally</span>
<span class="sd">        be None and can be computed every n_iter_check steps using</span>
<span class="sd">        the objective_error function.</span>

<span class="sd">    p0 : array-like, shape (n_params,)</span>
<span class="sd">        Initial parameter vector.</span>

<span class="sd">    it : int</span>
<span class="sd">        Current number of iterations (this function will be called more than</span>
<span class="sd">        once during the optimization).</span>

<span class="sd">    n_iter : int</span>
<span class="sd">        Maximum number of gradient descent iterations.</span>

<span class="sd">    n_iter_check : int</span>
<span class="sd">        Number of iterations before evaluating the global error. If the error</span>
<span class="sd">        is sufficiently low, we abort the optimization.</span>

<span class="sd">    n_iter_without_progress : int, optional (default: 300)</span>
<span class="sd">        Maximum number of iterations without progress before we abort the</span>
<span class="sd">        optimization.</span>

<span class="sd">    momentum : float, within (0.0, 1.0), optional (default: 0.8)</span>
<span class="sd">        The momentum generates a weight for previous gradients that decays</span>
<span class="sd">        exponentially.</span>

<span class="sd">    learning_rate : float, optional (default: 200.0)</span>
<span class="sd">        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If</span>
<span class="sd">        the learning rate is too high, the data may look like a &#39;ball&#39; with any</span>
<span class="sd">        point approximately equidistant from its nearest neighbours. If the</span>
<span class="sd">        learning rate is too low, most points may look compressed in a dense</span>
<span class="sd">        cloud with few outliers.</span>

<span class="sd">    min_gain : float, optional (default: 0.01)</span>
<span class="sd">        Minimum individual gain for each parameter.</span>

<span class="sd">    min_grad_norm : float, optional (default: 1e-7)</span>
<span class="sd">        If the gradient norm is below this threshold, the optimization will</span>
<span class="sd">        be aborted.</span>

<span class="sd">    verbose : int, optional (default: 0)</span>
<span class="sd">        Verbosity level.</span>

<span class="sd">    args : sequence</span>
<span class="sd">        Arguments to pass to objective function.</span>

<span class="sd">    kwargs : dict</span>
<span class="sd">        Keyword arguments to pass to objective function.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    p : array, shape (n_params,)</span>
<span class="sd">        Optimum parameters.</span>

<span class="sd">    error : float</span>
<span class="sd">        Optimum.</span>

<span class="sd">    i : int</span>
<span class="sd">        Last iteration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">p0</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">gains</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
    <span class="n">best_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
    <span class="n">best_iter</span> <span class="o">=</span> <span class="n">i</span> <span class="o">=</span> <span class="n">it</span>

    <span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="n">n_iter</span><span class="p">):</span>
        <span class="n">error</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">objective</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="n">inc</span> <span class="o">=</span> <span class="n">update</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">&lt;</span> <span class="mf">0.0</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">inc</span><span class="p">)</span>
        <span class="n">gains</span><span class="p">[</span><span class="n">inc</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.2</span>
        <span class="n">gains</span><span class="p">[</span><span class="n">dec</span><span class="p">]</span> <span class="o">*=</span> <span class="mf">0.8</span>
        <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">gains</span><span class="p">,</span> <span class="n">min_gain</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">gains</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">gains</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">update</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="n">p</span> <span class="o">+=</span> <span class="n">update</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_iter_check</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
            <span class="n">duration</span> <span class="o">=</span> <span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span>
            <span class="n">tic</span> <span class="o">=</span> <span class="n">toc</span>

            <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Iteration </span><span class="si">%d</span><span class="s2">: error = </span><span class="si">%.7f</span><span class="s2">,&quot;</span>
                      <span class="s2">&quot; gradient norm = </span><span class="si">%.7f</span><span class="s2">&quot;</span>
                      <span class="s2">&quot; (</span><span class="si">%s</span><span class="s2"> iterations in </span><span class="si">%0.3f</span><span class="s2">s)&quot;</span>
                      <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">error</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">,</span> <span class="n">n_iter_check</span><span class="p">,</span> <span class="n">duration</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="n">best_error</span><span class="p">:</span>
                <span class="n">best_error</span> <span class="o">=</span> <span class="n">error</span>
                <span class="n">best_iter</span> <span class="o">=</span> <span class="n">i</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">-</span> <span class="n">best_iter</span> <span class="o">&gt;</span> <span class="n">n_iter_without_progress</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Iteration </span><span class="si">%d</span><span class="s2">: did not make any progress &quot;</span>
                          <span class="s2">&quot;during the last </span><span class="si">%d</span><span class="s2"> episodes. Finished.&quot;</span>
                          <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_iter_without_progress</span><span class="p">))</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="n">grad_norm</span> <span class="o">&lt;=</span> <span class="n">min_grad_norm</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Iteration </span><span class="si">%d</span><span class="s2">: gradient norm </span><span class="si">%f</span><span class="s2">. Finished.&quot;</span>
                          <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grad_norm</span><span class="p">))</span>
                <span class="k">break</span>

    <span class="k">return</span> <span class="n">p</span><span class="p">,</span> <span class="n">error</span><span class="p">,</span> <span class="n">i</span>


<span class="k">def</span> <span class="nf">trustworthiness</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X_embedded</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">precomputed</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Expresses to what extent the local structure is retained.</span>

<span class="sd">    The trustworthiness is within [0, 1]. It is defined as</span>

<span class="sd">    .. math::</span>

<span class="sd">        T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1}</span>
<span class="sd">            \sum_{j \in U^{(k)}_i} (r(i, j) - k)</span>

<span class="sd">    where :math:`r(i, j)` is the rank of the embedded datapoint j</span>
<span class="sd">    according to the pairwise distances between the embedded datapoints,</span>
<span class="sd">    :math:`U^{(k)}_i` is the set of points that are in the k nearest</span>
<span class="sd">    neighbors in the embedded space but not in the original space.</span>

<span class="sd">    * &quot;Neighborhood Preservation in Nonlinear Projection Methods: An</span>
<span class="sd">      Experimental Study&quot;</span>
<span class="sd">      J. Venna, S. Kaski</span>
<span class="sd">    * &quot;Learning a Parametric Embedding by Preserving Local Structure&quot;</span>
<span class="sd">      L.J.P. van der Maaten</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array, shape (n_samples, n_features) or (n_samples, n_samples)</span>
<span class="sd">        If the metric is &#39;precomputed&#39; X must be a square distance</span>
<span class="sd">        matrix. Otherwise it contains a sample per row.</span>

<span class="sd">    X_embedded : array, shape (n_samples, n_components)</span>
<span class="sd">        Embedding of the training data in low-dimensional space.</span>

<span class="sd">    n_neighbors : int, optional (default: 5)</span>
<span class="sd">        Number of neighbors k that will be considered.</span>

<span class="sd">    precomputed : bool, optional (default: False)</span>
<span class="sd">        Set this flag if X is a precomputed square distance matrix.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    trustworthiness : float</span>
<span class="sd">        Trustworthiness of the low-dimensional embedding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">precomputed</span><span class="p">:</span>
        <span class="n">dist_X</span> <span class="o">=</span> <span class="n">X</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_X</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dist_X_embedded</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X_embedded</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ind_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dist_X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ind_X_embedded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dist_X_embedded</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">:</span><span class="n">n_neighbors</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">ranks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">):</span>
            <span class="n">ranks</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">ind_X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">ind_X_embedded</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ranks</span> <span class="o">-=</span> <span class="n">n_neighbors</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ranks</span><span class="p">[</span><span class="n">ranks</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">n_neighbors</span> <span class="o">*</span>
                          <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="n">n_neighbors</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">t</span>


<span class="k">class</span> <span class="nc">TSNE</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;t-distributed Stochastic Neighbor Embedding.</span>

<span class="sd">    t-SNE [1] is a tool to visualize high-dimensional data. It converts</span>
<span class="sd">    similarities between data points to joint probabilities and tries</span>
<span class="sd">    to minimize the Kullback-Leibler divergence between the joint</span>
<span class="sd">    probabilities of the low-dimensional embedding and the</span>
<span class="sd">    high-dimensional data. t-SNE has a cost function that is not convex,</span>
<span class="sd">    i.e. with different initializations we can get different results.</span>

<span class="sd">    It is highly recommended to use another dimensionality reduction</span>
<span class="sd">    method (e.g. PCA for dense data or TruncatedSVD for sparse data)</span>
<span class="sd">    to reduce the number of dimensions to a reasonable amount (e.g. 50)</span>
<span class="sd">    if the number of features is very high. This will suppress some</span>
<span class="sd">    noise and speed up the computation of pairwise distances between</span>
<span class="sd">    samples. For more tips see Laurens van der Maaten&#39;s FAQ [2].</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;t_sne&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_components : int, optional (default: 2)</span>
<span class="sd">        Dimension of the embedded space.</span>

<span class="sd">    perplexity : float, optional (default: 30)</span>
<span class="sd">        The perplexity is related to the number of nearest neighbors that</span>
<span class="sd">        is used in other manifold learning algorithms. Larger datasets</span>
<span class="sd">        usually require a larger perplexity. Consider selecting a value</span>
<span class="sd">        between 5 and 50. The choice is not extremely critical since t-SNE</span>
<span class="sd">        is quite insensitive to this parameter.</span>

<span class="sd">    early_exaggeration : float, optional (default: 12.0)</span>
<span class="sd">        Controls how tight natural clusters in the original space are in</span>
<span class="sd">        the embedded space and how much space will be between them. For</span>
<span class="sd">        larger values, the space between natural clusters will be larger</span>
<span class="sd">        in the embedded space. Again, the choice of this parameter is not</span>
<span class="sd">        very critical. If the cost function increases during initial</span>
<span class="sd">        optimization, the early exaggeration factor or the learning rate</span>
<span class="sd">        might be too high.</span>

<span class="sd">    learning_rate : float, optional (default: 200.0)</span>
<span class="sd">        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If</span>
<span class="sd">        the learning rate is too high, the data may look like a &#39;ball&#39; with any</span>
<span class="sd">        point approximately equidistant from its nearest neighbours. If the</span>
<span class="sd">        learning rate is too low, most points may look compressed in a dense</span>
<span class="sd">        cloud with few outliers. If the cost function gets stuck in a bad local</span>
<span class="sd">        minimum increasing the learning rate may help.</span>

<span class="sd">    n_iter : int, optional (default: 1000)</span>
<span class="sd">        Maximum number of iterations for the optimization. Should be at</span>
<span class="sd">        least 250.</span>

<span class="sd">    n_iter_without_progress : int, optional (default: 300)</span>
<span class="sd">        Maximum number of iterations without progress before we abort the</span>
<span class="sd">        optimization, used after 250 initial iterations with early</span>
<span class="sd">        exaggeration. Note that progress is only checked every 50 iterations so</span>
<span class="sd">        this value is rounded to the next multiple of 50.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           parameter *n_iter_without_progress* to control stopping criteria.</span>

<span class="sd">    min_grad_norm : float, optional (default: 1e-7)</span>
<span class="sd">        If the gradient norm is below this threshold, the optimization will</span>
<span class="sd">        be stopped.</span>

<span class="sd">    metric : string or callable, optional</span>
<span class="sd">        The metric to use when calculating distance between instances in a</span>
<span class="sd">        feature array. If metric is a string, it must be one of the options</span>
<span class="sd">        allowed by scipy.spatial.distance.pdist for its metric parameter, or</span>
<span class="sd">        a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.</span>
<span class="sd">        If metric is &quot;precomputed&quot;, X is assumed to be a distance matrix.</span>
<span class="sd">        Alternatively, if metric is a callable function, it is called on each</span>
<span class="sd">        pair of instances (rows) and the resulting value recorded. The callable</span>
<span class="sd">        should take two arrays from X as input and return a value indicating</span>
<span class="sd">        the distance between them. The default is &quot;euclidean&quot; which is</span>
<span class="sd">        interpreted as squared euclidean distance.</span>

<span class="sd">    init : string or numpy array, optional (default: &quot;random&quot;)</span>
<span class="sd">        Initialization of embedding. Possible options are &#39;random&#39;, &#39;pca&#39;,</span>
<span class="sd">        and a numpy array of shape (n_samples, n_components).</span>
<span class="sd">        PCA initialization cannot be used with precomputed distances and is</span>
<span class="sd">        usually more globally stable than random initialization.</span>

<span class="sd">    verbose : int, optional (default: 0)</span>
<span class="sd">        Verbosity level.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default: None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.  Note that different initializations might result in</span>
<span class="sd">        different local minima of the cost function.</span>

<span class="sd">    method : string (default: &#39;barnes_hut&#39;)</span>
<span class="sd">        By default the gradient calculation algorithm uses Barnes-Hut</span>
<span class="sd">        approximation running in O(NlogN) time. method=&#39;exact&#39;</span>
<span class="sd">        will run on the slower, but exact, algorithm in O(N^2) time. The</span>
<span class="sd">        exact algorithm should be used when nearest-neighbor errors need</span>
<span class="sd">        to be better than 3%. However, the exact method cannot scale to</span>
<span class="sd">        millions of examples.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Approximate optimization *method* via the Barnes-Hut.</span>

<span class="sd">    angle : float (default: 0.5)</span>
<span class="sd">        Only used if method=&#39;barnes_hut&#39;</span>
<span class="sd">        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.</span>
<span class="sd">        &#39;angle&#39; is the angular size (referred to as theta in [3]) of a distant</span>
<span class="sd">        node as measured from a point. If this size is below &#39;angle&#39; then it is</span>
<span class="sd">        used as a summary node of all points contained within it.</span>
<span class="sd">        This method is not very sensitive to changes in this parameter</span>
<span class="sd">        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing</span>
<span class="sd">        computation time and angle greater 0.8 has quickly increasing error.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    embedding_ : array-like, shape (n_samples, n_components)</span>
<span class="sd">        Stores the embedding vectors.</span>

<span class="sd">    kl_divergence_ : float</span>
<span class="sd">        Kullback-Leibler divergence after optimization.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        Number of iterations run.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.manifold import TSNE</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])</span>
<span class="sd">    &gt;&gt;&gt; X_embedded = TSNE(n_components=2).fit_transform(X)</span>
<span class="sd">    &gt;&gt;&gt; X_embedded.shape</span>
<span class="sd">    (4, 2)</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    [1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data</span>
<span class="sd">        Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</span>

<span class="sd">    [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding</span>
<span class="sd">        http://homepage.tudelft.nl/19j49/t-SNE.html</span>

<span class="sd">    [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.</span>
<span class="sd">        Journal of Machine Learning Research 15(Oct):3221-3245, 2014.</span>
<span class="sd">        http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Control the number of exploration iterations with early_exaggeration on</span>
    <span class="n">_EXPLORATION_N_ITER</span> <span class="o">=</span> <span class="mi">250</span>

    <span class="c1"># Control the number of iterations between progress checks</span>
    <span class="n">_N_ITER_CHECK</span> <span class="o">=</span> <span class="mi">50</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mf">30.0</span><span class="p">,</span>
                 <span class="n">early_exaggeration</span><span class="o">=</span><span class="mf">12.0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">200.0</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                 <span class="n">n_iter_without_progress</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">min_grad_norm</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
                 <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;random&quot;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;barnes_hut&#39;</span><span class="p">,</span> <span class="n">angle</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">perplexity</span> <span class="o">=</span> <span class="n">perplexity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_exaggeration</span> <span class="o">=</span> <span class="n">early_exaggeration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_without_progress</span> <span class="o">=</span> <span class="n">n_iter_without_progress</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_grad_norm</span> <span class="o">=</span> <span class="n">min_grad_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">angle</span> <span class="o">=</span> <span class="n">angle</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">skip_num_points</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model using X as training data.</span>

<span class="sd">        Note that sparse arrays can only be handled by method=&#39;exact&#39;.</span>
<span class="sd">        It is recommended that you convert your sparse array to dense</span>
<span class="sd">        (e.g. `X.toarray()`) if it fits in memory, or otherwise using a</span>
<span class="sd">        dimensionality reduction technique (e.g. TruncatedSVD).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array, shape (n_samples, n_features) or (n_samples, n_samples)</span>
<span class="sd">            If the metric is &#39;precomputed&#39; X must be a square distance</span>
<span class="sd">            matrix. Otherwise it contains a sample per row. Note that this</span>
<span class="sd">            when method=&#39;barnes_hut&#39;, X cannot be a sparse array and if need be</span>
<span class="sd">            will be converted to a 32 bit float array. Method=&#39;exact&#39; allows</span>
<span class="sd">            sparse arrays and 64bit floating point inputs.</span>

<span class="sd">        skip_num_points : int (optional, default:0)</span>
<span class="sd">            This does not compute the gradient for points with indices below</span>
<span class="sd">            `skip_num_points`. This is useful when computing transforms of new</span>
<span class="sd">            data where you&#39;d like to keep the old data fixed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;barnes_hut&#39;</span><span class="p">,</span> <span class="s1">&#39;exact&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;method&#39; must be &#39;barnes_hut&#39; or &#39;exact&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">angle</span> <span class="o">&lt;</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">angle</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;angle&#39; must be between 0.0 - 1.0&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;precomputed&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s1">&#39;pca&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The parameter init=</span><span class="se">\&quot;</span><span class="s2">pca</span><span class="se">\&quot;</span><span class="s2"> cannot be &quot;</span>
                                 <span class="s2">&quot;used with metric=</span><span class="se">\&quot;</span><span class="s2">precomputed</span><span class="se">\&quot;</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X should be a square distance matrix&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">X</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All distances should be positive, the &quot;</span>
                                 <span class="s2">&quot;precomputed distances given as X is not &quot;</span>
                                 <span class="s2">&quot;correct&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;barnes_hut&#39;</span> <span class="ow">and</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;A sparse matrix was passed, but dense &#39;</span>
                            <span class="s1">&#39;data is required for method=&quot;barnes_hut&quot;. Use &#39;</span>
                            <span class="s1">&#39;X.toarray() to convert to a dense numpy array if &#39;</span>
                            <span class="s1">&#39;the array is small enough for it to fit in &#39;</span>
                            <span class="s1">&#39;memory. Otherwise consider dimensionality &#39;</span>
                            <span class="s1">&#39;reduction techniques (e.g. TruncatedSVD)&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">],</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;barnes_hut&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;n_components&#39; should be inferior to 4 for the &quot;</span>
                             <span class="s2">&quot;barnes_hut algorithm as it relies on &quot;</span>
                             <span class="s2">&quot;quad-tree or oct-tree.&quot;</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_exaggeration</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;early_exaggeration must be at least 1, but is </span><span class="si">{}</span><span class="s2">&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_exaggeration</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">&lt;</span> <span class="mi">250</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_iter should be at least 250&quot;</span><span class="p">)</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">neighbors_nn</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;exact&quot;</span><span class="p">:</span>
            <span class="c1"># Retrieve the distance matrix, either using the precomputed one or</span>
            <span class="c1"># computing it.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;precomputed&quot;</span><span class="p">:</span>
                <span class="n">distances</span> <span class="o">=</span> <span class="n">X</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Computing pairwise distances...&quot;</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;euclidean&quot;</span><span class="p">:</span>
                    <span class="n">distances</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">,</span>
                                                   <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">distances</span> <span class="o">=</span> <span class="n">pairwise_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">distances</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All distances should be positive, the &quot;</span>
                                     <span class="s2">&quot;metric given is not correct&quot;</span><span class="p">)</span>

            <span class="c1"># compute the joint probability distribution for the input space</span>
            <span class="n">P</span> <span class="o">=</span> <span class="n">_joint_probabilities</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">perplexity</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">P</span><span class="p">)),</span> <span class="s2">&quot;All probabilities should be finite&quot;</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">P</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;All probabilities should be non-negative&quot;</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">P</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;All probabilities should be less &quot;</span>
                                    <span class="s2">&quot;or then equal to one&quot;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Cpmpute the number of nearest neighbors to find.</span>
            <span class="c1"># LvdM uses 3 * perplexity as the number of neighbors.</span>
            <span class="c1"># In the event that we have very small # of points</span>
            <span class="c1"># set the neighbors to n - 1.</span>
            <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">3.</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">perplexity</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Computing </span><span class="si">{}</span><span class="s2"> nearest neighbors...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>

            <span class="c1"># Find the nearest neighbors for every point</span>
            <span class="n">neighbors_method</span> <span class="o">=</span> <span class="s1">&#39;ball_tree&#39;</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">==</span> <span class="s1">&#39;precomputed&#39;</span><span class="p">):</span>
                <span class="n">neighbors_method</span> <span class="o">=</span> <span class="s1">&#39;brute&#39;</span>
            <span class="n">knn</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">neighbors_method</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                                   <span class="n">metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="p">)</span>
            <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
            <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Indexed </span><span class="si">{}</span><span class="s2"> samples in </span><span class="si">{:.3f}</span><span class="s2">s...&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">n_samples</span><span class="p">,</span> <span class="n">duration</span><span class="p">))</span>

            <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
            <span class="n">distances_nn</span><span class="p">,</span> <span class="n">neighbors_nn</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span>
                <span class="kc">None</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
            <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Computed neighbors for </span><span class="si">{}</span><span class="s2"> samples in </span><span class="si">{:.3f}</span><span class="s2">s...&quot;</span>
                      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">duration</span><span class="p">))</span>

            <span class="c1"># Free the memory used by the ball_tree</span>
            <span class="k">del</span> <span class="n">knn</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">==</span> <span class="s2">&quot;euclidean&quot;</span><span class="p">:</span>
                <span class="c1"># knn return the euclidean distance but we need it squared</span>
                <span class="c1"># to be consistent with the &#39;exact&#39; method. Note that the</span>
                <span class="c1"># the method was derived using the euclidean method as in the</span>
                <span class="c1"># input space. Not sure of the implication of using a different</span>
                <span class="c1"># metric.</span>
                <span class="n">distances_nn</span> <span class="o">**=</span> <span class="mi">2</span>

            <span class="c1"># compute the joint probability distribution for the input space</span>
            <span class="n">P</span> <span class="o">=</span> <span class="n">_joint_probabilities_nn</span><span class="p">(</span><span class="n">distances_nn</span><span class="p">,</span> <span class="n">neighbors_nn</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">perplexity</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">X_embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s1">&#39;pca&#39;</span><span class="p">:</span>
            <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> <span class="n">svd_solver</span><span class="o">=</span><span class="s1">&#39;randomized&#39;</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
            <span class="n">X_embedded</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">==</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span>
            <span class="c1"># The embedding is initialized with iid samples from Gaussians with</span>
            <span class="c1"># standard deviation 1e-4.</span>
            <span class="n">X_embedded</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="o">*</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;init&#39; must be &#39;pca&#39;, &#39;random&#39;, or &quot;</span>
                             <span class="s2">&quot;a numpy array&quot;</span><span class="p">)</span>

        <span class="c1"># Degrees of freedom of the Student&#39;s t-distribution. The suggestion</span>
        <span class="c1"># degrees_of_freedom = n_components - 1 comes from</span>
        <span class="c1"># &quot;Learning a Parametric Embedding by Preserving Local Structure&quot;</span>
        <span class="c1"># Laurens van der Maaten, 2009.</span>
        <span class="n">degrees_of_freedom</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tsne</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                          <span class="n">X_embedded</span><span class="o">=</span><span class="n">X_embedded</span><span class="p">,</span>
                          <span class="n">neighbors</span><span class="o">=</span><span class="n">neighbors_nn</span><span class="p">,</span>
                          <span class="n">skip_num_points</span><span class="o">=</span><span class="n">skip_num_points</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;Attribute n_iter_final was deprecated in version 0.19 and &quot;</span>
                <span class="s2">&quot;will be removed in 0.21. Use ``n_iter_`` instead&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">n_iter_final</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span>

    <span class="k">def</span> <span class="nf">_tsne</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">X_embedded</span><span class="p">,</span>
              <span class="n">neighbors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">skip_num_points</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Runs t-SNE.&quot;&quot;&quot;</span>
        <span class="c1"># t-SNE minimizes the Kullback-Leiber divergence of the Gaussians P</span>
        <span class="c1"># and the Student&#39;s t-distributions Q. The optimization algorithm that</span>
        <span class="c1"># we use is batch gradient descent with two stages:</span>
        <span class="c1"># * initial optimization with early exaggeration and momentum at 0.5</span>
        <span class="c1"># * final optimization with momentum at 0.8</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">X_embedded</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

        <span class="n">opt_args</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;it&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;n_iter_check&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_N_ITER_CHECK</span><span class="p">,</span>
            <span class="s2">&quot;min_grad_norm&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_grad_norm</span><span class="p">,</span>
            <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="s2">&quot;verbose&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="s2">&quot;kwargs&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">skip_num_points</span><span class="o">=</span><span class="n">skip_num_points</span><span class="p">),</span>
            <span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">P</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">],</span>
            <span class="s2">&quot;n_iter_without_progress&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EXPLORATION_N_ITER</span><span class="p">,</span>
            <span class="s2">&quot;n_iter&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EXPLORATION_N_ITER</span><span class="p">,</span>
            <span class="s2">&quot;momentum&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;barnes_hut&#39;</span><span class="p">:</span>
            <span class="n">obj_func</span> <span class="o">=</span> <span class="n">_kl_divergence_bh</span>
            <span class="n">opt_args</span><span class="p">[</span><span class="s1">&#39;kwargs&#39;</span><span class="p">][</span><span class="s1">&#39;angle&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">angle</span>
            <span class="c1"># Repeat verbose argument for _kl_divergence_bh</span>
            <span class="n">opt_args</span><span class="p">[</span><span class="s1">&#39;kwargs&#39;</span><span class="p">][</span><span class="s1">&#39;verbose&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">obj_func</span> <span class="o">=</span> <span class="n">_kl_divergence</span>

        <span class="c1"># Learning schedule (part 1): do 250 iteration with lower momentum but</span>
        <span class="c1"># higher learning rate controlled via the early exageration parameter</span>
        <span class="n">P</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_exaggeration</span>
        <span class="n">params</span><span class="p">,</span> <span class="n">kl_divergence</span><span class="p">,</span> <span class="n">it</span> <span class="o">=</span> <span class="n">_gradient_descent</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span>
                                                      <span class="o">**</span><span class="n">opt_args</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] KL divergence after </span><span class="si">%d</span><span class="s2"> iterations with early &quot;</span>
                  <span class="s2">&quot;exaggeration: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">it</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kl_divergence</span><span class="p">))</span>

        <span class="c1"># Learning schedule (part 2): disable early exaggeration and finish</span>
        <span class="c1"># optimization with a higher momentum at 0.8</span>
        <span class="n">P</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_exaggeration</span>
        <span class="n">remaining</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EXPLORATION_N_ITER</span>
        <span class="k">if</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_EXPLORATION_N_ITER</span> <span class="ow">or</span> <span class="n">remaining</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">opt_args</span><span class="p">[</span><span class="s1">&#39;n_iter&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span>
            <span class="n">opt_args</span><span class="p">[</span><span class="s1">&#39;it&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">it</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">opt_args</span><span class="p">[</span><span class="s1">&#39;momentum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.8</span>
            <span class="n">opt_args</span><span class="p">[</span><span class="s1">&#39;n_iter_without_progress&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_without_progress</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">kl_divergence</span><span class="p">,</span> <span class="n">it</span> <span class="o">=</span> <span class="n">_gradient_descent</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span>
                                                          <span class="o">**</span><span class="n">opt_args</span><span class="p">)</span>

        <span class="c1"># Save the final number of iterations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">it</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[t-SNE] Error after </span><span class="si">%d</span><span class="s2"> iterations: </span><span class="si">%f</span><span class="s2">&quot;</span>
                  <span class="o">%</span> <span class="p">(</span><span class="n">it</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kl_divergence</span><span class="p">))</span>

        <span class="n">X_embedded</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_divergence_</span> <span class="o">=</span> <span class="n">kl_divergence</span>

        <span class="k">return</span> <span class="n">X_embedded</span>

<div class="viewcode-block" id="TSNE.fit_transform"><a class="viewcode-back" href="../../../api_ibex_sklearn_manifold_tsne.html#ibex.sklearn.manifold.TSNE.fit_transform">[docs]</a>    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit X into an embedded space and return that transformed</span>
<span class="sd">        output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array, shape (n_samples, n_features) or (n_samples, n_samples)</span>
<span class="sd">            If the metric is &#39;precomputed&#39; X must be a square distance</span>
<span class="sd">            matrix. Otherwise it contains a sample per row.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_new : array, shape (n_samples, n_components)</span>
<span class="sd">            Embedding of the training data in low-dimensional space.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_</span></div>

<div class="viewcode-block" id="TSNE.fit"><a class="viewcode-back" href="../../../api_ibex_sklearn_manifold_tsne.html#ibex.sklearn.manifold.TSNE.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit X into an embedded space.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array, shape (n_samples, n_features) or (n_samples, n_samples)</span>
<span class="sd">            If the metric is &#39;precomputed&#39; X must be a square distance</span>
<span class="sd">            matrix. Otherwise it contains a sample per row. If the method</span>
<span class="sd">            is &#39;exact&#39;, X may be a sparse matrix of type &#39;csr&#39;, &#39;csc&#39;</span>
<span class="sd">            or &#39;coo&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>