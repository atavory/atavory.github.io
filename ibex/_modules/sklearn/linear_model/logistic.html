
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.linear_model.logistic &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.linear_model.logistic</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Logistic Regression</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Author: Gael Varoquaux &lt;gael.varoquaux@normalesup.org&gt;</span>
<span class="c1">#         Fabian Pedregosa &lt;f@bianp.net&gt;</span>
<span class="c1">#         Alexandre Gramfort &lt;alexandre.gramfort@telecom-paristech.fr&gt;</span>
<span class="c1">#         Manoj Kumar &lt;manojkumarsivaraj334@gmail.com&gt;</span>
<span class="c1">#         Lars Buitinck</span>
<span class="c1">#         Simon Wu &lt;s8wu@uwaterloo.ca&gt;</span>
<span class="c1">#         Arthur Mensch &lt;arthur.mensch@m4x.org</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">optimize</span><span class="p">,</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="k">import</span> <span class="n">expit</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span> <span class="n">SparseCoefMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">.sag</span> <span class="k">import</span> <span class="n">sag_solver</span>
<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="k">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">..svm.base</span> <span class="k">import</span> <span class="n">_fit_liblinear</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_consistent_length</span><span class="p">,</span> <span class="n">compute_class_weight</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="k">import</span> <span class="p">(</span><span class="n">log_logistic</span><span class="p">,</span> <span class="n">safe_sparse_dot</span><span class="p">,</span> <span class="n">softmax</span><span class="p">,</span>
                             <span class="n">squared_norm</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="k">import</span> <span class="n">row_norms</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="k">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">..utils.optimize</span> <span class="k">import</span> <span class="n">newton_cg</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="k">import</span> <span class="n">NotFittedError</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="k">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">..externals.joblib</span> <span class="k">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">..model_selection</span> <span class="k">import</span> <span class="n">check_cv</span>
<span class="kn">from</span> <span class="nn">..externals</span> <span class="k">import</span> <span class="n">six</span>
<span class="kn">from</span> <span class="nn">..metrics</span> <span class="k">import</span> <span class="n">SCORERS</span>


<span class="c1"># .. some helper functions for logistic_regression_path ..</span>
<span class="k">def</span> <span class="nf">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes y * np.dot(X, w).</span>

<span class="sd">    It takes into consideration if the intercept should be fit or not.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray, shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    w : ndarray, shape (n_features,)</span>
<span class="sd">        Coefficient vector without the intercept weight (w[-1]) if the</span>
<span class="sd">        intercept should be fit. Unchanged otherwise.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data. Unchanged.</span>

<span class="sd">    yz : float</span>
<span class="sd">        y * np.dot(X, w).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">c</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">if</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
    <span class="n">yz</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span>


<span class="k">def</span> <span class="nf">_logistic_loss_and_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the logistic loss and gradient.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray, shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : float</span>
<span class="sd">        Logistic loss.</span>

<span class="sd">    grad : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Logistic gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>

    <span class="c1"># Logistic loss is the negative of the log of the logistic function.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">log_logistic</span><span class="p">(</span><span class="n">yz</span><span class="p">))</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">yz</span><span class="p">)</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>

    <span class="n">grad</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">w</span>

    <span class="c1"># Case where we fit the intercept.</span>
    <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">n_features</span><span class="p">:</span>
        <span class="n">grad</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">z0</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">grad</span>


<span class="k">def</span> <span class="nf">_logistic_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the logistic loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray, shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    out : float</span>
<span class="sd">        Logistic loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Logistic loss is the negative of the log of the logistic function.</span>
    <span class="n">out</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">log_logistic</span><span class="p">(</span><span class="n">yz</span><span class="p">))</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_logistic_grad_hess</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the gradient and the Hessian, in the case of a logistic loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : ndarray, shape (n_samples,)</span>
<span class="sd">        Array of labels.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad : ndarray, shape (n_features,) or (n_features + 1,)</span>
<span class="sd">        Logistic gradient.</span>

<span class="sd">    Hs : callable</span>
<span class="sd">        Function that takes the gradient as a parameter and returns the</span>
<span class="sd">        matrix product of the Hessian and gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">n_features</span>

    <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">yz</span> <span class="o">=</span> <span class="n">_intercept_dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">yz</span><span class="p">)</span>
    <span class="n">z0</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>

    <span class="n">grad</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z0</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">w</span>

    <span class="c1"># Case where we fit the intercept.</span>
    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">grad</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">z0</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># The mat-vec product of the Hessian</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sparse</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">sparse</span><span class="o">.</span><span class="n">dia_matrix</span><span class="p">((</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                             <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)),</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Precompute as much as possible</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">d</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span>

    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="c1"># Calculate the double derivative with respect to intercept</span>
        <span class="c1"># In the case of sparse matrices this returns a matrix object.</span>
        <span class="n">dd_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dX</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">Hs</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">ret</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dX</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]))</span>
        <span class="n">ret</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">s</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span>

        <span class="c1"># For the fit intercept case.</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">ret</span><span class="p">[:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dd_intercept</span>
            <span class="n">ret</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">dd_intercept</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="n">n_features</span><span class="p">])</span>
            <span class="n">ret</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">Hs</span>


<span class="k">def</span> <span class="nf">_multinomial_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes multinomial loss and class probabilities.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    Y : ndarray, shape (n_samples, n_classes)</span>
<span class="sd">        Transformed labels according to the output of LabelBinarizer.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>
<span class="sd">        Multinomial loss.</span>

<span class="sd">    p : ndarray, shape (n_samples, n_classes)</span>
<span class="sd">        Estimated class probabilities.</span>

<span class="sd">    w : ndarray, shape (n_classes, n_features)</span>
<span class="sd">        Reshaped param vector excluding intercept terms.</span>

<span class="sd">    Reference</span>
<span class="sd">    ---------</span>
<span class="sd">    Bishop, C. M. (2006). Pattern recognition and machine learning.</span>
<span class="sd">    Springer. (Chapter 4.3.4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">intercept</span>
    <span class="n">p</span> <span class="o">-=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">Y</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">squared_norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span>


<span class="k">def</span> <span class="nf">_multinomial_loss_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes the multinomial loss, gradient and class probabilities.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    Y : ndarray, shape (n_samples, n_classes)</span>
<span class="sd">        Transformed labels according to the output of LabelBinarizer.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>
<span class="sd">        Multinomial loss.</span>

<span class="sd">    grad : ndarray, shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Ravelled gradient of the multinomial loss.</span>

<span class="sd">    p : ndarray, shape (n_samples, n_classes)</span>
<span class="sd">        Estimated class probabilities</span>

<span class="sd">    Reference</span>
<span class="sd">    ---------</span>
<span class="sd">    Bishop, C. M. (2006). Pattern recognition and machine learning.</span>
<span class="sd">    Springer. (Chapter 4.3.4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">n_classes</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">)),</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">_multinomial_loss</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">w</span>
    <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">grad</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">_multinomial_grad_hess</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradient and the Hessian, in the case of a multinomial loss.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    w : ndarray, shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Coefficient vector.</span>

<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    Y : ndarray, shape (n_samples, n_classes)</span>
<span class="sd">        Transformed labels according to the output of LabelBinarizer.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Regularization parameter. alpha is equal to 1 / C.</span>

<span class="sd">    sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad : array, shape (n_classes * n_features,) or</span>
<span class="sd">        (n_classes * (n_features + 1),)</span>
<span class="sd">        Ravelled gradient of the multinomial loss.</span>

<span class="sd">    hessp : callable</span>
<span class="sd">        Function that takes in a vector input of shape (n_classes * n_features)</span>
<span class="sd">        or (n_classes * (n_features + 1)) and returns matrix-vector product</span>
<span class="sd">        with hessian.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Barak A. Pearlmutter (1993). Fast Exact Multiplication by the Hessian.</span>
<span class="sd">        http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># `loss` is unused. Refactoring to avoid computing it does not</span>
    <span class="c1"># significantly speed up the computation and decreases readability</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">_multinomial_loss_grad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="c1"># Hessian-vector product derived by applying the R-operator on the gradient</span>
    <span class="c1"># of the multinomial loss function.</span>
    <span class="k">def</span> <span class="nf">hessp</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">inter_terms</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inter_terms</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># r_yhat holds the result of applying the R-operator on the multinomial</span>
        <span class="c1"># estimator.</span>
        <span class="n">r_yhat</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">r_yhat</span> <span class="o">+=</span> <span class="n">inter_terms</span>
        <span class="n">r_yhat</span> <span class="o">+=</span> <span class="p">(</span><span class="o">-</span><span class="n">p</span> <span class="o">*</span> <span class="n">r_yhat</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">r_yhat</span> <span class="o">*=</span> <span class="n">p</span>
        <span class="n">r_yhat</span> <span class="o">*=</span> <span class="n">sample_weight</span>
        <span class="n">hessProd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">)))</span>
        <span class="n">hessProd</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">r_yhat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="n">hessProd</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_features</span><span class="p">]</span> <span class="o">+=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">alpha</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">hessProd</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">r_yhat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hessProd</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hessp</span>


<span class="k">def</span> <span class="nf">_check_solver_option</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">multi_class</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">,</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Logistic Regression supports only liblinear, &quot;</span>
                         <span class="s2">&quot;newton-cg, lbfgs, sag and saga solvers, got </span><span class="si">%s</span><span class="s2">&quot;</span>
                         <span class="o">%</span> <span class="n">solver</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">multi_class</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="s1">&#39;ovr&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;multi_class should be either multinomial or &quot;</span>
                         <span class="s2">&quot;ovr, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">multi_class</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span> <span class="ow">and</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Solver </span><span class="si">%s</span><span class="s2"> does not support &quot;</span>
                         <span class="s2">&quot;a multinomial backend.&quot;</span> <span class="o">%</span> <span class="n">solver</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">penalty</span> <span class="o">!=</span> <span class="s1">&#39;l2&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Solver </span><span class="si">%s</span><span class="s2"> supports only l2 penalties, &quot;</span>
                             <span class="s2">&quot;got </span><span class="si">%s</span><span class="s2"> penalty.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">penalty</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">solver</span> <span class="o">!=</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dual</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Solver </span><span class="si">%s</span><span class="s2"> supports only &quot;</span>
                             <span class="s2">&quot;dual=False, got dual=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">dual</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">logistic_regression_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                             <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span>
                             <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span>
                             <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">max_squared_sum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute a Logistic Regression model for a list of regularization</span>
<span class="sd">    parameters.</span>

<span class="sd">    This is an implementation that uses the result of the previous model</span>
<span class="sd">    to speed up computations along the set of solutions, making it faster</span>
<span class="sd">    than sequentially calling LogisticRegression for the different parameters.</span>
<span class="sd">    Note that there will be no speedup with liblinear solver, since it does</span>
<span class="sd">    not handle warm-starting.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape (n_samples, n_features)</span>
<span class="sd">        Input data.</span>

<span class="sd">    y : array-like, shape (n_samples,)</span>
<span class="sd">        Input data, target values.</span>

<span class="sd">    pos_class : int, None</span>
<span class="sd">        The class with respect to which we perform a one-vs-all fit.</span>
<span class="sd">        If None, then it is assumed that the given problem is binary.</span>

<span class="sd">    Cs : int | array-like, shape (n_cs,)</span>
<span class="sd">        List of values for the regularization parameter or integer specifying</span>
<span class="sd">        the number of regularization parameters that should be used. In this</span>
<span class="sd">        case, the parameters will be chosen in a logarithmic scale between</span>
<span class="sd">        1e-4 and 1e4.</span>

<span class="sd">    fit_intercept : bool</span>
<span class="sd">        Whether to fit an intercept for the model. In this case the shape of</span>
<span class="sd">        the returned array is (n_cs, n_features + 1).</span>

<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum number of iterations for the solver.</span>

<span class="sd">    tol : float</span>
<span class="sd">        Stopping criterion. For the newton-cg and lbfgs solvers, the iteration</span>
<span class="sd">        will stop when ``max{|g_i | i = 1, ..., n} &lt;= tol``</span>
<span class="sd">        where ``g_i`` is the i-th component of the gradient.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        For the liblinear and lbfgs solvers set verbose to any positive</span>
<span class="sd">        number for verbosity.</span>

<span class="sd">    solver : {&#39;lbfgs&#39;, &#39;newton-cg&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;}</span>
<span class="sd">        Numerical solver to use.</span>

<span class="sd">    coef : array-like, shape (n_features,), default None</span>
<span class="sd">        Initialization value for coefficients of logistic regression.</span>
<span class="sd">        Useless for liblinear solver.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, optional</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    dual : bool</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    penalty : str, &#39;l1&#39; or &#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties.</span>

<span class="sd">    intercept_scaling : float, default 1.</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equal to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.</span>

<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    multi_class : str, {&#39;ovr&#39;, &#39;multinomial&#39;}</span>
<span class="sd">        Multiclass option can be either &#39;ovr&#39; or &#39;multinomial&#39;. If the option</span>
<span class="sd">        chosen is &#39;ovr&#39;, then a binary problem is fit for each label. Else</span>
<span class="sd">        the loss minimised is the multinomial loss fit across</span>
<span class="sd">        the entire probability distribution. Works only for the &#39;lbfgs&#39; and</span>
<span class="sd">        &#39;newton-cg&#39; solvers.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional, default None</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`. Used when ``solver`` == &#39;sag&#39; or</span>
<span class="sd">        &#39;liblinear&#39;.</span>

<span class="sd">    check_input : bool, default True</span>
<span class="sd">        If False, the input arrays X and y will not be checked.</span>

<span class="sd">    max_squared_sum : float, default None</span>
<span class="sd">        Maximum squared sum of X over samples. Used only in SAG solver.</span>
<span class="sd">        If None, it will be computed, going through all the samples.</span>
<span class="sd">        The value should be precomputed to speed up cross validation.</span>

<span class="sd">    sample_weight : array-like, shape(n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)</span>
<span class="sd">        List of coefficients for the Logistic Regression model. If</span>
<span class="sd">        fit_intercept is set to True then the second dimension will be</span>
<span class="sd">        n_features + 1, where the last item represents the intercept.</span>

<span class="sd">    Cs : ndarray</span>
<span class="sd">        Grid of Cs used for cross-validation.</span>

<span class="sd">    n_iter : array, shape (n_cs,)</span>
<span class="sd">        Actual number of iteration for each Cs.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    You might get slightly different results with the solver liblinear than</span>
<span class="sd">    with the others since this uses LIBLINEAR which penalizes the intercept.</span>

<span class="sd">    .. versionchanged:: 0.19</span>
<span class="sd">        The &quot;copy&quot; parameter was removed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">Cs</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
        <span class="n">Cs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Cs</span><span class="p">)</span>

    <span class="n">_check_solver_option</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">multi_class</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">)</span>

    <span class="c1"># Preprocessing.</span>
    <span class="k">if</span> <span class="n">check_input</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pos_class</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">multi_class</span> <span class="o">!=</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;To fit OvR, use the pos_class argument&#39;</span><span class="p">)</span>
        <span class="c1"># np.unique(y) gives labels in sorted order.</span>
        <span class="n">pos_class</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># If sample weights exist, convert them to array (support for lists)</span>
    <span class="c1"># and check length</span>
    <span class="c1"># Otherwise set them to 1 for all examples</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># If class_weights is a dict (provided by the user), the weights</span>
    <span class="c1"># are assigned to the original labels. If it is &quot;balanced&quot;, then</span>
    <span class="c1"># the class_weights are assigned after masking the labels with a OvR.</span>
    <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="n">class_weight_</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">class_weight_</span><span class="p">[</span><span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)]</span>

    <span class="c1"># For doing a ovr, we need to mask the labels first. for the</span>
    <span class="c1"># multinomial case this is not necessary.</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
        <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">mask_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">pos_class</span><span class="p">)</span>
        <span class="n">y_bin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">y_bin</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>
        <span class="c1"># for compute_class_weight</span>

        <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced&quot;</span><span class="p">:</span>
            <span class="n">class_weight_</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">mask_classes</span><span class="p">,</span>
                                                 <span class="n">y_bin</span><span class="p">)</span>
            <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">class_weight_</span><span class="p">[</span><span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_bin</span><span class="p">)]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">lbin</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>
            <span class="n">Y_multi</span> <span class="o">=</span> <span class="n">lbin</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">Y_multi</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">Y_multi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y_multi</span><span class="p">,</span> <span class="n">Y_multi</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># SAG multinomial solver needs LabelEncoder, not LabelBinarizer</span>
            <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
            <span class="n">Y_multi</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">fit_intercept</span><span class="p">)),</span>
                      <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">coef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># it must work both giving the bias term and not</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">coef</span><span class="o">.</span><span class="n">size</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">w0</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Initialization coef is of shape </span><span class="si">%d</span><span class="s1">, expected shape &#39;</span>
                    <span class="s1">&#39;</span><span class="si">%d</span><span class="s1"> or </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">w0</span><span class="o">.</span><span class="n">size</span><span class="p">))</span>
            <span class="n">w0</span><span class="p">[:</span><span class="n">coef</span><span class="o">.</span><span class="n">size</span><span class="p">]</span> <span class="o">=</span> <span class="n">coef</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For binary problems coef.shape[0] should be 1, otherwise it</span>
            <span class="c1"># should be classes.size.</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span>
            <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_classes</span> <span class="ow">or</span>
                    <span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Initialization coef is of shape (</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">), expected &#39;</span>
                    <span class="s1">&#39;shape (</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">) or (</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span>
                        <span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>
                        <span class="n">n_features</span><span class="p">,</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">w0</span><span class="p">[:,</span> <span class="p">:</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">coef</span>

    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="c1"># fmin_l_bfgs_b and newton-cg accepts only ravelled parameters.</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">]:</span>
            <span class="n">w0</span> <span class="o">=</span> <span class="n">w0</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">Y_multi</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_multinomial_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_multinomial_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_multinomial_loss_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="n">_multinomial_grad_hess</span>
        <span class="n">warm_start_sag</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;coef&#39;</span><span class="p">:</span> <span class="n">w0</span><span class="o">.</span><span class="n">T</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">y_bin</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="n">_logistic_loss_and_grad</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">:</span>
            <span class="n">func</span> <span class="o">=</span> <span class="n">_logistic_loss</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">_logistic_loss_and_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="n">_logistic_grad_hess</span>
        <span class="n">warm_start_sag</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;coef&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)}</span>

    <span class="n">coefs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">n_iter</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Cs</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">C</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">Cs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;lbfgs&#39;</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">w0</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">fmin_l_bfgs_b</span><span class="p">(</span>
                    <span class="n">func</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">fprime</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">),</span>
                    <span class="n">iprint</span><span class="o">=</span><span class="p">(</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pgtol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
                <span class="c1"># old scipy doesn&#39;t have maxiter</span>
                <span class="n">w0</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">fmin_l_bfgs_b</span><span class="p">(</span>
                    <span class="n">func</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">fprime</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">),</span>
                    <span class="n">iprint</span><span class="o">=</span><span class="p">(</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pgtol</span><span class="o">=</span><span class="n">tol</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">info</span><span class="p">[</span><span class="s2">&quot;warnflag&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;lbfgs failed to converge. Increase the number &quot;</span>
                              <span class="s2">&quot;of iterations.&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">n_iter_i</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;nit&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">n_iter_i</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;funcalls&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;newton-cg&#39;</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
            <span class="n">w0</span><span class="p">,</span> <span class="n">n_iter_i</span> <span class="o">=</span> <span class="n">newton_cg</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
                                     <span class="n">maxiter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
            <span class="n">coef_</span><span class="p">,</span> <span class="n">intercept_</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">,</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
                <span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">intercept_</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">w0</span> <span class="o">=</span> <span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

        <span class="k">elif</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;multinomial&#39;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;log&#39;</span>
            <span class="k">if</span> <span class="n">penalty</span> <span class="o">==</span> <span class="s1">&#39;l1&#39;</span><span class="p">:</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">C</span>
                <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.</span>
            <span class="n">w0</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">,</span> <span class="n">warm_start_sag</span> <span class="o">=</span> <span class="n">sag_solver</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span>
                <span class="n">beta</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                <span class="n">verbose</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">max_squared_sum</span><span class="p">,</span> <span class="n">warm_start_sag</span><span class="p">,</span>
                <span class="n">is_saga</span><span class="o">=</span><span class="p">(</span><span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;saga&#39;</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;solver must be one of {&#39;liblinear&#39;, &#39;lbfgs&#39;, &quot;</span>
                             <span class="s2">&quot;&#39;newton-cg&#39;, &#39;sag&#39;}, got &#39;</span><span class="si">%s</span><span class="s2">&#39; instead&quot;</span> <span class="o">%</span> <span class="n">solver</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">multi_w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">classes</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">multi_w0</span> <span class="o">=</span> <span class="n">multi_w0</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">multi_w0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w0</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

        <span class="n">n_iter</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_iter_i</span>

    <span class="k">return</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Cs</span><span class="p">),</span> <span class="n">n_iter</span>


<span class="c1"># helper function for LogisticCV</span>
<span class="k">def</span> <span class="nf">_log_reg_scoring_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                          <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                          <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span>
                          <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                          <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">max_squared_sum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes scores across logistic_regression_path</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : array-like, shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">        Target labels.</span>

<span class="sd">    train : list of indices</span>
<span class="sd">        The indices of the train set.</span>

<span class="sd">    test : list of indices</span>
<span class="sd">        The indices of the test set.</span>

<span class="sd">    pos_class : int, None</span>
<span class="sd">        The class with respect to which we perform a one-vs-all fit.</span>
<span class="sd">        If None, then it is assumed that the given problem is binary.</span>

<span class="sd">    Cs : list of floats | int</span>
<span class="sd">        Each of the values in Cs describes the inverse of</span>
<span class="sd">        regularization strength. If Cs is as an int, then a grid of Cs</span>
<span class="sd">        values are chosen in a logarithmic scale between 1e-4 and 1e4.</span>
<span class="sd">        If not provided, then a fixed set of values for Cs are used.</span>

<span class="sd">    scoring : callable or None, optional, default: None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``. For a list of scoring functions</span>
<span class="sd">        that can be used, look at :mod:`sklearn.metrics`. The</span>
<span class="sd">        default scoring option used is accuracy_score.</span>

<span class="sd">    fit_intercept : bool</span>
<span class="sd">        If False, then the bias term is set to zero. Else the last</span>
<span class="sd">        term of each coef_ gives us the intercept.</span>

<span class="sd">    max_iter : int</span>
<span class="sd">        Maximum number of iterations for the solver.</span>

<span class="sd">    tol : float</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, optional</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        For the liblinear and lbfgs solvers set verbose to any positive</span>
<span class="sd">        number for verbosity.</span>

<span class="sd">    solver : {&#39;lbfgs&#39;, &#39;newton-cg&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;}</span>
<span class="sd">        Decides which solver to use.</span>

<span class="sd">    penalty : str, &#39;l1&#39; or &#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties.</span>

<span class="sd">    dual : bool</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    intercept_scaling : float, default 1.</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equals to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes intercept_scaling * synthetic feature weight</span>
<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    multi_class : str, {&#39;ovr&#39;, &#39;multinomial&#39;}</span>
<span class="sd">        Multiclass option can be either &#39;ovr&#39; or &#39;multinomial&#39;. If the option</span>
<span class="sd">        chosen is &#39;ovr&#39;, then a binary problem is fit for each label. Else</span>
<span class="sd">        the loss minimised is the multinomial loss fit across</span>
<span class="sd">        the entire probability distribution. Does not work for</span>
<span class="sd">        liblinear solver.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional, default None</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`. Used when ``solver`` == &#39;sag&#39; and</span>
<span class="sd">        &#39;liblinear&#39;.</span>

<span class="sd">    max_squared_sum : float, default None</span>
<span class="sd">        Maximum squared sum of X over samples. Used only in SAG solver.</span>
<span class="sd">        If None, it will be computed, going through all the samples.</span>
<span class="sd">        The value should be precomputed to speed up cross validation.</span>

<span class="sd">    sample_weight : array-like, shape(n_samples,) optional</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1)</span>
<span class="sd">        List of coefficients for the Logistic Regression model. If</span>
<span class="sd">        fit_intercept is set to True then the second dimension will be</span>
<span class="sd">        n_features + 1, where the last item represents the intercept.</span>

<span class="sd">    Cs : ndarray</span>
<span class="sd">        Grid of Cs used for cross-validation.</span>

<span class="sd">    scores : ndarray, shape (n_cs,)</span>
<span class="sd">        Scores obtained for each Cs.</span>

<span class="sd">    n_iter : array, shape(n_cs,)</span>
<span class="sd">        Actual number of iteration for each Cs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_solver_option</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">multi_class</span><span class="p">,</span> <span class="n">penalty</span><span class="p">,</span> <span class="n">dual</span><span class="p">)</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>

    <span class="n">coefs</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="n">logistic_regression_path</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="n">Cs</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
        <span class="n">pos_class</span><span class="o">=</span><span class="n">pos_class</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span>
        <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="n">dual</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
        <span class="n">intercept_scaling</span><span class="o">=</span><span class="n">intercept_scaling</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">)</span>

    <span class="c1"># The score method of Logistic Regression has a classes_ attribute.</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
        <span class="n">log_reg</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
        <span class="n">log_reg</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;multi_class should be either multinomial or ovr, &quot;</span>
                         <span class="s2">&quot;got </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">multi_class</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pos_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">pos_class</span><span class="p">)</span>
        <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y_test</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scoring</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
        <span class="n">scoring</span> <span class="o">=</span> <span class="n">SCORERS</span><span class="p">[</span><span class="n">scoring</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">coefs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">if</span> <span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span>
            <span class="n">log_reg</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="k">if</span> <span class="n">scoring</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scoring</span><span class="p">(</span><span class="n">log_reg</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">n_iter</span>


<div class="viewcode-block" id="LogisticRegression"><a class="viewcode-back" href="../../../api_ibex_sklearn_linear_model_logisticregression.html#ibex.sklearn.linear_model.LogisticRegression">[docs]</a><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span>
                         <span class="n">SparseCoefMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logistic Regression (aka logit, MaxEnt) classifier.</span>

<span class="sd">    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)</span>
<span class="sd">    scheme if the &#39;multi_class&#39; option is set to &#39;ovr&#39;, and uses the cross-</span>
<span class="sd">    entropy loss if the &#39;multi_class&#39; option is set to &#39;multinomial&#39;.</span>
<span class="sd">    (Currently the &#39;multinomial&#39; option is supported only by the &#39;lbfgs&#39;,</span>
<span class="sd">    &#39;sag&#39; and &#39;newton-cg&#39; solvers.)</span>

<span class="sd">    This class implements regularized logistic regression using the</span>
<span class="sd">    &#39;liblinear&#39; library, &#39;newton-cg&#39;, &#39;sag&#39; and &#39;lbfgs&#39; solvers. It can handle</span>
<span class="sd">    both dense and sparse input. Use C-ordered arrays or CSR matrices</span>
<span class="sd">    containing 64-bit floats for optimal performance; any other input format</span>
<span class="sd">    will be converted (and copied).</span>

<span class="sd">    The &#39;newton-cg&#39;, &#39;sag&#39;, and &#39;lbfgs&#39; solvers support only L2 regularization</span>
<span class="sd">    with primal formulation. The &#39;liblinear&#39; solver supports both L1 and L2</span>
<span class="sd">    regularization, with a dual formulation only for the L2 penalty.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    penalty : str, &#39;l1&#39; or &#39;l2&#39;, default: &#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties.</span>

<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           l1 penalty with SAGA solver (allowing &#39;multinomial&#39; + L1)</span>

<span class="sd">    dual : bool, default: False</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    tol : float, default: 1e-4</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    C : float, default: 1.0</span>
<span class="sd">        Inverse of regularization strength; must be a positive float.</span>
<span class="sd">        Like in support vector machines, smaller values specify stronger</span>
<span class="sd">        regularization.</span>

<span class="sd">    fit_intercept : bool, default: True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added to the decision function.</span>

<span class="sd">    intercept_scaling : float, default 1.</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equal to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.</span>

<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, default: None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *class_weight=&#39;balanced&#39;*</span>

<span class="sd">    random_state : int, RandomState instance or None, optional, default: None</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`. Used when ``solver`` == &#39;sag&#39; or</span>
<span class="sd">        &#39;liblinear&#39;.</span>

<span class="sd">    solver : {&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;},</span>
<span class="sd">        default: &#39;liblinear&#39;</span>
<span class="sd">        Algorithm to use in the optimization problem.</span>

<span class="sd">        - For small datasets, &#39;liblinear&#39; is a good choice, whereas &#39;sag&#39; and</span>
<span class="sd">            &#39;saga&#39; are faster for large ones.</span>
<span class="sd">        - For multiclass problems, only &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39;</span>
<span class="sd">            handle multinomial loss; &#39;liblinear&#39; is limited to one-versus-rest</span>
<span class="sd">            schemes.</span>
<span class="sd">        - &#39;newton-cg&#39;, &#39;lbfgs&#39; and &#39;sag&#39; only handle L2 penalty, whereas</span>
<span class="sd">            &#39;liblinear&#39; and &#39;saga&#39; handle L1 penalty.</span>

<span class="sd">        Note that &#39;sag&#39; and &#39;saga&#39; fast convergence is only guaranteed on</span>
<span class="sd">        features with approximately the same scale. You can</span>
<span class="sd">        preprocess the data with a scaler from sklearn.preprocessing.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Stochastic Average Gradient descent solver.</span>
<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           SAGA solver.</span>

<span class="sd">    max_iter : int, default: 100</span>
<span class="sd">        Useful only for the newton-cg, sag and lbfgs solvers.</span>
<span class="sd">        Maximum number of iterations taken for the solvers to converge.</span>

<span class="sd">    multi_class : str, {&#39;ovr&#39;, &#39;multinomial&#39;}, default: &#39;ovr&#39;</span>
<span class="sd">        Multiclass option can be either &#39;ovr&#39; or &#39;multinomial&#39;. If the option</span>
<span class="sd">        chosen is &#39;ovr&#39;, then a binary problem is fit for each label. Else</span>
<span class="sd">        the loss minimised is the multinomial loss fit across</span>
<span class="sd">        the entire probability distribution. Does not work for liblinear</span>
<span class="sd">        solver.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Stochastic Average Gradient descent solver for &#39;multinomial&#39; case.</span>

<span class="sd">    verbose : int, default: 0</span>
<span class="sd">        For the liblinear and lbfgs solvers set verbose to any positive</span>
<span class="sd">        number for verbosity.</span>

<span class="sd">    warm_start : bool, default: False</span>
<span class="sd">        When set to True, reuse the solution of the previous call to fit as</span>
<span class="sd">        initialization, otherwise, just erase the previous solution.</span>
<span class="sd">        Useless for liblinear solver.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.</span>

<span class="sd">    n_jobs : int, default: 1</span>
<span class="sd">        Number of CPU cores used when parallelizing over classes if</span>
<span class="sd">        multi_class=&#39;ovr&#39;&quot;. This parameter is ignored when the ``solver``is set</span>
<span class="sd">        to &#39;liblinear&#39; regardless of whether &#39;multi_class&#39; is specified or</span>
<span class="sd">        not. If given a value of -1, all cores are used.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>

<span class="sd">    coef_ : array, shape (1, n_features) or (n_classes, n_features)</span>
<span class="sd">        Coefficient of the features in the decision function.</span>

<span class="sd">        `coef_` is of shape (1, n_features) when the given problem</span>
<span class="sd">        is binary.</span>

<span class="sd">    intercept_ : array, shape (1,) or (n_classes,)</span>
<span class="sd">        Intercept (a.k.a. bias) added to the decision function.</span>

<span class="sd">        If `fit_intercept` is set to False, the intercept is set to zero.</span>
<span class="sd">        `intercept_` is of shape(1,) when the problem is binary.</span>

<span class="sd">    n_iter_ : array, shape (n_classes,) or (1, )</span>
<span class="sd">        Actual number of iterations for all classes. If binary or multinomial,</span>
<span class="sd">        it returns only 1 element. For liblinear solver, only the maximum</span>
<span class="sd">        number of iteration across all classes is given.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    SGDClassifier : incrementally trained logistic regression (when given</span>
<span class="sd">        the parameter ``loss=&quot;log&quot;``).</span>
<span class="sd">    sklearn.svm.LinearSVC : learns SVM models using the same algorithm.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The underlying C implementation uses a random number generator to</span>
<span class="sd">    select features when fitting the model. It is thus not uncommon,</span>
<span class="sd">    to have slightly different results for the same input data. If</span>
<span class="sd">    that happens, try with a smaller tol parameter.</span>

<span class="sd">    Predict output may not match that of standalone liblinear in certain</span>
<span class="sd">    cases. See :ref:`differences from liblinear &lt;liblinear_differences&gt;`</span>
<span class="sd">    in the narrative documentation.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    LIBLINEAR -- A Library for Large Linear Classification</span>
<span class="sd">        http://www.csie.ntu.edu.tw/~cjlin/liblinear/</span>

<span class="sd">    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach</span>
<span class="sd">        Minimizing Finite Sums with the Stochastic Average Gradient</span>
<span class="sd">        https://hal.inria.fr/hal-00860051/document</span>

<span class="sd">    SAGA -- Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).</span>
<span class="sd">        SAGA: A Fast Incremental Gradient Method With Support</span>
<span class="sd">        for Non-Strongly Convex Composite Objectives</span>
<span class="sd">        https://arxiv.org/abs/1407.0202</span>

<span class="sd">    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</span>
<span class="sd">        methods for logistic regression and maximum entropy models.</span>
<span class="sd">        Machine Learning 85(1-2):41-75.</span>
<span class="sd">        http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span> <span class="o">=</span> <span class="n">intercept_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">=</span> <span class="n">solver</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">=</span> <span class="n">multi_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

<div class="viewcode-block" id="LogisticRegression.fit"><a class="viewcode-back" href="../../../api_ibex_sklearn_linear_model_logisticregression.html#ibex.sklearn.linear_model.LogisticRegression.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target vector relative to X.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">            Array of weights that are assigned to individual samples.</span>
<span class="sd">            If not provided, then each sample is given unit weight.</span>

<span class="sd">            .. versionadded:: 0.17</span>
<span class="sd">               *sample_weight* support to LogisticRegression.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration must be positive;&quot;</span>
                             <span class="s2">&quot; got (max_iter=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tolerance for stopping criteria must be &quot;</span>
                             <span class="s2">&quot;positive; got (tol=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;newton-cg&#39;</span><span class="p">]:</span>
            <span class="n">_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">_dtype</span><span class="p">,</span>
                         <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">_check_solver_option</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s1">&#39;liblinear&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;&#39;n_jobs&#39; &gt; 1 does not have any effect when&quot;</span>
                              <span class="s2">&quot; &#39;solver&#39; is set to &#39;liblinear&#39;. Got &#39;n_jobs&#39;&quot;</span>
                              <span class="s2">&quot; = </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n_iter_</span><span class="p">])</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        <span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This solver needs samples of at least 2 classes&quot;</span>
                             <span class="s2">&quot; in the data, but the data contains only one&quot;</span>
                             <span class="s2">&quot; class: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">classes_</span> <span class="o">=</span> <span class="n">classes_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;coef_&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">warm_start_coef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">warm_start_coef</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                                        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>

        <span class="c1"># Hack so that we iterate only once for the multinomial case.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">classes_</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="p">[</span><span class="n">warm_start_coef</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">warm_start_coef</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_classes</span>

        <span class="n">path_func</span> <span class="o">=</span> <span class="n">delayed</span><span class="p">(</span><span class="n">logistic_regression_path</span><span class="p">)</span>

        <span class="c1"># The SAG solver releases the GIL so it&#39;s more efficient to use</span>
        <span class="c1"># threads for this solver.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;threading&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;multiprocessing&#39;</span>
        <span class="n">fold_coefs_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                               <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)(</span>
            <span class="n">path_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">class_</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">],</span>
                      <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                      <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span>
                      <span class="n">multi_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                      <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="n">warm_start_coef_</span><span class="p">,</span>
                      <span class="n">penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                      <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">class_</span><span class="p">,</span> <span class="n">warm_start_coef_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">classes_</span><span class="p">,</span> <span class="n">warm_start_coef</span><span class="p">))</span>

        <span class="n">fold_coefs_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">fold_coefs_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">fold_coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fold_coefs_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span>
                                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="LogisticRegression.predict_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_linear_model_logisticregression.html#ibex.sklearn.linear_model.LogisticRegression.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability estimates.</span>

<span class="sd">        The returned estimates for all classes are ordered by the</span>
<span class="sd">        label of classes.</span>

<span class="sd">        For a multi_class problem, if multi_class is set to be &quot;multinomial&quot;</span>
<span class="sd">        the softmax function is used to find the predicted probability of</span>
<span class="sd">        each class.</span>
<span class="sd">        Else use a one-vs-rest approach, i.e calculate the probability</span>
<span class="sd">        of each class assuming it to be positive using the logistic function.</span>
<span class="sd">        and normalize these values across all the classes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape = [n_samples, n_classes]</span>
<span class="sd">            Returns the probability of the sample for each class in the model,</span>
<span class="sd">            where classes are ordered as they are in ``self.classes_``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">NotFittedError</span><span class="p">(</span><span class="s2">&quot;Call fit before prediction&quot;</span><span class="p">)</span>
        <span class="n">calculate_ovr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s2">&quot;ovr&quot;</span>
        <span class="k">if</span> <span class="n">calculate_ovr</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_predict_proba_lr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>

<div class="viewcode-block" id="LogisticRegression.predict_log_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_linear_model_logisticregression.html#ibex.sklearn.linear_model.LogisticRegression.predict_log_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Log of probability estimates.</span>

<span class="sd">        The returned estimates for all classes are ordered by the</span>
<span class="sd">        label of classes.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape = [n_samples, n_classes]</span>
<span class="sd">            Returns the log-probability of the sample for each class in the</span>
<span class="sd">            model, where classes are ordered as they are in ``self.classes_``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span></div></div>


<span class="k">class</span> <span class="nc">LogisticRegressionCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">,</span>
                           <span class="n">LinearClassifierMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logistic Regression CV (aka logit, MaxEnt) classifier.</span>

<span class="sd">    This class implements logistic regression using liblinear, newton-cg, sag</span>
<span class="sd">    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2</span>
<span class="sd">    regularization with primal formulation. The liblinear solver supports both</span>
<span class="sd">    L1 and L2 regularization, with a dual formulation only for the L2 penalty.</span>

<span class="sd">    For the grid of Cs values (that are set by default to be ten values in</span>
<span class="sd">    a logarithmic scale between 1e-4 and 1e4), the best hyperparameter is</span>
<span class="sd">    selected by the cross-validator StratifiedKFold, but it can be changed</span>
<span class="sd">    using the cv parameter. In the case of newton-cg and lbfgs solvers,</span>
<span class="sd">    we warm start along the path i.e guess the initial coefficients of the</span>
<span class="sd">    present fit to be the coefficients got after convergence in the previous</span>
<span class="sd">    fit, so it is supposed to be faster for high-dimensional dense data.</span>

<span class="sd">    For a multiclass problem, the hyperparameters for each class are computed</span>
<span class="sd">    using the best scores got by doing a one-vs-rest in parallel across all</span>
<span class="sd">    folds and classes. Hence this is not the true multinomial loss.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;logistic_regression&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Cs : list of floats | int</span>
<span class="sd">        Each of the values in Cs describes the inverse of regularization</span>
<span class="sd">        strength. If Cs is as an int, then a grid of Cs values are chosen</span>
<span class="sd">        in a logarithmic scale between 1e-4 and 1e4.</span>
<span class="sd">        Like in support vector machines, smaller values specify stronger</span>
<span class="sd">        regularization.</span>

<span class="sd">    fit_intercept : bool, default: True</span>
<span class="sd">        Specifies if a constant (a.k.a. bias or intercept) should be</span>
<span class="sd">        added to the decision function.</span>

<span class="sd">    cv : integer or cross-validation generator</span>
<span class="sd">        The default cross-validation generator used is Stratified K-Folds.</span>
<span class="sd">        If an integer is provided, then it is the number of folds used.</span>
<span class="sd">        See the module :mod:`sklearn.model_selection` module for the</span>
<span class="sd">        list of possible cross-validation objects.</span>

<span class="sd">    dual : bool</span>
<span class="sd">        Dual or primal formulation. Dual formulation is only implemented for</span>
<span class="sd">        l2 penalty with liblinear solver. Prefer dual=False when</span>
<span class="sd">        n_samples &gt; n_features.</span>

<span class="sd">    penalty : str, &#39;l1&#39; or &#39;l2&#39;</span>
<span class="sd">        Used to specify the norm used in the penalization. The &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39; and &#39;lbfgs&#39; solvers support only l2 penalties.</span>

<span class="sd">    scoring : string, callable, or None</span>
<span class="sd">        A string (see model evaluation documentation) or</span>
<span class="sd">        a scorer callable object / function with signature</span>
<span class="sd">        ``scorer(estimator, X, y)``. For a list of scoring functions</span>
<span class="sd">        that can be used, look at :mod:`sklearn.metrics`. The</span>
<span class="sd">        default scoring option used is &#39;accuracy&#39;.</span>

<span class="sd">    solver : {&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;},</span>
<span class="sd">        default: &#39;liblinear&#39;</span>
<span class="sd">        Algorithm to use in the optimization problem.</span>

<span class="sd">        - For small datasets, &#39;liblinear&#39; is a good choice, whereas &#39;sag&#39; and</span>
<span class="sd">            &#39;saga&#39; are faster for large ones.</span>
<span class="sd">        - For multiclass problems, only &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39;</span>
<span class="sd">            handle multinomial loss; &#39;liblinear&#39; is limited to one-versus-rest</span>
<span class="sd">            schemes.</span>
<span class="sd">        - &#39;newton-cg&#39;, &#39;lbfgs&#39; and &#39;sag&#39; only handle L2 penalty, whereas</span>
<span class="sd">            &#39;liblinear&#39; and &#39;saga&#39; handle L1 penalty.</span>
<span class="sd">        - &#39;liblinear&#39; might be slower in LogisticRegressionCV because it does</span>
<span class="sd">            not handle warm-starting.</span>

<span class="sd">        Note that &#39;sag&#39; and &#39;saga&#39; fast convergence is only guaranteed on</span>
<span class="sd">        features with approximately the same scale. You can preprocess the data</span>
<span class="sd">        with a scaler from sklearn.preprocessing.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           Stochastic Average Gradient descent solver.</span>
<span class="sd">        .. versionadded:: 0.19</span>
<span class="sd">           SAGA solver.</span>

<span class="sd">    tol : float, optional</span>
<span class="sd">        Tolerance for stopping criteria.</span>

<span class="sd">    max_iter : int, optional</span>
<span class="sd">        Maximum number of iterations of the optimization algorithm.</span>

<span class="sd">    class_weight : dict or &#39;balanced&#39;, optional</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           class_weight == &#39;balanced&#39;</span>

<span class="sd">    n_jobs : int, optional</span>
<span class="sd">        Number of CPU cores used during the cross-validation loop. If given</span>
<span class="sd">        a value of -1, all cores are used.</span>

<span class="sd">    verbose : int</span>
<span class="sd">        For the &#39;liblinear&#39;, &#39;sag&#39; and &#39;lbfgs&#39; solvers set verbose to any</span>
<span class="sd">        positive number for verbosity.</span>

<span class="sd">    refit : bool</span>
<span class="sd">        If set to True, the scores are averaged across all folds, and the</span>
<span class="sd">        coefs and the C that corresponds to the best score is taken, and a</span>
<span class="sd">        final refit is done using these parameters.</span>
<span class="sd">        Otherwise the coefs, intercepts and C that correspond to the</span>
<span class="sd">        best scores across folds are averaged.</span>

<span class="sd">    intercept_scaling : float, default 1.</span>
<span class="sd">        Useful only when the solver &#39;liblinear&#39; is used</span>
<span class="sd">        and self.fit_intercept is set to True. In this case, x becomes</span>
<span class="sd">        [x, self.intercept_scaling],</span>
<span class="sd">        i.e. a &quot;synthetic&quot; feature with constant value equal to</span>
<span class="sd">        intercept_scaling is appended to the instance vector.</span>
<span class="sd">        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.</span>

<span class="sd">        Note! the synthetic feature weight is subject to l1/l2 regularization</span>
<span class="sd">        as all other features.</span>
<span class="sd">        To lessen the effect of regularization on synthetic feature weight</span>
<span class="sd">        (and therefore on the intercept) intercept_scaling has to be increased.</span>

<span class="sd">    multi_class : str, {&#39;ovr&#39;, &#39;multinomial&#39;}</span>
<span class="sd">        Multiclass option can be either &#39;ovr&#39; or &#39;multinomial&#39;. If the option</span>
<span class="sd">        chosen is &#39;ovr&#39;, then a binary problem is fit for each label. Else</span>
<span class="sd">        the loss minimised is the multinomial loss fit across</span>
<span class="sd">        the entire probability distribution. Works only for the &#39;newton-cg&#39;,</span>
<span class="sd">        &#39;sag&#39;, &#39;saga&#39; and &#39;lbfgs&#39; solver.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Stochastic Average Gradient descent solver for &#39;multinomial&#39; case.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional, default None</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape (1, n_features) or (n_classes, n_features)</span>
<span class="sd">        Coefficient of the features in the decision function.</span>

<span class="sd">        `coef_` is of shape (1, n_features) when the given problem</span>
<span class="sd">        is binary.</span>

<span class="sd">    intercept_ : array, shape (1,) or (n_classes,)</span>
<span class="sd">        Intercept (a.k.a. bias) added to the decision function.</span>

<span class="sd">        If `fit_intercept` is set to False, the intercept is set to zero.</span>
<span class="sd">        `intercept_` is of shape(1,) when the problem is binary.</span>

<span class="sd">    Cs_ : array</span>
<span class="sd">        Array of C i.e. inverse of regularization parameter values used</span>
<span class="sd">        for cross-validation.</span>

<span class="sd">    coefs_paths_ : array, shape ``(n_folds, len(Cs_), n_features)`` or \</span>
<span class="sd">                   ``(n_folds, len(Cs_), n_features + 1)``</span>
<span class="sd">        dict with classes as the keys, and the path of coefficients obtained</span>
<span class="sd">        during cross-validating across each fold and then across each Cs</span>
<span class="sd">        after doing an OvR for the corresponding class as values.</span>
<span class="sd">        If the &#39;multi_class&#39; option is set to &#39;multinomial&#39;, then</span>
<span class="sd">        the coefs_paths are the coefficients corresponding to each class.</span>
<span class="sd">        Each dict value has shape ``(n_folds, len(Cs_), n_features)`` or</span>
<span class="sd">        ``(n_folds, len(Cs_), n_features + 1)`` depending on whether the</span>
<span class="sd">        intercept is fit or not.</span>

<span class="sd">    scores_ : dict</span>
<span class="sd">        dict with classes as the keys, and the values as the</span>
<span class="sd">        grid of scores obtained during cross-validating each fold, after doing</span>
<span class="sd">        an OvR for the corresponding class. If the &#39;multi_class&#39; option</span>
<span class="sd">        given is &#39;multinomial&#39; then the same scores are repeated across</span>
<span class="sd">        all classes, since this is the multinomial class.</span>
<span class="sd">        Each dict value has shape (n_folds, len(Cs))</span>

<span class="sd">    C_ : array, shape (n_classes,) or (n_classes - 1,)</span>
<span class="sd">        Array of C that maps to the best scores across every class. If refit is</span>
<span class="sd">        set to False, then for each class, the best C is the average of the</span>
<span class="sd">        C&#39;s that correspond to the best scores for each fold.</span>
<span class="sd">        `C_` is of shape(n_classes,) when the problem is binary.</span>

<span class="sd">    n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</span>
<span class="sd">        Actual number of iterations for all classes, folds and Cs.</span>
<span class="sd">        In the binary or multinomial cases, the first dimension is equal to 1.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    LogisticRegression</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                 <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">intercept_scaling</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Cs</span> <span class="o">=</span> <span class="n">Cs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span> <span class="o">=</span> <span class="n">dual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">=</span> <span class="n">solver</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">refit</span> <span class="o">=</span> <span class="n">refit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span> <span class="o">=</span> <span class="n">intercept_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">=</span> <span class="n">multi_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

<div class="viewcode-block" id="LogisticRegressionCV.fit"><a class="viewcode-back" href="../../../api_ibex_sklearn_linear_model_logisticregressioncv.html#ibex.sklearn.linear_model.LogisticRegressionCV.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training vector, where n_samples is the number of samples and</span>
<span class="sd">            n_features is the number of features.</span>

<span class="sd">        y : array-like, shape (n_samples,)</span>
<span class="sd">            Target vector relative to X.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,) optional</span>
<span class="sd">            Array of weights that are assigned to individual samples.</span>
<span class="sd">            If not provided, then each sample is given unit weight.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">_check_solver_option</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration must be positive;&quot;</span>
                             <span class="s2">&quot; got (max_iter=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Tolerance for stopping criteria must be &quot;</span>
                             <span class="s2">&quot;positive; got (tol=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                         <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span>

        <span class="c1"># Encode for string labels</span>
        <span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">class_weight</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="bp">cls</span><span class="p">])[</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="p">)</span>
                                <span class="k">for</span> <span class="bp">cls</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">class_weight</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

        <span class="c1"># The original class labels</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span>
        <span class="n">encoded_labels</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># init cross-validation generator</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">folds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

        <span class="c1"># Use the label encoded classes</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This solver needs samples of at least 2 classes&quot;</span>
                             <span class="s2">&quot; in the data, but the data contains only one&quot;</span>
                             <span class="s2">&quot; class: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">classes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># OvR in case of binary problems is as good as fitting</span>
            <span class="c1"># the higher label</span>
            <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">encoded_labels</span> <span class="o">=</span> <span class="n">encoded_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="c1"># We need this hack to iterate only once over labels, in the case of</span>
        <span class="c1"># multi_class = multinomial, without changing the value of the labels.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">iter_encoded_labels</span> <span class="o">=</span> <span class="n">iter_classes</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iter_encoded_labels</span> <span class="o">=</span> <span class="n">encoded_labels</span>
            <span class="n">iter_classes</span> <span class="o">=</span> <span class="n">classes</span>

        <span class="c1"># compute the class weights for the entire dataset y</span>
        <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced&quot;</span><span class="p">:</span>
            <span class="n">class_weight</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span>
                                                <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)),</span>
                                                <span class="n">y</span><span class="p">)</span>
            <span class="n">class_weight</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">class_weight</span><span class="p">))</span>

        <span class="n">path_func</span> <span class="o">=</span> <span class="n">delayed</span><span class="p">(</span><span class="n">_log_reg_scoring_path</span><span class="p">)</span>

        <span class="c1"># The SAG solver releases the GIL so it&#39;s more efficient to use</span>
        <span class="c1"># threads for this solver.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="s1">&#39;saga&#39;</span><span class="p">]:</span>
            <span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;threading&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;multiprocessing&#39;</span>
        <span class="n">fold_coefs_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                               <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)(</span>
            <span class="n">path_func</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs</span><span class="p">,</span>
                      <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                      <span class="n">dual</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                      <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                      <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">,</span>
                      <span class="n">multi_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span>
                      <span class="n">intercept_scaling</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
                      <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                      <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span>
                      <span class="p">)</span>
            <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">iter_encoded_labels</span>
            <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">folds</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">multi_coefs_paths</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">multi_scores</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">fold_coefs_</span><span class="p">)</span>
            <span class="n">multi_coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">multi_coefs_paths</span><span class="p">)</span>
            <span class="n">multi_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">multi_scores</span><span class="p">)</span>

            <span class="c1"># This is just to maintain API similarity between the ovr and</span>
            <span class="c1"># multinomial option.</span>
            <span class="c1"># Coefs_paths in now n_folds X len(Cs) X n_classes X n_features</span>
            <span class="c1"># we need it to be n_classes X len(Cs) X n_folds X n_features</span>
            <span class="c1"># to be similar to &quot;ovr&quot;.</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rollaxis</span><span class="p">(</span><span class="n">multi_coefs_paths</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

            <span class="c1"># Multinomial has a true score across all labels. Hence the</span>
            <span class="c1"># shape is n_folds X len(Cs). We need to repeat this score</span>
            <span class="c1"># across all labels for API similarity.</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">multi_scores</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span> <span class="o">=</span> <span class="n">Cs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span>
                                                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">coefs_paths</span><span class="p">,</span> <span class="n">Cs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">fold_coefs_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span> <span class="o">=</span> <span class="n">Cs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span>
                                                   <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span>
                                                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">coefs_paths</span><span class="p">))</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">scores</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">C_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>

        <span class="c1"># hack to iterate only once for multinomial case.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">multi_scores</span>
            <span class="n">coefs_paths</span> <span class="o">=</span> <span class="n">multi_coefs_paths</span>

        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">encoded_label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">iter_classes</span><span class="p">,</span> <span class="n">iter_encoded_labels</span><span class="p">)):</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;ovr&#39;</span><span class="p">:</span>
                <span class="c1"># The scores_ / coefs_paths_ dict have unencoded class</span>
                <span class="c1"># labels as their keys</span>
                <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scores_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>
                <span class="n">coefs_paths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coefs_paths_</span><span class="p">[</span><span class="bp">cls</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
                <span class="n">best_index</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

                <span class="n">C_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">[</span><span class="n">best_index</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">C_</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
                    <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">[:,</span> <span class="n">best_index</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span>
                                        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">coefs_paths</span><span class="p">[:,</span> <span class="n">best_index</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Note that y is label encoded and hence pos_class must be</span>
                <span class="c1"># the encoded label / None (for &#39;multinomial&#39;)</span>
                <span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">logistic_regression_path</span><span class="p">(</span>
                    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pos_class</span><span class="o">=</span><span class="n">encoded_label</span><span class="p">,</span> <span class="n">Cs</span><span class="o">=</span><span class="p">[</span><span class="n">C_</span><span class="p">],</span> <span class="n">solver</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span>
                    <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="n">coef_init</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
                    <span class="n">penalty</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
                    <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
                    <span class="n">multi_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
                    <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
                    <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
                    <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Take the best scores across every fold and the average of all</span>
                <span class="c1"># coefficients corresponding to the best scores.</span>
                <span class="n">best_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">coefs_paths</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">best_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
                             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">folds</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Cs_</span><span class="p">[</span><span class="n">best_indices</span><span class="p">]))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s1">&#39;multinomial&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">C_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="p">:</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[:</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">C_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C_</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>