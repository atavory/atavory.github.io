
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.linear_model.stochastic_gradient &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.linear_model.stochastic_gradient</h1><div class="highlight"><pre>
<span></span><span class="c1"># Authors: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt; (main author)</span>
<span class="c1">#          Mathieu Blondel (partial_fit support)</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>
<span class="sd">&quot;&quot;&quot;Classification and regression using Stochastic Gradient Descent (SGD).&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="kn">from</span> <span class="nn">..externals.joblib</span> <span class="k">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span> <span class="n">SparseCoefMixin</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">make_dataset</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="k">import</span> <span class="n">safe_sparse_dot</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="k">import</span> <span class="n">_check_partial_fit_first_call</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="k">import</span> <span class="n">ConvergenceWarning</span>
<span class="kn">from</span> <span class="nn">..externals</span> <span class="k">import</span> <span class="n">six</span>

<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">plain_sgd</span><span class="p">,</span> <span class="n">average_sgd</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">compute_class_weight</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">Hinge</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">SquaredHinge</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">Log</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">ModifiedHuber</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">SquaredLoss</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">Huber</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">EpsilonInsensitive</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="k">import</span> <span class="n">SquaredEpsilonInsensitive</span>


<span class="n">LEARNING_RATE_TYPES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;constant&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;optimal&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;invscaling&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
                       <span class="s2">&quot;pa1&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;pa2&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>

<span class="n">PENALTY_TYPES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;none&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;elasticnet&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>

<span class="n">DEFAULT_EPSILON</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="c1"># Default value of ``epsilon`` parameter.</span>


<span class="k">class</span> <span class="nc">BaseSGD</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">SparseCoefMixin</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for SGD classification and regression.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;optimal&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">power_t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span> <span class="o">=</span> <span class="n">eta0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">power_t</span> <span class="o">=</span> <span class="n">power_t</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">=</span> <span class="n">average</span>

        <span class="k">if</span> <span class="n">n_iter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;n_iter parameter is deprecated in 0.19 and will be&quot;</span>
                          <span class="s2">&quot; removed in 0.21. Use max_iter and tol instead.&quot;</span><span class="p">,</span>
                          <span class="ne">DeprecationWarning</span><span class="p">)</span>
            <span class="c1"># Same behavior as before 0.19</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
            <span class="n">tol</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">elif</span> <span class="n">tol</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">max_iter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;max_iter and tol parameters have been added in </span><span class="si">%s</span><span class="s2"> in 0.19. If&quot;</span>
                <span class="s2">&quot; both are left unset, they default to max_iter=5 and tol=None&quot;</span>
                <span class="s2">&quot;. If tol is not None, max_iter defaults to max_iter=1000. &quot;</span>
                <span class="s2">&quot;From 0.21, default max_iter will be 1000, &quot;</span>
                <span class="s2">&quot;and default tol will be 1e-3.&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="ne">FutureWarning</span><span class="p">)</span>
            <span class="c1"># Before 0.19, default was n_iter=5</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span> <span class="k">if</span> <span class="n">max_iter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1000</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BaseSGD</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit model.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_validate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Validate input params. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;shuffle must be either True or False&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_iter must be &gt; zero. Got </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;l1_ratio must be in [0, 1]&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be &gt;= 0&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="s2">&quot;invscaling&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;eta0 must be &gt; 0&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">==</span> <span class="s2">&quot;optimal&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be &gt; 0 since &quot;</span>
                             <span class="s2">&quot;learning_rate is &#39;optimal&#39;. alpha is used &quot;</span>
                             <span class="s2">&quot;to compute the optimal learning rate.&quot;</span><span class="p">)</span>

        <span class="c1"># raises ValueError if not registered</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_get_penalty_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_get_learning_rate_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_functions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The loss </span><span class="si">%s</span><span class="s2"> is not supported. &quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get concrete ``LossFunction`` object for str ``loss``. &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">loss_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_functions</span><span class="p">[</span><span class="n">loss</span><span class="p">]</span>
            <span class="n">loss_class</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">loss_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">if</span> <span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon_insensitive&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;squared_epsilon_insensitive&#39;</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="p">)</span>
            <span class="k">return</span> <span class="n">loss_class</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The loss </span><span class="si">%s</span><span class="s2"> is not supported. &quot;</span> <span class="o">%</span> <span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_learning_rate_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LEARNING_RATE_TYPES</span><span class="p">[</span><span class="n">learning_rate</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;learning rate </span><span class="si">%s</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;is not supported. &quot;</span> <span class="o">%</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_penalty_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="p">):</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">penalty</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">PENALTY_TYPES</span><span class="p">[</span><span class="n">penalty</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty </span><span class="si">%s</span><span class="s2"> is not supported. &quot;</span> <span class="o">%</span> <span class="n">penalty</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_sample_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the sample weight array.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># uniform sample weights</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># user-provided array</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                       <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Shapes of X and sample_weight do not match.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sample_weight</span>

    <span class="k">def</span> <span class="nf">_allocate_parameter_mem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Allocate mem for parameters; initialize if provided.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># allocate coef_ for multi-class</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">coef_init</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">coef_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided ``coef_`` does not match &quot;</span>
                                     <span class="s2">&quot;dataset. &quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef_init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">),</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

            <span class="c1"># allocate intercept_ for multi-class</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">intercept_init</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided intercept_init &quot;</span>
                                     <span class="s2">&quot;does not match dataset.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">intercept_init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                           <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># allocate coef_ for binary problem</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">coef_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                       <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="n">coef_init</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">coef_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided coef_init does not &quot;</span>
                                     <span class="s2">&quot;match dataset.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef_init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                      <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

            <span class="c1"># allocate intercept_ for binary problem</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">intercept_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided intercept_init &quot;</span>
                                     <span class="s2">&quot;does not match dataset.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

        <span class="c1"># initialize average parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                          <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                               <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                               <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_prepare_fit_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization for fit_binary.</span>

<span class="sd">    Returns y, coef, intercept.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="n">y_i</span><span class="p">[</span><span class="n">y</span> <span class="o">!=</span> <span class="n">est</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
    <span class="n">average_intercept</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">average_coef</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">average_coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">average_intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">average_coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">average_intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span> <span class="n">average_intercept</span>


<span class="k">def</span> <span class="nf">fit_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
               <span class="n">pos_weight</span><span class="p">,</span> <span class="n">neg_weight</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit a single binary classifier.</span>

<span class="sd">    The i&#39;th class is considered the &quot;positive&quot; class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if average is not true, average_coef, and average_intercept will be</span>
    <span class="c1"># unused</span>
    <span class="n">y_i</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span> <span class="n">average_intercept</span> <span class="o">=</span> \
        <span class="n">_prepare_fit_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">y_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="n">intercept_decay</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">penalty_type</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_get_penalty_type</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
    <span class="n">learning_rate_type</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_get_learning_rate_type</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># XXX should have random_state_!</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="c1"># numpy mtrand expects a C long which is a signed 32 bit integer under</span>
    <span class="c1"># Windows</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

    <span class="n">tol</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">tol</span> <span class="k">if</span> <span class="n">est</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">plain_sgd</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">loss_function_</span><span class="p">,</span>
                         <span class="n">penalty_type</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                         <span class="n">dataset</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span>
                         <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span> <span class="n">seed</span><span class="p">,</span>
                         <span class="n">pos_weight</span><span class="p">,</span> <span class="n">neg_weight</span><span class="p">,</span>
                         <span class="n">learning_rate_type</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span>
                         <span class="n">est</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span> <span class="n">intercept_decay</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">standard_coef</span><span class="p">,</span> <span class="n">standard_intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span> <span class="n">average_intercept</span><span class="p">,</span> \
            <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">average_sgd</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span>
                                  <span class="n">average_intercept</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">loss_function_</span><span class="p">,</span>
                                  <span class="n">penalty_type</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                                  <span class="n">dataset</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                                  <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
                                  <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span> <span class="n">seed</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span>
                                  <span class="n">neg_weight</span><span class="p">,</span> <span class="n">learning_rate_type</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span>
                                  <span class="n">est</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span> <span class="n">intercept_decay</span><span class="p">,</span>
                                  <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_intercept</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_intercept</span>

        <span class="k">return</span> <span class="n">standard_coef</span><span class="p">,</span> <span class="n">standard_intercept</span><span class="p">,</span> <span class="n">n_iter_</span>


<span class="k">class</span> <span class="nc">BaseSGDClassifier</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseSGD</span><span class="p">,</span>
                                           <span class="n">LinearClassifierMixin</span><span class="p">)):</span>

    <span class="n">loss_functions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;hinge&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Hinge</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s2">&quot;squared_hinge&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredHinge</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s2">&quot;perceptron&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Hinge</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
        <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Log</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;modified_huber&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">ModifiedHuber</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;squared_loss&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredLoss</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;huber&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Huber</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">EpsilonInsensitive</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;squared_epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredEpsilonInsensitive</span><span class="p">,</span>
                                        <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;optimal&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">BaseSGDClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
                                                <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
                                                <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
                                                <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                                                <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
                                                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                                                <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
                                                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                                <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                                <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span> <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span>
                                                <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
                                                <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
                                                <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_jobs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;Attribute loss_function was deprecated in version 0.19 and &quot;</span>
                <span class="s2">&quot;will be removed in 0.21. Use ``loss_function_`` instead&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_</span>

    <span class="k">def</span> <span class="nf">_partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                     <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
                     <span class="n">classes</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                     <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
        <span class="n">_check_partial_fit_first_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Allocate datastructures from input arguments</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
                                                           <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_allocate_parameter_mem</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span>
                                         <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of features </span><span class="si">%d</span><span class="s2"> does not match previous &quot;</span>
                             <span class="s2">&quot;data </span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;t_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="c1"># delegate to concrete training procedure</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_multiclass</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                 <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_binary</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
                             <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                             <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                             <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The number of class labels must be &quot;</span>
                             <span class="s2">&quot;greater than one.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># labels can be encoded as float, int, or string literals</span>
        <span class="c1"># np.unique sorts in asc order; largest class id is positive class</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Clear iteration count for multiple call to fit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                          <span class="n">classes</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration reached before &quot;</span>
                          <span class="s2">&quot;convergence. Consider increasing max_iter to &quot;</span>
                          <span class="s2">&quot;improve the fit.&quot;</span><span class="p">,</span>
                          <span class="n">ConvergenceWarning</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit a binary classifier on X and y. &quot;&quot;&quot;</span>
        <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">fit_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                                              <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="n">sample_weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter_</span>

        <span class="c1"># need to be 2d</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># intercept is a float, need to convert it to an array of length 1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit_multiclass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                        <span class="n">sample_weight</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit a multi-class classifier by combining binary classifiers</span>

<span class="sd">        Each binary classifier predicts one class versus all others. This</span>
<span class="sd">        strategy is called OVA: One Versus All.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Use joblib to fit OvA in parallel.</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">fit_binary</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                                <span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                <span class="mf">1.</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)))</span>

        <span class="c1"># take the maximum of n_iter_ over every binary fit</span>
        <span class="n">n_iter_</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">intercept</span>
            <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter_</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit linear model with Stochastic Gradient Descent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Subset of the training data</span>

<span class="sd">        y : numpy array, shape (n_samples,)</span>
<span class="sd">            Subset of the target values</span>

<span class="sd">        classes : array, shape (n_classes,)</span>
<span class="sd">            Classes across all calls to partial_fit.</span>
<span class="sd">            Can be obtained by via `np.unique(y_all)`, where y_all is the</span>
<span class="sd">            target vector of the entire dataset.</span>
<span class="sd">            This argument is required for the first call to partial_fit</span>
<span class="sd">            and can be omitted in the subsequent calls.</span>
<span class="sd">            Note that y doesn&#39;t need to contain all labels in `classes`.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples.</span>
<span class="sd">            If not provided, uniform weights are assumed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;balanced&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;class_weight &#39;</span><span class="si">{0}</span><span class="s2">&#39; is not supported for &quot;</span>
                             <span class="s2">&quot;partial_fit. In order to use &#39;balanced&#39; weights,&quot;</span>
                             <span class="s2">&quot; use compute_class_weight(&#39;</span><span class="si">{0}</span><span class="s2">&#39;, classes, y). &quot;</span>
                             <span class="s2">&quot;In place of y you can us a large enough sample &quot;</span>
                             <span class="s2">&quot;of the full training set target to properly &quot;</span>
                             <span class="s2">&quot;estimate the class frequency distributions. &quot;</span>
                             <span class="s2">&quot;Pass the resulting weights as the class_weight &quot;</span>
                             <span class="s2">&quot;parameter.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                 <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit linear model with Stochastic Gradient Descent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training data</span>

<span class="sd">        y : numpy array, shape (n_samples,)</span>
<span class="sd">            Target values</span>

<span class="sd">        coef_init : array, shape (n_classes, n_features)</span>
<span class="sd">            The initial coefficients to warm-start the optimization.</span>

<span class="sd">        intercept_init : array, shape (n_classes,)</span>
<span class="sd">            The initial intercept to warm-start the optimization.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples.</span>
<span class="sd">            If not provided, uniform weights are assumed. These weights will</span>
<span class="sd">            be multiplied with class_weight (passed through the</span>
<span class="sd">            constructor) if class_weight is specified</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                         <span class="n">coef_init</span><span class="o">=</span><span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="n">intercept_init</span><span class="p">,</span>
                         <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SGDClassifier</span><span class="p">(</span><span class="n">BaseSGDClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</span>

<span class="sd">    This estimator implements regularized linear models with stochastic</span>
<span class="sd">    gradient descent (SGD) learning: the gradient of the loss is estimated</span>
<span class="sd">    each sample at a time and the model is updated along the way with a</span>
<span class="sd">    decreasing strength schedule (aka learning rate). SGD allows minibatch</span>
<span class="sd">    (online/out-of-core) learning, see the partial_fit method.</span>
<span class="sd">    For best results using the default learning rate schedule, the data should</span>
<span class="sd">    have zero mean and unit variance.</span>

<span class="sd">    This implementation works with data represented as dense or sparse arrays</span>
<span class="sd">    of floating point values for the features. The model it fits can be</span>
<span class="sd">    controlled with the loss parameter; by default, it fits a linear support</span>
<span class="sd">    vector machine (SVM).</span>

<span class="sd">    The regularizer is a penalty added to the loss function that shrinks model</span>
<span class="sd">    parameters towards the zero vector using either the squared euclidean norm</span>
<span class="sd">    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the</span>
<span class="sd">    parameter update crosses the 0.0 value because of the regularizer, the</span>
<span class="sd">    update is truncated to 0.0 to allow for learning sparse models and achieve</span>
<span class="sd">    online feature selection.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;sgd&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : str, default: &#39;hinge&#39;</span>
<span class="sd">        The loss function to be used. Defaults to &#39;hinge&#39;, which gives a</span>
<span class="sd">        linear SVM.</span>

<span class="sd">        The possible options are &#39;hinge&#39;, &#39;log&#39;, &#39;modified_huber&#39;,</span>
<span class="sd">        &#39;squared_hinge&#39;, &#39;perceptron&#39;, or a regression loss: &#39;squared_loss&#39;,</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;.</span>

<span class="sd">        The &#39;log&#39; loss gives logistic regression, a probabilistic classifier.</span>
<span class="sd">        &#39;modified_huber&#39; is another smooth loss that brings tolerance to</span>
<span class="sd">        outliers as well as probability estimates.</span>
<span class="sd">        &#39;squared_hinge&#39; is like hinge but is quadratically penalized.</span>
<span class="sd">        &#39;perceptron&#39; is the linear loss used by the perceptron algorithm.</span>
<span class="sd">        The other losses are designed for regression but can be useful in</span>
<span class="sd">        classification as well; see SGDRegressor for a description.</span>

<span class="sd">    penalty : str, &#39;none&#39;, &#39;l2&#39;, &#39;l1&#39;, or &#39;elasticnet&#39;</span>
<span class="sd">        The penalty (aka regularization term) to be used. Defaults to &#39;l2&#39;</span>
<span class="sd">        which is the standard regularizer for linear SVM models. &#39;l1&#39; and</span>
<span class="sd">        &#39;elasticnet&#39; might bring sparsity to the model (feature selection)</span>
<span class="sd">        not achievable with &#39;l2&#39;.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Constant that multiplies the regularization term. Defaults to 0.0001</span>
<span class="sd">        Also used to compute learning_rate when set to &#39;optimal&#39;.</span>

<span class="sd">    l1_ratio : float</span>
<span class="sd">        The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.</span>
<span class="sd">        Defaults to 0.15.</span>

<span class="sd">    fit_intercept : bool</span>
<span class="sd">        Whether the intercept should be estimated or not. If False, the</span>
<span class="sd">        data is assumed to be already centered. Defaults to True.</span>

<span class="sd">    max_iter : int, optional</span>
<span class="sd">        The maximum number of passes over the training data (aka epochs).</span>
<span class="sd">        It only impacts the behavior in the ``fit`` method, and not the</span>
<span class="sd">        `partial_fit`.</span>
<span class="sd">        Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    tol : float or None, optional</span>
<span class="sd">        The stopping criterion. If it is not None, the iterations will stop</span>
<span class="sd">        when (loss &gt; previous_loss - tol). Defaults to None.</span>
<span class="sd">        Defaults to 1e-3 from 0.21.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    shuffle : bool, optional</span>
<span class="sd">        Whether or not the training data should be shuffled after each epoch.</span>
<span class="sd">        Defaults to True.</span>

<span class="sd">    verbose : integer, optional</span>
<span class="sd">        The verbosity level</span>

<span class="sd">    epsilon : float</span>
<span class="sd">        Epsilon in the epsilon-insensitive loss functions; only if `loss` is</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;.</span>
<span class="sd">        For &#39;huber&#39;, determines the threshold at which it becomes less</span>
<span class="sd">        important to get the prediction exactly right.</span>
<span class="sd">        For epsilon-insensitive, any differences between the current prediction</span>
<span class="sd">        and the correct label are ignored if they are less than this threshold.</span>

<span class="sd">    n_jobs : integer, optional</span>
<span class="sd">        The number of CPUs to use to do the OVA (One Versus All, for</span>
<span class="sd">        multi-class problems) computation. -1 means &#39;all CPUs&#39;. Defaults</span>
<span class="sd">        to 1.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    learning_rate : string, optional</span>
<span class="sd">        The learning rate schedule:</span>

<span class="sd">        - &#39;constant&#39;: eta = eta0</span>
<span class="sd">        - &#39;optimal&#39;: eta = 1.0 / (alpha * (t + t0)) [default]</span>
<span class="sd">        - &#39;invscaling&#39;: eta = eta0 / pow(t, power_t)</span>

<span class="sd">        where t0 is chosen by a heuristic proposed by Leon Bottou.</span>

<span class="sd">    eta0 : double</span>
<span class="sd">        The initial learning rate for the &#39;constant&#39; or &#39;invscaling&#39;</span>
<span class="sd">        schedules. The default value is 0.0 as eta0 is not used by the</span>
<span class="sd">        default schedule &#39;optimal&#39;.</span>

<span class="sd">    power_t : double</span>
<span class="sd">        The exponent for inverse scaling learning rate [default 0.5].</span>

<span class="sd">    class_weight : dict, {class_label: weight} or &quot;balanced&quot; or None, optional</span>
<span class="sd">        Preset for the class_weight fit parameter.</span>

<span class="sd">        Weights associated with classes. If not given, all classes</span>
<span class="sd">        are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">    warm_start : bool, optional</span>
<span class="sd">        When set to True, reuse the solution of the previous call to fit as</span>
<span class="sd">        initialization, otherwise, just erase the previous solution.</span>

<span class="sd">    average : bool or int, optional</span>
<span class="sd">        When set to True, computes the averaged SGD weights and stores the</span>
<span class="sd">        result in the ``coef_`` attribute. If set to an int greater than 1,</span>
<span class="sd">        averaging will begin once the total number of samples seen reaches</span>
<span class="sd">        average. So ``average=10`` will begin averaging after seeing 10</span>
<span class="sd">        samples.</span>

<span class="sd">    n_iter : int, optional</span>
<span class="sd">        The number of passes over the training data (aka epochs).</span>
<span class="sd">        Defaults to None. Deprecated, will be removed in 0.21.</span>

<span class="sd">        .. versionchanged:: 0.19</span>
<span class="sd">            Deprecated</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,\</span>
<span class="sd">            n_features)</span>
<span class="sd">        Weights assigned to the features.</span>

<span class="sd">    intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        The actual number of iterations to reach the stopping criterion.</span>
<span class="sd">        For multiclass fits, it is the maximum over every binary fit.</span>

<span class="sd">    loss_function_ : concrete ``LossFunction``</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn import linear_model</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])</span>
<span class="sd">    &gt;&gt;&gt; Y = np.array([1, 1, 2, 2])</span>
<span class="sd">    &gt;&gt;&gt; clf = linear_model.SGDClassifier()</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, Y)</span>
<span class="sd">    ... #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span>
<span class="sd">           eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span>
<span class="sd">           learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=5, n_iter=None,</span>
<span class="sd">           n_jobs=1, penalty=&#39;l2&#39;, power_t=0.5, random_state=None,</span>
<span class="sd">           shuffle=True, tol=None, verbose=0, warm_start=False)</span>

<span class="sd">    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))</span>
<span class="sd">    [1]</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    LinearSVC, LogisticRegression, Perceptron</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
                 <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;optimal&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SGDClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span>
            <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;t_&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="s2">&quot;modified_huber&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;probability estimates are not available for&quot;</span>
                                 <span class="s2">&quot; loss=</span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability estimates.</span>

<span class="sd">        This method is only available for log loss and modified Huber loss.</span>

<span class="sd">        Multiclass probability estimates are derived from binary (one-vs.-rest)</span>
<span class="sd">        estimates by simple normalization, as recommended by Zadrozny and</span>
<span class="sd">        Elkan.</span>

<span class="sd">        Binary probability estimates for loss=&quot;modified_huber&quot; are given by</span>
<span class="sd">        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions</span>
<span class="sd">        it is necessary to perform proper probability calibration by wrapping</span>
<span class="sd">        the classifier with</span>
<span class="sd">        :class:`sklearn.calibration.CalibratedClassifierCV` instead.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array, shape (n_samples, n_classes)</span>
<span class="sd">            Returns the probability of the sample for each class in the model,</span>
<span class="sd">            where classes are ordered as they are in `self.classes_`.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        Zadrozny and Elkan, &quot;Transforming classifier scores into multiclass</span>
<span class="sd">        probability estimates&quot;, SIGKDD&#39;02,</span>
<span class="sd">        http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf</span>

<span class="sd">        The justification for the formula in the loss=&quot;modified_huber&quot;</span>
<span class="sd">        case is in the appendix B in:</span>
<span class="sd">        http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_proba</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_proba</span>

    <span class="k">def</span> <span class="nf">_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_proba_lr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s2">&quot;modified_huber&quot;</span><span class="p">:</span>
            <span class="n">binary</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">prob2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">prob2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">scores</span>

            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
            <span class="n">prob</span> <span class="o">+=</span> <span class="mf">1.</span>
            <span class="n">prob</span> <span class="o">/=</span> <span class="mf">2.</span>

            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">prob2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">prob</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">prob2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># the above might assign zero to all classes, which doesn&#39;t</span>
                <span class="c1"># normalize neatly; work around this to produce uniform</span>
                <span class="c1"># probabilities</span>
                <span class="n">prob_sum</span> <span class="o">=</span> <span class="n">prob</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">all_zero</span> <span class="o">=</span> <span class="p">(</span><span class="n">prob_sum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">all_zero</span><span class="p">):</span>
                    <span class="n">prob</span><span class="p">[</span><span class="n">all_zero</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">prob_sum</span><span class="p">[</span><span class="n">all_zero</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

                <span class="c1"># normalize</span>
                <span class="n">prob</span> <span class="o">/=</span> <span class="n">prob_sum</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">prob</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

            <span class="k">return</span> <span class="n">prob</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;predict_(log_)proba only supported when&quot;</span>
                                      <span class="s2">&quot; loss=&#39;log&#39; or loss=&#39;modified_huber&#39; &quot;</span>
                                      <span class="s2">&quot;(</span><span class="si">%r</span><span class="s2"> given)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Log of probability estimates.</span>

<span class="sd">        This method is only available for log loss and modified Huber loss.</span>

<span class="sd">        When loss=&quot;modified_huber&quot;, probability estimates may be hard zeros</span>
<span class="sd">        and ones, so taking the logarithm is not possible.</span>

<span class="sd">        See ``predict_proba`` for details.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Returns the log-probability of the sample for each class in the</span>
<span class="sd">            model, where classes are ordered as they are in</span>
<span class="sd">            `self.classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_proba</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_log_proba</span>

    <span class="k">def</span> <span class="nf">_predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">BaseSGDRegressor</span><span class="p">(</span><span class="n">BaseSGD</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>

    <span class="n">loss_functions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;squared_loss&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredLoss</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;huber&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Huber</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">EpsilonInsensitive</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;squared_epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredEpsilonInsensitive</span><span class="p">,</span>
                                        <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_loss&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;invscaling&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BaseSGDRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
                                               <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
                                               <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
                                               <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                                               <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
                                               <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                                               <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
                                               <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                               <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                               <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span> <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span>
                                               <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
                                               <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
                                               <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                     <span class="n">max_iter</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>

        <span class="c1"># Allocate datastructures from input arguments</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_allocate_parameter_mem</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span>
                                         <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of features </span><span class="si">%d</span><span class="s2"> does not match previous &quot;</span>
                             <span class="s2">&quot;data </span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;average_coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                          <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                               <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                               <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fit_regressor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                            <span class="n">sample_weight</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit linear model with Stochastic Gradient Descent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Subset of training data</span>

<span class="sd">        y : numpy array of shape (n_samples,)</span>
<span class="sd">            Subset of target values</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples.</span>
<span class="sd">            If not provided, uniform weights are assumed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                 <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Clear iteration count for multiple call to fit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span>
                          <span class="n">intercept_init</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration reached before &quot;</span>
                          <span class="s2">&quot;convergence. Consider increasing max_iter to &quot;</span>
                          <span class="s2">&quot;improve the fit.&quot;</span><span class="p">,</span>
                          <span class="n">ConvergenceWarning</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit linear model with Stochastic Gradient Descent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training data</span>

<span class="sd">        y : numpy array, shape (n_samples,)</span>
<span class="sd">            Target values</span>

<span class="sd">        coef_init : array, shape (n_features,)</span>
<span class="sd">            The initial coefficients to warm-start the optimization.</span>

<span class="sd">        intercept_init : array, shape (1,)</span>
<span class="sd">            The initial intercept to warm-start the optimization.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples (1. for unweighted).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                         <span class="n">coef_init</span><span class="o">=</span><span class="n">coef_init</span><span class="p">,</span>
                         <span class="n">intercept_init</span><span class="o">=</span><span class="n">intercept_init</span><span class="p">,</span>
                         <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict using the linear model</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array, shape (n_samples,)</span>
<span class="sd">           Predicted target values per element in X.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;t_&quot;</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="s2">&quot;intercept_&quot;</span><span class="p">],</span> <span class="n">all_or_any</span><span class="o">=</span><span class="nb">all</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                 <span class="n">dense_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="k">return</span> <span class="n">scores</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict using the linear model</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array, shape (n_samples,)</span>
<span class="sd">           Predicted target values per element in X.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit_regressor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                       <span class="n">sample_weight</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="n">dataset</span><span class="p">,</span> <span class="n">intercept_decay</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">loss_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">penalty_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_penalty_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
        <span class="n">learning_rate_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_learning_rate_type</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;t_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="c1"># numpy mtrand expects a C long which is a signed 32 bit integer under</span>
        <span class="c1"># Windows</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

        <span class="n">tol</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">,</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span>\
                <span class="n">average_sgd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">loss_function</span><span class="p">,</span>
                            <span class="n">penalty_type</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                            <span class="n">dataset</span><span class="p">,</span>
                            <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span>
                            <span class="n">seed</span><span class="p">,</span>
                            <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">learning_rate_type</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span>
                            <span class="n">intercept_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> \
                <span class="n">plain_sgd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="n">loss_function</span><span class="p">,</span>
                          <span class="n">penalty_type</span><span class="p">,</span>
                          <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                          <span class="n">dataset</span><span class="p">,</span>
                          <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span>
                          <span class="n">seed</span><span class="p">,</span>
                          <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                          <span class="n">learning_rate_type</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span>
                          <span class="n">intercept_decay</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SGDRegressor</span><span class="p">(</span><span class="n">BaseSGDRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear model fitted by minimizing a regularized empirical loss with SGD</span>

<span class="sd">    SGD stands for Stochastic Gradient Descent: the gradient of the loss is</span>
<span class="sd">    estimated each sample at a time and the model is updated along the way with</span>
<span class="sd">    a decreasing strength schedule (aka learning rate).</span>

<span class="sd">    The regularizer is a penalty added to the loss function that shrinks model</span>
<span class="sd">    parameters towards the zero vector using either the squared euclidean norm</span>
<span class="sd">    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the</span>
<span class="sd">    parameter update crosses the 0.0 value because of the regularizer, the</span>
<span class="sd">    update is truncated to 0.0 to allow for learning sparse models and achieve</span>
<span class="sd">    online feature selection.</span>

<span class="sd">    This implementation works with data represented as dense numpy arrays of</span>
<span class="sd">    floating point values for the features.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;sgd&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : str, default: &#39;squared_loss&#39;</span>
<span class="sd">        The loss function to be used. The possible values are &#39;squared_loss&#39;,</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;</span>

<span class="sd">        The &#39;squared_loss&#39; refers to the ordinary least squares fit.</span>
<span class="sd">        &#39;huber&#39; modifies &#39;squared_loss&#39; to focus less on getting outliers</span>
<span class="sd">        correct by switching from squared to linear loss past a distance of</span>
<span class="sd">        epsilon. &#39;epsilon_insensitive&#39; ignores errors less than epsilon and is</span>
<span class="sd">        linear past that; this is the loss function used in SVR.</span>
<span class="sd">        &#39;squared_epsilon_insensitive&#39; is the same but becomes squared loss past</span>
<span class="sd">        a tolerance of epsilon.</span>

<span class="sd">    penalty : str, &#39;none&#39;, &#39;l2&#39;, &#39;l1&#39;, or &#39;elasticnet&#39;</span>
<span class="sd">        The penalty (aka regularization term) to be used. Defaults to &#39;l2&#39;</span>
<span class="sd">        which is the standard regularizer for linear SVM models. &#39;l1&#39; and</span>
<span class="sd">        &#39;elasticnet&#39; might bring sparsity to the model (feature selection)</span>
<span class="sd">        not achievable with &#39;l2&#39;.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Constant that multiplies the regularization term. Defaults to 0.0001</span>
<span class="sd">        Also used to compute learning_rate when set to &#39;optimal&#39;.</span>

<span class="sd">    l1_ratio : float</span>
<span class="sd">        The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.</span>
<span class="sd">        Defaults to 0.15.</span>

<span class="sd">    fit_intercept : bool</span>
<span class="sd">        Whether the intercept should be estimated or not. If False, the</span>
<span class="sd">        data is assumed to be already centered. Defaults to True.</span>

<span class="sd">    max_iter : int, optional</span>
<span class="sd">        The maximum number of passes over the training data (aka epochs).</span>
<span class="sd">        It only impacts the behavior in the ``fit`` method, and not the</span>
<span class="sd">        `partial_fit`.</span>
<span class="sd">        Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    tol : float or None, optional</span>
<span class="sd">        The stopping criterion. If it is not None, the iterations will stop</span>
<span class="sd">        when (loss &gt; previous_loss - tol). Defaults to None.</span>
<span class="sd">        Defaults to 1e-3 from 0.21.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    shuffle : bool, optional</span>
<span class="sd">        Whether or not the training data should be shuffled after each epoch.</span>
<span class="sd">        Defaults to True.</span>

<span class="sd">    verbose : integer, optional</span>
<span class="sd">        The verbosity level.</span>

<span class="sd">    epsilon : float</span>
<span class="sd">        Epsilon in the epsilon-insensitive loss functions; only if `loss` is</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;.</span>
<span class="sd">        For &#39;huber&#39;, determines the threshold at which it becomes less</span>
<span class="sd">        important to get the prediction exactly right.</span>
<span class="sd">        For epsilon-insensitive, any differences between the current prediction</span>
<span class="sd">        and the correct label are ignored if they are less than this threshold.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    learning_rate : string, optional</span>
<span class="sd">        The learning rate schedule:</span>

<span class="sd">        - &#39;constant&#39;: eta = eta0</span>
<span class="sd">        - &#39;optimal&#39;: eta = 1.0 / (alpha * (t + t0)) [default]</span>
<span class="sd">        - &#39;invscaling&#39;: eta = eta0 / pow(t, power_t)</span>

<span class="sd">        where t0 is chosen by a heuristic proposed by Leon Bottou.</span>

<span class="sd">    eta0 : double, optional</span>
<span class="sd">        The initial learning rate [default 0.01].</span>

<span class="sd">    power_t : double, optional</span>
<span class="sd">        The exponent for inverse scaling learning rate [default 0.25].</span>

<span class="sd">    warm_start : bool, optional</span>
<span class="sd">        When set to True, reuse the solution of the previous call to fit as</span>
<span class="sd">        initialization, otherwise, just erase the previous solution.</span>

<span class="sd">    average : bool or int, optional</span>
<span class="sd">        When set to True, computes the averaged SGD weights and stores the</span>
<span class="sd">        result in the ``coef_`` attribute. If set to an int greater than 1,</span>
<span class="sd">        averaging will begin once the total number of samples seen reaches</span>
<span class="sd">        average. So ``average=10`` will begin averaging after seeing 10</span>
<span class="sd">        samples.</span>

<span class="sd">    n_iter : int, optional</span>
<span class="sd">        The number of passes over the training data (aka epochs).</span>
<span class="sd">        Defaults to None. Deprecated, will be removed in 0.21.</span>

<span class="sd">        .. versionchanged:: 0.19</span>
<span class="sd">            Deprecated</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape (n_features,)</span>
<span class="sd">        Weights assigned to the features.</span>

<span class="sd">    intercept_ : array, shape (1,)</span>
<span class="sd">        The intercept term.</span>

<span class="sd">    average_coef_ : array, shape (n_features,)</span>
<span class="sd">        Averaged weights assigned to the features.</span>

<span class="sd">    average_intercept_ : array, shape (1,)</span>
<span class="sd">        The averaged intercept term.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        The actual number of iterations to reach the stopping criterion.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn import linear_model</span>
<span class="sd">    &gt;&gt;&gt; n_samples, n_features = 10, 5</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">    &gt;&gt;&gt; y = np.random.randn(n_samples)</span>
<span class="sd">    &gt;&gt;&gt; X = np.random.randn(n_samples, n_features)</span>
<span class="sd">    &gt;&gt;&gt; clf = linear_model.SGDRegressor()</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    ... #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,</span>
<span class="sd">           fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;,</span>
<span class="sd">           loss=&#39;squared_loss&#39;, max_iter=5, n_iter=None, penalty=&#39;l2&#39;,</span>
<span class="sd">           power_t=0.25, random_state=None, shuffle=True, tol=None,</span>
<span class="sd">           verbose=0, warm_start=False)</span>


<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    Ridge, ElasticNet, Lasso, SVR</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_loss&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;invscaling&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SGDRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
                                           <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
                                           <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span>
                                           <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
                                           <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                                           <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
                                           <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                                           <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                           <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span> <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span>
                                           <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
                                           <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>