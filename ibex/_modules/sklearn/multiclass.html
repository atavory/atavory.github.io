
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.multiclass &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.multiclass</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Multiclass and multilabel classification strategies</span>
<span class="sd">===================================================</span>

<span class="sd">This module implements multiclass learning algorithms:</span>
<span class="sd">    - one-vs-the-rest / one-vs-all</span>
<span class="sd">    - one-vs-one</span>
<span class="sd">    - error correcting output codes</span>

<span class="sd">The estimators provided in this module are meta-estimators: they require a base</span>
<span class="sd">estimator to be provided in their constructor. For example, it is possible to</span>
<span class="sd">use these estimators to turn a binary classifier or a regressor into a</span>
<span class="sd">multiclass classifier. It is also possible to use these estimators with</span>
<span class="sd">multiclass estimators in the hope that their accuracy or runtime performance</span>
<span class="sd">improves.</span>

<span class="sd">All classifiers in scikit-learn implement multiclass classification; you</span>
<span class="sd">only need to use this module if you want to experiment with custom multiclass</span>
<span class="sd">strategies.</span>

<span class="sd">The one-vs-the-rest meta-classifier also implements a `predict_proba` method,</span>
<span class="sd">so long as such a method is implemented by the base classifier. This method</span>
<span class="sd">returns probabilities of class membership in both the single label and</span>
<span class="sd">multilabel case.  Note that in the multilabel case, probabilities are the</span>
<span class="sd">marginal probability that a given sample falls in the given class. As such, in</span>
<span class="sd">the multilabel case the sum of these probabilities over all possible labels</span>
<span class="sd">for a given sample *will not* sum to unity, as they do in the single label</span>
<span class="sd">case.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Author: Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="c1"># Author: Hamzeh Alsalhi &lt;93hamsal@gmail.com&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">import</span> <span class="nn">array</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">itertools</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">clone</span><span class="p">,</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">MetaEstimatorMixin</span><span class="p">,</span> <span class="n">is_regressor</span>
<span class="kn">from</span> <span class="nn">.preprocessing</span> <span class="k">import</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">.metrics.pairwise</span> <span class="k">import</span> <span class="n">euclidean_distances</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">.utils.validation</span> <span class="k">import</span> <span class="n">_num_samples</span>
<span class="kn">from</span> <span class="nn">.utils.validation</span> <span class="k">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">.utils.validation</span> <span class="k">import</span> <span class="n">check_X_y</span><span class="p">,</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">.utils.multiclass</span> <span class="k">import</span> <span class="p">(</span><span class="n">_check_partial_fit_first_call</span><span class="p">,</span>
                               <span class="n">check_classification_targets</span><span class="p">,</span>
                               <span class="n">_ovr_decision_function</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">.utils.metaestimators</span> <span class="k">import</span> <span class="n">_safe_split</span><span class="p">,</span> <span class="n">if_delegate_has_method</span>

<span class="kn">from</span> <span class="nn">.externals.joblib</span> <span class="k">import</span> <span class="n">Parallel</span>
<span class="kn">from</span> <span class="nn">.externals.joblib</span> <span class="k">import</span> <span class="n">delayed</span>
<span class="kn">from</span> <span class="nn">.externals.six.moves</span> <span class="k">import</span> <span class="nb">zip</span> <span class="k">as</span> <span class="n">izip</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;OneVsRestClassifier&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OneVsOneClassifier&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OutputCodeClassifier&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">_fit_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit a single binary estimator.&quot;&quot;&quot;</span>
    <span class="n">unique_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_y</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">classes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Label </span><span class="si">%s</span><span class="s2"> is present in all training examples.&quot;</span> <span class="o">%</span>
                          <span class="nb">str</span><span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="n">c</span><span class="p">]))</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">_ConstantPredictor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">unique_y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">estimator</span>


<span class="k">def</span> <span class="nf">_partial_fit_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Partially fit a single binary estimator.&quot;&quot;&quot;</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">estimator</span>


<span class="k">def</span> <span class="nf">_predict_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make predictions using a single binary estimator.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_regressor</span><span class="p">(</span><span class="n">estimator</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">NotImplementedError</span><span class="p">):</span>
        <span class="c1"># probabilities of the positive class</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">score</span>


<span class="k">def</span> <span class="nf">_check_estimator</span><span class="p">(</span><span class="n">estimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make sure that an estimator implements the necessary methods.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">)</span> <span class="ow">and</span>
            <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;predict_proba&quot;</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The base estimator should implement &quot;</span>
                         <span class="s2">&quot;decision_function or predict_proba!&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ConstantPredictor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;y_&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;y_&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;y_&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">])],</span>
                         <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OneVsRestClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">MetaEstimatorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;One-vs-the-rest (OvR) multiclass/multilabel strategy</span>

<span class="sd">    Also known as one-vs-all, this strategy consists in fitting one classifier</span>
<span class="sd">    per class. For each classifier, the class is fitted against all the other</span>
<span class="sd">    classes. In addition to its computational efficiency (only `n_classes`</span>
<span class="sd">    classifiers are needed), one advantage of this approach is its</span>
<span class="sd">    interpretability. Since each class is represented by one and one classifier</span>
<span class="sd">    only, it is possible to gain knowledge about the class by inspecting its</span>
<span class="sd">    corresponding classifier. This is the most commonly used strategy for</span>
<span class="sd">    multiclass classification and is a fair default choice.</span>

<span class="sd">    This strategy can also be used for multilabel learning, where a classifier</span>
<span class="sd">    is used to predict multiple labels for instance, by fitting on a 2-d matrix</span>
<span class="sd">    in which cell [i, j] is 1 if sample i has label j and 0 otherwise.</span>

<span class="sd">    In the multilabel learning literature, OvR is also known as the binary</span>
<span class="sd">    relevance method.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;ovr_classification&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator object</span>
<span class="sd">        An estimator object implementing `fit` and one of `decision_function`</span>
<span class="sd">        or `predict_proba`.</span>

<span class="sd">    n_jobs : int, optional, default: 1</span>
<span class="sd">        The number of jobs to use for the computation. If -1 all CPUs are used.</span>
<span class="sd">        If 1 is given, no parallel computing code is used at all, which is</span>
<span class="sd">        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are</span>
<span class="sd">        used. Thus for n_jobs = -2, all CPUs but one are used.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of `n_classes` estimators</span>
<span class="sd">        Estimators used for predictions.</span>

<span class="sd">    classes_ : array, shape = [`n_classes`]</span>
<span class="sd">        Class labels.</span>
<span class="sd">    label_binarizer_ : LabelBinarizer object</span>
<span class="sd">        Object used to transform multiclass labels to binary labels and</span>
<span class="sd">        vice-versa.</span>
<span class="sd">    multilabel_ : boolean</span>
<span class="sd">        Whether a OneVsRestClassifier is a multilabel classifier.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

<div class="viewcode-block" id="OneVsRestClassifier.fit"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsrestclassifier.html#ibex.sklearn.multiclass.OneVsRestClassifier.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit underlying estimators.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]</span>
<span class="sd">            Multi-class targets. An indicator matrix turns on multilabel</span>
<span class="sd">            classification.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># A sparse LabelBinarizer, with sparse_output=True, has been shown to</span>
        <span class="c1"># outpreform or match a dense label binarizer in all cases and has also</span>
        <span class="c1"># resulted in less or equal memory consumption in the fit_ovr function</span>
        <span class="c1"># overall.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">tocsc</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">classes_</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># In cases where individual estimators are very fast to train setting</span>
        <span class="c1"># n_jobs &gt; 1 in can results in slower performance due to the overhead</span>
        <span class="c1"># of spawning threads.  See joblib issue #112.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">_fit_binary</span><span class="p">)(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span>
                <span class="s2">&quot;not </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">column</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">columns</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="OneVsRestClassifier.partial_fit"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsrestclassifier.html#ibex.sklearn.multiclass.OneVsRestClassifier.partial_fit">[docs]</a>    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="s1">&#39;estimator&#39;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Partially fit underlying estimators</span>

<span class="sd">        Should be used when memory is inefficient to train all data.</span>
<span class="sd">        Chunks of data can be passed in several iteration.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]</span>
<span class="sd">            Multi-class targets. An indicator matrix turns on multilabel</span>
<span class="sd">            classification.</span>

<span class="sd">        classes : array, shape (n_classes, )</span>
<span class="sd">            Classes across all calls to partial_fit.</span>
<span class="sd">            Can be obtained via `np.unique(y_all)`, where y_all is the</span>
<span class="sd">            target vector of the entire dataset.</span>
<span class="sd">            This argument is only required in the first call of partial_fit</span>
<span class="sd">            and can be omitted in the subsequent calls.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_check_partial_fit_first_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;partial_fit&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s2">&quot;Base estimator </span><span class="si">{0}</span><span class="s2">, doesn&#39;t have &quot;</span>
                                 <span class="s2">&quot;partial_fit method&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span>
                                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">)]</span>

            <span class="c1"># A sparse LabelBinarizer, with sparse_output=True, has been</span>
            <span class="c1"># shown to outperform or match a dense label binarizer in all</span>
            <span class="c1"># cases and has also resulted in less or equal memory consumption</span>
            <span class="c1"># in the fit_ovr function overall.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s2">&quot;Mini-batch contains </span><span class="si">{0}</span><span class="s2"> while classes &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;must be subset of </span><span class="si">{1}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                                                             <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">tocsc</span><span class="p">()</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">(</span><span class="n">col</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_partial_fit_binary</span><span class="p">)(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">column</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">izip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">columns</span><span class="p">))</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="OneVsRestClassifier.predict"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsrestclassifier.html#ibex.sklearn.multiclass.OneVsRestClassifier.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict multi-class targets using underlying estimators.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes].</span>
<span class="sd">            Predicted multi-class targets.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">)</span> <span class="ow">and</span>
                <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
            <span class="n">thresh</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">thresh</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">_num_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">y_type_</span> <span class="o">==</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">:</span>
            <span class="n">maxima</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
            <span class="n">maxima</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
            <span class="n">argmaxima</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">_predict_binary</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
                <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">maxima</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">maxima</span><span class="p">)</span>
                <span class="n">argmaxima</span><span class="p">[</span><span class="n">maxima</span> <span class="o">==</span> <span class="n">pred</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">argmaxima</span><span class="o">.</span><span class="n">T</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">)</span>
            <span class="n">indptr</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
                <span class="n">indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">_predict_binary</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">indptr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">indicator</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csc_matrix</span><span class="p">((</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">indptr</span><span class="p">),</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)))</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">indicator</span><span class="p">)</span></div>

<div class="viewcode-block" id="OneVsRestClassifier.predict_proba"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsrestclassifier.html#ibex.sklearn.multiclass.OneVsRestClassifier.predict_proba">[docs]</a>    <span class="nd">@if_delegate_has_method</span><span class="p">([</span><span class="s1">&#39;_first_estimator&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">])</span>
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability estimates.</span>

<span class="sd">        The returned estimates for all classes are ordered by label of classes.</span>

<span class="sd">        Note that in the multilabel case, each sample can have any number of</span>
<span class="sd">        labels. This returns the marginal probability that the given sample has</span>
<span class="sd">        the label in question. For example, it is entirely consistent that two</span>
<span class="sd">        labels both have a 90% probability of applying to a given sample.</span>

<span class="sd">        In the single label multiclass case, the rows of the returned matrix</span>
<span class="sd">        sum to 1.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : (sparse) array-like, shape = [n_samples, n_classes]</span>
<span class="sd">            Returns the probability of the sample for each class in the model,</span>
<span class="sd">            where classes are ordered as they are in `self.classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="c1"># Y[i, j] gives the probability that sample i has the label j.</span>
        <span class="c1"># In the multi-label case, these are not disjoint.</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">e</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Only one estimator, but we still want to return probabilities</span>
            <span class="c1"># for two classes.</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">),</span> <span class="n">Y</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">multilabel_</span><span class="p">:</span>
            <span class="c1"># Then, probabilities should be normalized to 1.</span>
            <span class="n">Y</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">Y</span></div>

<div class="viewcode-block" id="OneVsRestClassifier.decision_function"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsrestclassifier.html#ibex.sklearn.multiclass.OneVsRestClassifier.decision_function">[docs]</a>    <span class="nd">@if_delegate_has_method</span><span class="p">([</span><span class="s1">&#39;_first_estimator&#39;</span><span class="p">,</span> <span class="s1">&#39;estimator&#39;</span><span class="p">])</span>
    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns the distance of each sample from the decision boundary for</span>
<span class="sd">        each class. This can only be used with estimators which implement the</span>
<span class="sd">        decision_function method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape = [n_samples, n_classes]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">est</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
                         <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span><span class="o">.</span><span class="n">T</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">multilabel_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Whether this is a multilabel classifier&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_binarizer_</span><span class="o">.</span><span class="n">y_type_</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">coef_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;coef_&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Base estimator doesn&#39;t have a coef_ attribute.&quot;</span><span class="p">)</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">coefs</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">return</span> <span class="n">sp</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">intercept_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;intercept_&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Base estimator doesn&#39;t have an intercept_ attribute.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">e</span><span class="o">.</span><span class="n">intercept_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_pairwise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Indicate if wrapped estimator is using a precomputed Gram matrix&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;_pairwise&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_first_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">_fit_ovo_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit a single binary estimator (one-vs-one).&quot;&quot;&quot;</span>
    <span class="n">cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="o">==</span> <span class="n">j</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span>
    <span class="n">y_binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="n">y_binary</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">y_binary</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">indcond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">cond</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_fit_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span>
                       <span class="n">_safe_split</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">indcond</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                       <span class="n">y_binary</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]),</span> <span class="n">indcond</span>


<span class="k">def</span> <span class="nf">_partial_fit_ovo_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Partially fit a single binary estimator(one-vs-one).&quot;&quot;&quot;</span>

    <span class="n">cond</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="o">==</span> <span class="n">j</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">cond</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">y_binary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y_binary</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">_partial_fit_binary</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">cond</span><span class="p">],</span> <span class="n">y_binary</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">estimator</span>


<span class="k">class</span> <span class="nc">OneVsOneClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">MetaEstimatorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;One-vs-one multiclass strategy</span>

<span class="sd">    This strategy consists in fitting one classifier per class pair.</span>
<span class="sd">    At prediction time, the class which received the most votes is selected.</span>
<span class="sd">    Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers,</span>
<span class="sd">    this method is usually slower than one-vs-the-rest, due to its</span>
<span class="sd">    O(n_classes^2) complexity. However, this method may be advantageous for</span>
<span class="sd">    algorithms such as kernel algorithms which don&#39;t scale well with</span>
<span class="sd">    `n_samples`. This is because each individual learning problem only involves</span>
<span class="sd">    a small subset of the data whereas, with one-vs-the-rest, the complete</span>
<span class="sd">    dataset is used `n_classes` times.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;ovo_classification&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator object</span>
<span class="sd">        An estimator object implementing `fit` and one of `decision_function`</span>
<span class="sd">        or `predict_proba`.</span>

<span class="sd">    n_jobs : int, optional, default: 1</span>
<span class="sd">        The number of jobs to use for the computation. If -1 all CPUs are used.</span>
<span class="sd">        If 1 is given, no parallel computing code is used at all, which is</span>
<span class="sd">        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are</span>
<span class="sd">        used. Thus for n_jobs = -2, all CPUs but one are used.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of `n_classes * (n_classes - 1) / 2` estimators</span>
<span class="sd">        Estimators used for predictions.</span>

<span class="sd">    classes_ : numpy array of shape [n_classes]</span>
<span class="sd">        Array containing labels.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

<div class="viewcode-block" id="OneVsOneClassifier.fit"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsoneclassifier.html#ibex.sklearn.multiclass.OneVsOneClassifier.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit underlying estimators.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Multi-class targets.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">])</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;OneVsOneClassifier can not be fit when only one&quot;</span>
                             <span class="s2">&quot; class is present.&quot;</span><span class="p">)</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">estimators_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_ovo_binary</span><span class="p">)</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)))))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">estimators_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_indices_</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">estimators_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pairwise</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_indices_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="OneVsOneClassifier.partial_fit"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsoneclassifier.html#ibex.sklearn.multiclass.OneVsOneClassifier.partial_fit">[docs]</a>    <span class="nd">@if_delegate_has_method</span><span class="p">(</span><span class="n">delegate</span><span class="o">=</span><span class="s1">&#39;estimator&#39;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Partially fit underlying estimators</span>

<span class="sd">        Should be used when memory is inefficient to train all data. Chunks</span>
<span class="sd">        of data can be passed in several iteration, where the first call</span>
<span class="sd">        should have an array of all target variables.</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Multi-class targets.</span>

<span class="sd">        classes : array, shape (n_classes, )</span>
<span class="sd">            Classes across all calls to partial_fit.</span>
<span class="sd">            Can be obtained via `np.unique(y_all)`, where y_all is the</span>
<span class="sd">            target vector of the entire dataset.</span>
<span class="sd">            This argument is only required in the first call of partial_fit</span>
<span class="sd">            and can be omitted in the subsequent calls.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_check_partial_fit_first_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classes</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[</span><span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                                <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">*</span>
                                      <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Mini-batch contains </span><span class="si">{0}</span><span class="s2"> while it &quot;</span>
                             <span class="s2">&quot;must be subset of </span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
                                                            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">])</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">combinations</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">_partial_fit_ovo_binary</span><span class="p">)(</span>
                    <span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">estimator</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="n">izip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span>
                                              <span class="p">(</span><span class="n">combinations</span><span class="p">)))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_indices_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="OneVsOneClassifier.predict"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsoneclassifier.html#ibex.sklearn.multiclass.OneVsOneClassifier.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Estimate the best class label for each sample in X.</span>

<span class="sd">        This is implemented as ``argmax(decision_function(X), axis=1)`` which</span>
<span class="sd">        will return the label of the class with most votes by estimators</span>
<span class="sd">        predicting the outcome of a decision for each possible class pair.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : numpy array of shape [n_samples]</span>
<span class="sd">            Predicted multi-class targets.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[(</span><span class="n">Y</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">Y</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span></div>

<div class="viewcode-block" id="OneVsOneClassifier.decision_function"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_onevsoneclassifier.html#ibex.sklearn.multiclass.OneVsOneClassifier.decision_function">[docs]</a>    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Decision function for the OneVsOneClassifier.</span>

<span class="sd">        The decision values for the samples are computed by adding the</span>
<span class="sd">        normalized sum of pair-wise classification confidence levels to the</span>
<span class="sd">        votes in order to disambiguate between the decision values when the</span>
<span class="sd">        votes for all the classes are equal leading to a tie.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Y : array-like, shape = [n_samples, n_classes]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pairwise_indices_</span>
        <span class="k">if</span> <span class="n">indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">Xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xi</span><span class="p">)</span>
                                 <span class="k">for</span> <span class="n">est</span><span class="p">,</span> <span class="n">Xi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">Xs</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
        <span class="n">confidences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">_predict_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">Xi</span><span class="p">)</span>
                                 <span class="k">for</span> <span class="n">est</span><span class="p">,</span> <span class="n">Xi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">Xs</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">_ovr_decision_function</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span>
                                   <span class="n">confidences</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">Y</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_classes_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_pairwise</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Indicate if wrapped estimator is using a precomputed Gram matrix&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;_pairwise&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OutputCodeClassifier</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">MetaEstimatorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;(Error-Correcting) Output-Code multiclass strategy</span>

<span class="sd">    Output-code based strategies consist in representing each class with a</span>
<span class="sd">    binary code (an array of 0s and 1s). At fitting time, one binary</span>
<span class="sd">    classifier per bit in the code book is fitted.  At prediction time, the</span>
<span class="sd">    classifiers are used to project new points in the class space and the class</span>
<span class="sd">    closest to the points is chosen. The main advantage of these strategies is</span>
<span class="sd">    that the number of classifiers used can be controlled by the user, either</span>
<span class="sd">    for compressing the model (0 &lt; code_size &lt; 1) or for making the model more</span>
<span class="sd">    robust to errors (code_size &gt; 1). See the documentation for more details.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;ecoc&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    estimator : estimator object</span>
<span class="sd">        An estimator object implementing `fit` and one of `decision_function`</span>
<span class="sd">        or `predict_proba`.</span>

<span class="sd">    code_size : float</span>
<span class="sd">        Percentage of the number of classes to be used to create the code book.</span>
<span class="sd">        A number between 0 and 1 will require fewer classifiers than</span>
<span class="sd">        one-vs-the-rest. A number greater than 1 will require more classifiers</span>
<span class="sd">        than one-vs-the-rest.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional, default: None</span>
<span class="sd">        The generator used to initialize the codebook.  If int, random_state is</span>
<span class="sd">        the seed used by the random number generator; If RandomState instance,</span>
<span class="sd">        random_state is the random number generator; If None, the random number</span>
<span class="sd">        generator is the RandomState instance used by `np.random`.</span>

<span class="sd">    n_jobs : int, optional, default: 1</span>
<span class="sd">        The number of jobs to use for the computation. If -1 all CPUs are used.</span>
<span class="sd">        If 1 is given, no parallel computing code is used at all, which is</span>
<span class="sd">        useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are</span>
<span class="sd">        used. Thus for n_jobs = -2, all CPUs but one are used.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of `int(n_classes * code_size)` estimators</span>
<span class="sd">        Estimators used for predictions.</span>

<span class="sd">    classes_ : numpy array of shape [n_classes]</span>
<span class="sd">        Array containing labels.</span>

<span class="sd">    code_book_ : numpy array of shape [n_classes, code_size]</span>
<span class="sd">        Binary array containing the code of each class.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] &quot;Solving multiclass learning problems via error-correcting output</span>
<span class="sd">       codes&quot;,</span>
<span class="sd">       Dietterich T., Bakiri G.,</span>
<span class="sd">       Journal of Artificial Intelligence Research 2,</span>
<span class="sd">       1995.</span>

<span class="sd">    .. [2] &quot;The error coding method and PICTs&quot;,</span>
<span class="sd">       James G., Hastie T.,</span>
<span class="sd">       Journal of Computational and Graphical statistics 7,</span>
<span class="sd">       1998.</span>

<span class="sd">    .. [3] &quot;The Elements of Statistical Learning&quot;,</span>
<span class="sd">       Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)</span>
<span class="sd">       2008.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">code_size</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">code_size</span> <span class="o">=</span> <span class="n">code_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

<div class="viewcode-block" id="OutputCodeClassifier.fit"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_outputcodeclassifier.html#ibex.sklearn.multiclass.OutputCodeClassifier.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit underlying estimators.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        y : numpy array of shape [n_samples]</span>
<span class="sd">            Multi-class targets.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">code_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;code_size should be greater than 0, got </span><span class="si">{0}</span><span class="s2">&quot;</span>
                             <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">code_size</span><span class="p">))</span>

        <span class="n">_check_estimator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">code_size_</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_classes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">code_size</span><span class="p">)</span>

        <span class="c1"># FIXME: there are more elaborate methods than generating the codebook</span>
        <span class="c1"># randomly.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">code_size_</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="s2">&quot;decision_function&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">classes_index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span><span class="p">[</span><span class="n">classes_index</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]]</span>
                      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_binary</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="OutputCodeClassifier.predict"><a class="viewcode-back" href="../../api_ibex_sklearn_multiclass_outputcodeclassifier.html#ibex.sklearn.multiclass.OutputCodeClassifier.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict multi-class targets using underlying estimators.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : (sparse) array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Data.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : numpy array of shape [n_samples]</span>
<span class="sd">            Predicted multi-class targets.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">_predict_binary</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">code_book_</span><span class="p">)</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../index.html">
              <img class="logo" src="../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>