
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.ensemble.weight_boosting &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.ensemble.weight_boosting</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Weight Boosting</span>

<span class="sd">This module contains weight boosting estimators for both classification and</span>
<span class="sd">regression.</span>

<span class="sd">The module structure is the following:</span>

<span class="sd">- The ``BaseWeightBoosting`` base class implements a common ``fit`` method</span>
<span class="sd">  for all the estimators in the module. Regression and classification</span>
<span class="sd">  only differ from each other in the loss function that is optimized.</span>

<span class="sd">- ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for</span>
<span class="sd">  classification problems.</span>

<span class="sd">- ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for</span>
<span class="sd">  regression problems.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Noel Dawe &lt;noel@dawe.me&gt;</span>
<span class="c1">#          Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="c1">#          Hamzeh Alsalhi &lt;ha258@cornell.edu&gt;</span>
<span class="c1">#          Arnaud Joly &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.core.umath_tests</span> <span class="k">import</span> <span class="n">inner1d</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">BaseEnsemble</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">is_regressor</span><span class="p">,</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">..externals</span> <span class="k">import</span> <span class="n">six</span>
<span class="kn">from</span> <span class="nn">..externals.six.moves</span> <span class="k">import</span> <span class="nb">zip</span>
<span class="kn">from</span> <span class="nn">..externals.six.moves</span> <span class="k">import</span> <span class="n">xrange</span> <span class="k">as</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">.forest</span> <span class="k">import</span> <span class="n">BaseForest</span>
<span class="kn">from</span> <span class="nn">..tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">..tree.tree</span> <span class="k">import</span> <span class="n">BaseDecisionTree</span>
<span class="kn">from</span> <span class="nn">..tree._tree</span> <span class="k">import</span> <span class="n">DTYPE</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_X_y</span><span class="p">,</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="k">import</span> <span class="n">stable_cumsum</span>
<span class="kn">from</span> <span class="nn">..metrics</span> <span class="k">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="k">import</span> <span class="n">has_fit_parameter</span><span class="p">,</span> <span class="n">check_is_fitted</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;AdaBoostClassifier&#39;</span><span class="p">,</span>
    <span class="s1">&#39;AdaBoostRegressor&#39;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">class</span> <span class="nc">BaseWeightBoosting</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEnsemble</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for AdaBoost estimators.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">BaseWeightBoosting</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build a boosted classifier/regressor from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is</span>
<span class="sd">            forced to DTYPE from tree._tree if the base classifier of this</span>
<span class="sd">            ensemble weighted boosting classifier is a tree or forest.</span>

<span class="sd">        y : array-like of shape = [n_samples]</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">            Sample weights. If None, the sample weights are initialized to</span>
<span class="sd">            1 / n_samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;learning_rate must be greater than zero&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span><span class="p">,</span> <span class="p">(</span><span class="n">BaseDecisionTree</span><span class="p">,</span>
                                                 <span class="n">BaseForest</span><span class="p">))):</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">DTYPE</span>
            <span class="n">accept_sparse</span> <span class="o">=</span> <span class="s1">&#39;csc&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">accept_sparse</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">]</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="n">accept_sparse</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                         <span class="n">y_numeric</span><span class="o">=</span><span class="n">is_regressor</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Initialize weights to 1 / n_samples</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
            <span class="n">sample_weight</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="c1"># Normalize existing weights</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="c1"># Check that the sample weights sum is positive</span>
            <span class="k">if</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Attempting to fit with a non-positive &quot;</span>
                    <span class="s2">&quot;weighted number of samples.&quot;</span><span class="p">)</span>

        <span class="c1"># Check parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

        <span class="c1"># Clear any previous fit results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator_errors_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">iboost</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="c1"># Boosting step</span>
            <span class="n">sample_weight</span><span class="p">,</span> <span class="n">estimator_weight</span><span class="p">,</span> <span class="n">estimator_error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_boost</span><span class="p">(</span>
                <span class="n">iboost</span><span class="p">,</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">sample_weight</span><span class="p">,</span>
                <span class="n">random_state</span><span class="p">)</span>

            <span class="c1"># Early termination</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">[</span><span class="n">iboost</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator_weight</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimator_errors_</span><span class="p">[</span><span class="n">iboost</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator_error</span>

            <span class="c1"># Stop if error is zero</span>
            <span class="k">if</span> <span class="n">estimator_error</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">sample_weight_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>

            <span class="c1"># Stop if the sum of sample weights has become non-positive</span>
            <span class="k">if</span> <span class="n">sample_weight_sum</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="k">if</span> <span class="n">iboost</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Normalize</span>
                <span class="n">sample_weight</span> <span class="o">/=</span> <span class="n">sample_weight_sum</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_boost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Implement a single boost.</span>

<span class="sd">        Warning: This method needs to be overridden by subclasses.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        iboost : int</span>
<span class="sd">            The index of the current boost iteration.</span>

<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">        y : array-like of shape = [n_samples]</span>
<span class="sd">            The target values (class labels).</span>

<span class="sd">        sample_weight : array-like of shape = [n_samples]</span>
<span class="sd">            The current sample weights.</span>

<span class="sd">        random_state : numpy.RandomState</span>
<span class="sd">            The current random number generator</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sample_weight : array-like of shape = [n_samples] or None</span>
<span class="sd">            The reweighted sample weights.</span>
<span class="sd">            If None then boosting has terminated early.</span>

<span class="sd">        estimator_weight : float</span>
<span class="sd">            The weight for the current boost.</span>
<span class="sd">            If None then boosting has terminated early.</span>

<span class="sd">        error : float</span>
<span class="sd">            The classification error for the current boost.</span>
<span class="sd">            If None then boosting has terminated early.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">staged_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return staged scores for X, y.</span>

<span class="sd">        This generator method yields the ensemble score after each iteration of</span>
<span class="sd">        boosting and therefore allows monitoring, such as to determine the</span>
<span class="sd">        score on a test set after each boost.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Labels for X.</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">            Sample weights.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        z : float</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">y_pred</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">staged_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the feature importances (the higher, the more important the</span>
<span class="sd">           feature).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_importances_ : array, shape = [n_features]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Estimator not fitted, &quot;</span>
                             <span class="s2">&quot;call `fit` before `feature_importances_`.&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">return</span> <span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">clf</span>
                    <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>
                    <span class="o">/</span> <span class="n">norm</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to compute feature importances &quot;</span>
                <span class="s2">&quot;since base_estimator does not have a &quot;</span>
                <span class="s2">&quot;feature_importances_ attribute&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_X_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Ensure that X is in the proper format&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span><span class="p">,</span>
                           <span class="p">(</span><span class="n">BaseDecisionTree</span><span class="p">,</span> <span class="n">BaseForest</span><span class="p">))):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">X</span>


<span class="k">def</span> <span class="nf">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate algorithm 4, step 2, equation c) of Zhu et al [1].</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, &quot;Multi-class AdaBoost&quot;, 2009.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">proba</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># Displace zero probabilities so the log is defined.</span>
    <span class="c1"># Also fix negative elements which may occur with</span>
    <span class="c1"># negative sample weights.</span>
    <span class="n">proba</span><span class="p">[</span><span class="n">proba</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">proba</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">proba</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>
    <span class="n">log_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">log_proba</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">n_classes</span><span class="p">)</span>
                              <span class="o">*</span> <span class="n">log_proba</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">AdaBoostClassifier</span><span class="p">(</span><span class="n">BaseWeightBoosting</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An AdaBoost classifier.</span>

<span class="sd">    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a</span>
<span class="sd">    classifier on the original dataset and then fits additional copies of the</span>
<span class="sd">    classifier on the same dataset but where the weights of incorrectly</span>
<span class="sd">    classified instances are adjusted such that subsequent classifiers focus</span>
<span class="sd">    more on difficult cases.</span>

<span class="sd">    This class implements the algorithm known as AdaBoost-SAMME [2].</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;adaboost&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object, optional (default=DecisionTreeClassifier)</span>
<span class="sd">        The base estimator from which the boosted ensemble is built.</span>
<span class="sd">        Support for sample weighting is required, as well as proper `classes_`</span>
<span class="sd">        and `n_classes_` attributes.</span>

<span class="sd">    n_estimators : integer, optional (default=50)</span>
<span class="sd">        The maximum number of estimators at which boosting is terminated.</span>
<span class="sd">        In case of perfect fit, the learning procedure is stopped early.</span>

<span class="sd">    learning_rate : float, optional (default=1.)</span>
<span class="sd">        Learning rate shrinks the contribution of each classifier by</span>
<span class="sd">        ``learning_rate``. There is a trade-off between ``learning_rate`` and</span>
<span class="sd">        ``n_estimators``.</span>

<span class="sd">    algorithm : {&#39;SAMME&#39;, &#39;SAMME.R&#39;}, optional (default=&#39;SAMME.R&#39;)</span>
<span class="sd">        If &#39;SAMME.R&#39; then use the SAMME.R real boosting algorithm.</span>
<span class="sd">        ``base_estimator`` must support calculation of class probabilities.</span>
<span class="sd">        If &#39;SAMME&#39; then use the SAMME discrete boosting algorithm.</span>
<span class="sd">        The SAMME.R algorithm typically converges faster than SAMME,</span>
<span class="sd">        achieving a lower test error with fewer boosting iterations.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of classifiers</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    classes_ : array of shape = [n_classes]</span>
<span class="sd">        The classes labels.</span>

<span class="sd">    n_classes_ : int</span>
<span class="sd">        The number of classes.</span>

<span class="sd">    estimator_weights_ : array of floats</span>
<span class="sd">        Weights for each estimator in the boosted ensemble.</span>

<span class="sd">    estimator_errors_ : array of floats</span>
<span class="sd">        Classification error for each estimator in the boosted</span>
<span class="sd">        ensemble.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances if supported by the ``base_estimator``.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Y. Freund, R. Schapire, &quot;A Decision-Theoretic Generalization of</span>
<span class="sd">           on-Line Learning and an Application to Boosting&quot;, 1995.</span>

<span class="sd">    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, &quot;Multi-class AdaBoost&quot;, 2009.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;SAMME.R&#39;</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="n">algorithm</span>

<div class="viewcode-block" id="AdaBoostClassifier.fit"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build a boosted classifier from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        y : array-like of shape = [n_samples]</span>
<span class="sd">            The target values (class labels).</span>

<span class="sd">        sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">            Sample weights. If None, the sample weights are initialized to</span>
<span class="sd">            ``1 / n_samples``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check that algorithm is supported</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;SAMME&#39;</span><span class="p">,</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;algorithm </span><span class="si">%s</span><span class="s2"> is not supported&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)</span>

        <span class="c1"># Fit</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_validate_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check the estimator and set the base_estimator_ attribute.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">(</span>
            <span class="n">default</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1">#  SAMME-R requires predict_proba-enabled base estimators</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">,</span> <span class="s1">&#39;predict_proba&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;AdaBoostClassifier with algorithm=&#39;SAMME.R&#39; requires &quot;</span>
                    <span class="s2">&quot;that the weak learner supports the calculation of class &quot;</span>
                    <span class="s2">&quot;probabilities with a predict_proba method.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s2">&quot;Please change the base estimator or set &quot;</span>
                    <span class="s2">&quot;algorithm=&#39;SAMME&#39; instead.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">has_fit_parameter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="p">,</span> <span class="s2">&quot;sample_weight&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> doesn&#39;t support sample_weight.&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator_</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_boost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Implement a single boost.</span>

<span class="sd">        Perform a single boost according to the real multi-class SAMME.R</span>
<span class="sd">        algorithm or to the discrete SAMME algorithm and return the updated</span>
<span class="sd">        sample weights.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        iboost : int</span>
<span class="sd">            The index of the current boost iteration.</span>

<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        y : array-like of shape = [n_samples]</span>
<span class="sd">            The target values (class labels).</span>

<span class="sd">        sample_weight : array-like of shape = [n_samples]</span>
<span class="sd">            The current sample weights.</span>

<span class="sd">        random_state : numpy.RandomState</span>
<span class="sd">            The current random number generator</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sample_weight : array-like of shape = [n_samples] or None</span>
<span class="sd">            The reweighted sample weights.</span>
<span class="sd">            If None then boosting has terminated early.</span>

<span class="sd">        estimator_weight : float</span>
<span class="sd">            The weight for the current boost.</span>
<span class="sd">            If None then boosting has terminated early.</span>

<span class="sd">        estimator_error : float</span>
<span class="sd">            The classification error for the current boost.</span>
<span class="sd">            If None then boosting has terminated early.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_boost_real</span><span class="p">(</span><span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># elif self.algorithm == &quot;SAMME&quot;:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_boost_discrete</span><span class="p">(</span><span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                        <span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_boost_real</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Implement a single boost using the SAMME.R real algorithm.&quot;&quot;&quot;</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">y_predict_proba</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">iboost</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s1">&#39;classes_&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

        <span class="n">y_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_predict_proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                       <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Instances incorrectly classified</span>
        <span class="n">incorrect</span> <span class="o">=</span> <span class="n">y_predict</span> <span class="o">!=</span> <span class="n">y</span>

        <span class="c1"># Error fraction</span>
        <span class="n">estimator_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">incorrect</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># Stop if classification is perfect</span>
        <span class="k">if</span> <span class="n">estimator_error</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span>

        <span class="c1"># Construct y coding as described in Zhu et al [2]:</span>
        <span class="c1">#</span>
        <span class="c1">#    y_k = 1 if c == k else -1 / (K - 1)</span>
        <span class="c1">#</span>
        <span class="c1"># where K == n_classes_ and c, k in [0, K) are indices along the second</span>
        <span class="c1"># axis of the y coding with c being the index corresponding to the true</span>
        <span class="c1"># class label.</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>
        <span class="n">y_codes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">1.</span><span class="p">])</span>
        <span class="n">y_coding</span> <span class="o">=</span> <span class="n">y_codes</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">classes</span> <span class="o">==</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

        <span class="c1"># Displace zero probabilities so the log is defined.</span>
        <span class="c1"># Also fix negative elements which may occur with</span>
        <span class="c1"># negative sample weights.</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">y_predict_proba</span>  <span class="c1"># alias for readability</span>
        <span class="n">proba</span><span class="p">[</span><span class="n">proba</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">proba</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">proba</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span>

        <span class="c1"># Boost weight using multi-class AdaBoost SAMME.R alg</span>
        <span class="n">estimator_weight</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>
                                <span class="o">*</span> <span class="p">(((</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_classes</span><span class="p">)</span> <span class="o">*</span>
                                   <span class="n">inner1d</span><span class="p">(</span><span class="n">y_coding</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_predict_proba</span><span class="p">))))</span>

        <span class="c1"># Only boost the weights if it will fit again</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">iboost</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Only boost positive weights</span>
            <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">estimator_weight</span> <span class="o">*</span>
                                    <span class="p">((</span><span class="n">sample_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span>
                                     <span class="p">(</span><span class="n">estimator_weight</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">estimator_error</span>

    <span class="k">def</span> <span class="nf">_boost_discrete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Implement a single boost using the SAMME discrete algorithm.&quot;&quot;&quot;</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">y_predict</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">iboost</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="s1">&#39;classes_&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

        <span class="c1"># Instances incorrectly classified</span>
        <span class="n">incorrect</span> <span class="o">=</span> <span class="n">y_predict</span> <span class="o">!=</span> <span class="n">y</span>

        <span class="c1"># Error fraction</span>
        <span class="n">estimator_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">incorrect</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># Stop if classification is perfect</span>
        <span class="k">if</span> <span class="n">estimator_error</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>

        <span class="c1"># Stop if the error is at least as bad as random guessing</span>
        <span class="k">if</span> <span class="n">estimator_error</span> <span class="o">&gt;=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">n_classes</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;BaseClassifier in AdaBoostClassifier &#39;</span>
                                 <span class="s1">&#39;ensemble is worse than random, ensemble &#39;</span>
                                 <span class="s1">&#39;can not be fit.&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="c1"># Boost weight using multi-class AdaBoost SAMME alg</span>
        <span class="n">estimator_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">estimator_error</span><span class="p">)</span> <span class="o">/</span> <span class="n">estimator_error</span><span class="p">)</span> <span class="o">+</span>
            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">))</span>

        <span class="c1"># Only boost the weights if I will fit again</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">iboost</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Only boost positive weights</span>
            <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">estimator_weight</span> <span class="o">*</span> <span class="n">incorrect</span> <span class="o">*</span>
                                    <span class="p">((</span><span class="n">sample_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span>
                                     <span class="p">(</span><span class="n">estimator_weight</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">estimator_weight</span><span class="p">,</span> <span class="n">estimator_error</span>

<div class="viewcode-block" id="AdaBoostClassifier.predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict classes for X.</span>

<span class="sd">        The predicted class of an input sample is computed as the weighted mean</span>
<span class="sd">        prediction of the classifiers in the ensemble.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples]</span>
<span class="sd">            The predicted classes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="AdaBoostClassifier.staged_predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.staged_predict">[docs]</a>    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return staged predictions for X.</span>

<span class="sd">        The predicted class of an input sample is computed as the weighted mean</span>
<span class="sd">        prediction of the classifiers in the ensemble.</span>

<span class="sd">        This generator method yields the ensemble prediction after each</span>
<span class="sd">        iteration of boosting and therefore allows monitoring, such as to</span>
<span class="sd">        determine the prediction on a test set after each boost.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of array, shape = [n_samples]</span>
<span class="sd">            The predicted classes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">take</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span></div>

<div class="viewcode-block" id="AdaBoostClassifier.decision_function"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.decision_function">[docs]</a>    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the decision function of ``X``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : array, shape = [n_samples, k]</span>
<span class="sd">            The decision function of the input samples. The order of</span>
<span class="sd">            outputs is the same of that of the `classes_` attribute.</span>
<span class="sd">            Binary classification is a special cases with ``k == 1``,</span>
<span class="sd">            otherwise ``k==n_classes``. For binary classification,</span>
<span class="sd">            values closer to -1 or 1 mean more like the first or second</span>
<span class="sd">            class in ``classes_``, respectively.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_classes_&quot;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">:</span>
            <span class="c1"># The weights are all 1. for SAMME.R</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
                       <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># self.algorithm == &quot;SAMME&quot;</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">classes</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">w</span>
                       <span class="k">for</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span>
                                               <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">))</span>

        <span class="n">pred</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">return</span> <span class="n">pred</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred</span></div>

<div class="viewcode-block" id="AdaBoostClassifier.staged_decision_function"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.staged_decision_function">[docs]</a>    <span class="k">def</span> <span class="nf">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each boosting iteration.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each boosting iteration.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : generator of array, shape = [n_samples, k]</span>
<span class="sd">            The decision function of the input samples. The order of</span>
<span class="sd">            outputs is the same of that of the `classes_` attribute.</span>
<span class="sd">            Binary classification is a special cases with ``k == 1``,</span>
<span class="sd">            otherwise ``k==n_classes``. For binary classification,</span>
<span class="sd">            values closer to -1 or 1 mean more like the first or second</span>
<span class="sd">            class in ``classes_``, respectively.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_classes_&quot;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
            <span class="n">norm</span> <span class="o">+=</span> <span class="n">weight</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">:</span>
                <span class="c1"># The weights are all 1. for SAMME.R</span>
                <span class="n">current_pred</span> <span class="o">=</span> <span class="n">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># elif self.algorithm == &quot;SAMME&quot;:</span>
                <span class="n">current_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">current_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_pred</span> <span class="o">==</span> <span class="n">classes</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">weight</span>

            <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">current_pred</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pred</span> <span class="o">+=</span> <span class="n">current_pred</span>

            <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">tmp_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
                <span class="n">tmp_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="k">yield</span> <span class="p">(</span><span class="n">tmp_pred</span> <span class="o">/</span> <span class="n">norm</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">pred</span> <span class="o">/</span> <span class="n">norm</span></div>

<div class="viewcode-block" id="AdaBoostClassifier.predict_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">        The predicted class probabilities of an input sample is computed as</span>
<span class="sd">        the weighted mean predicted class probabilities of the classifiers</span>
<span class="sd">        in the ensemble.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : array of shape = [n_samples]</span>
<span class="sd">            The class probabilities of the input samples. The order of</span>
<span class="sd">            outputs is the same of that of the `classes_` attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;n_classes_&quot;</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">:</span>
            <span class="c1"># The weights are all 1. for SAMME.R</span>
            <span class="n">proba</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># self.algorithm == &quot;SAMME&quot;</span>
            <span class="n">proba</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span>
                        <span class="k">for</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">))</span>

        <span class="n">proba</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">proba</span><span class="p">)</span>
        <span class="n">normalizer</span> <span class="o">=</span> <span class="n">proba</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">normalizer</span><span class="p">[</span><span class="n">normalizer</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">proba</span> <span class="o">/=</span> <span class="n">normalizer</span>

        <span class="k">return</span> <span class="n">proba</span></div>

<div class="viewcode-block" id="AdaBoostClassifier.staged_predict_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.staged_predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">        The predicted class probabilities of an input sample is computed as</span>
<span class="sd">        the weighted mean predicted class probabilities of the classifiers</span>
<span class="sd">        in the ensemble.</span>

<span class="sd">        This generator method yields the ensemble predicted class probabilities</span>
<span class="sd">        after each iteration of boosting and therefore allows monitoring, such</span>
<span class="sd">        as to determine the predicted class probabilities on a test set after</span>
<span class="sd">        each boost.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : generator of array, shape = [n_samples]</span>
<span class="sd">            The class probabilities of the input samples. The order of</span>
<span class="sd">            outputs is the same of that of the `classes_` attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
            <span class="n">norm</span> <span class="o">+=</span> <span class="n">weight</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s1">&#39;SAMME.R&#39;</span><span class="p">:</span>
                <span class="c1"># The weights are all 1. for SAMME.R</span>
                <span class="n">current_proba</span> <span class="o">=</span> <span class="n">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># elif self.algorithm == &quot;SAMME&quot;:</span>
                <span class="n">current_proba</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight</span>

            <span class="k">if</span> <span class="n">proba</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">proba</span> <span class="o">=</span> <span class="n">current_proba</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">proba</span> <span class="o">+=</span> <span class="n">current_proba</span>

            <span class="n">real_proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">proba</span> <span class="o">/</span> <span class="n">norm</span><span class="p">))</span>
            <span class="n">normalizer</span> <span class="o">=</span> <span class="n">real_proba</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
            <span class="n">normalizer</span><span class="p">[</span><span class="n">normalizer</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="n">real_proba</span> <span class="o">/=</span> <span class="n">normalizer</span>

            <span class="k">yield</span> <span class="n">real_proba</span></div>

<div class="viewcode-block" id="AdaBoostClassifier.predict_log_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostclassifier.html#ibex.sklearn.ensemble.AdaBoostClassifier.predict_log_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>

<span class="sd">        The predicted class log-probabilities of an input sample is computed as</span>
<span class="sd">        the weighted mean predicted class log-probabilities of the classifiers</span>
<span class="sd">        in the ensemble.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : array of shape = [n_samples]</span>
<span class="sd">            The class probabilities of the input samples. The order of</span>
<span class="sd">            outputs is the same of that of the `classes_` attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span></div>


<span class="k">class</span> <span class="nc">AdaBoostRegressor</span><span class="p">(</span><span class="n">BaseWeightBoosting</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An AdaBoost regressor.</span>

<span class="sd">    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a</span>
<span class="sd">    regressor on the original dataset and then fits additional copies of the</span>
<span class="sd">    regressor on the same dataset but where the weights of instances are</span>
<span class="sd">    adjusted according to the error of the current prediction. As such,</span>
<span class="sd">    subsequent regressors focus more on difficult cases.</span>

<span class="sd">    This class implements the algorithm known as AdaBoost.R2 [2].</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;adaboost&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator : object, optional (default=DecisionTreeRegressor)</span>
<span class="sd">        The base estimator from which the boosted ensemble is built.</span>
<span class="sd">        Support for sample weighting is required.</span>

<span class="sd">    n_estimators : integer, optional (default=50)</span>
<span class="sd">        The maximum number of estimators at which boosting is terminated.</span>
<span class="sd">        In case of perfect fit, the learning procedure is stopped early.</span>

<span class="sd">    learning_rate : float, optional (default=1.)</span>
<span class="sd">        Learning rate shrinks the contribution of each regressor by</span>
<span class="sd">        ``learning_rate``. There is a trade-off between ``learning_rate`` and</span>
<span class="sd">        ``n_estimators``.</span>

<span class="sd">    loss : {&#39;linear&#39;, &#39;square&#39;, &#39;exponential&#39;}, optional (default=&#39;linear&#39;)</span>
<span class="sd">        The loss function to use when updating the weights after each</span>
<span class="sd">        boosting iteration.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of classifiers</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    estimator_weights_ : array of floats</span>
<span class="sd">        Weights for each estimator in the boosted ensemble.</span>

<span class="sd">    estimator_errors_ : array of floats</span>
<span class="sd">        Regression error for each estimator in the boosted ensemble.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances if supported by the ``base_estimator``.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Y. Freund, R. Schapire, &quot;A Decision-Theoretic Generalization of</span>
<span class="sd">           on-Line Learning and an Application to Boosting&quot;, 1995.</span>

<span class="sd">    .. [2] H. Drucker, &quot;Improving Regressors using Boosting Techniques&quot;, 1997.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                 <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBoostRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

<div class="viewcode-block" id="AdaBoostRegressor.fit"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostregressor.html#ibex.sklearn.ensemble.AdaBoostRegressor.fit">[docs]</a>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build a boosted regressor from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        y : array-like of shape = [n_samples]</span>
<span class="sd">            The target values (real numbers).</span>

<span class="sd">        sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">            Sample weights. If None, the sample weights are initialized to</span>
<span class="sd">            1 / n_samples.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Check loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;square&#39;</span><span class="p">,</span> <span class="s1">&#39;exponential&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;loss must be &#39;linear&#39;, &#39;square&#39;, or &#39;exponential&#39;&quot;</span><span class="p">)</span>

        <span class="c1"># Fit</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">AdaBoostRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_validate_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check the estimator and set the base_estimator_ attribute.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBoostRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">(</span>
            <span class="n">default</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_boost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iboost</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Implement a single boost for regression</span>

<span class="sd">        Perform a single boost according to the AdaBoost.R2 algorithm and</span>
<span class="sd">        return the updated sample weights.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        iboost : int</span>
<span class="sd">            The index of the current boost iteration.</span>

<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        y : array-like of shape = [n_samples]</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        sample_weight : array-like of shape = [n_samples]</span>
<span class="sd">            The current sample weights.</span>

<span class="sd">        random_state : numpy.RandomState</span>
<span class="sd">            The current random number generator</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sample_weight : array-like of shape = [n_samples] or None</span>
<span class="sd">            The reweighted sample weights.</span>
<span class="sd">            If None then boosting has terminated early.</span>

<span class="sd">        estimator_weight : float</span>
<span class="sd">            The weight for the current boost.</span>
<span class="sd">            If None then boosting has terminated early.</span>

<span class="sd">        estimator_error : float</span>
<span class="sd">            The regression error for the current boost.</span>
<span class="sd">            If None then boosting has terminated early.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

        <span class="c1"># Weighted sampling of the training set with replacement</span>
        <span class="c1"># For NumPy &gt;= 1.7.0 use np.random.choice</span>
        <span class="n">cdf</span> <span class="o">=</span> <span class="n">stable_cumsum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">cdf</span> <span class="o">/=</span> <span class="n">cdf</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">uniform_samples</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">random_sample</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">bootstrap_idx</span> <span class="o">=</span> <span class="n">cdf</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">uniform_samples</span><span class="p">,</span> <span class="n">side</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
        <span class="c1"># searchsorted returns a scalar</span>
        <span class="n">bootstrap_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bootstrap_idx</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Fit on the bootstrapped sample and obtain a prediction</span>
        <span class="c1"># for all samples in the training set</span>
        <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">bootstrap_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">bootstrap_idx</span><span class="p">])</span>
        <span class="n">y_predict</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">error_vect</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y_predict</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">error_max</span> <span class="o">=</span> <span class="n">error_vect</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">error_max</span> <span class="o">!=</span> <span class="mf">0.</span><span class="p">:</span>
            <span class="n">error_vect</span> <span class="o">/=</span> <span class="n">error_max</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;square&#39;</span><span class="p">:</span>
            <span class="n">error_vect</span> <span class="o">**=</span> <span class="mi">2</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;exponential&#39;</span><span class="p">:</span>
            <span class="n">error_vect</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">error_vect</span><span class="p">)</span>

        <span class="c1"># Calculate the average loss</span>
        <span class="n">estimator_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">error_vect</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">estimator_error</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Stop if fit is perfect</span>
            <span class="k">return</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span>

        <span class="k">elif</span> <span class="n">estimator_error</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="c1"># Discard current estimator only if it isn&#39;t the only one</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="n">beta</span> <span class="o">=</span> <span class="n">estimator_error</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">estimator_error</span><span class="p">)</span>

        <span class="c1"># Boost weight using AdaBoost.R2 alg</span>
        <span class="n">estimator_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">iboost</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span>
                <span class="n">beta</span><span class="p">,</span>
                <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">error_vect</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">estimator_weight</span><span class="p">,</span> <span class="n">estimator_error</span>

    <span class="k">def</span> <span class="nf">_get_median_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">limit</span><span class="p">):</span>
        <span class="c1"># Evaluate predictions of all estimators</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
            <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">est</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[:</span><span class="n">limit</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

        <span class="c1"># Sort the predictions</span>
        <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Find index of median prediction for each sample</span>
        <span class="n">weight_cdf</span> <span class="o">=</span> <span class="n">stable_cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">median_or_above</span> <span class="o">=</span> <span class="n">weight_cdf</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">weight_cdf</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">median_idx</span> <span class="o">=</span> <span class="n">median_or_above</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">median_estimators</span> <span class="o">=</span> <span class="n">sorted_idx</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">median_idx</span><span class="p">]</span>

        <span class="c1"># Return median predictions</span>
        <span class="k">return</span> <span class="n">predictions</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">median_estimators</span><span class="p">]</span>

<div class="viewcode-block" id="AdaBoostRegressor.predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostregressor.html#ibex.sklearn.ensemble.AdaBoostRegressor.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression value for X.</span>

<span class="sd">        The predicted regression value of an input sample is computed</span>
<span class="sd">        as the weighted median prediction of the classifiers in the ensemble.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples]</span>
<span class="sd">            The predicted regression values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimator_weights_&quot;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_median_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span></div>

<div class="viewcode-block" id="AdaBoostRegressor.staged_predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_adaboostregressor.html#ibex.sklearn.ensemble.AdaBoostRegressor.staged_predict">[docs]</a>    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return staged predictions for X.</span>

<span class="sd">        The predicted regression value of an input sample is computed</span>
<span class="sd">        as the weighted median prediction of the classifiers in the ensemble.</span>

<span class="sd">        This generator method yields the ensemble prediction after each</span>
<span class="sd">        iteration of boosting and therefore allows monitoring, such as to</span>
<span class="sd">        determine the prediction on a test set after each boost.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">            DOK, or LIL. DOK and LIL are converted to CSR.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of array, shape = [n_samples]</span>
<span class="sd">            The predicted regression values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimator_weights_&quot;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_median_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">i</span><span class="p">)</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>