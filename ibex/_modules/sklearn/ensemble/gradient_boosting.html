
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sklearn.ensemble.gradient_boosting &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sklearn.ensemble.gradient_boosting</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Gradient Boosted Regression Trees</span>

<span class="sd">This module contains methods for fitting gradient boosted regression trees for</span>
<span class="sd">both classification and regression.</span>

<span class="sd">The module structure is the following:</span>

<span class="sd">- The ``BaseGradientBoosting`` base class implements a common ``fit`` method</span>
<span class="sd">  for all the estimators in the module. Regression and classification</span>
<span class="sd">  only differ in the concrete ``LossFunction`` used.</span>

<span class="sd">- ``GradientBoostingClassifier`` implements gradient boosting for</span>
<span class="sd">  classification problems.</span>

<span class="sd">- ``GradientBoostingRegressor`` implements gradient boosting for</span>
<span class="sd">  regression problems.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,</span>
<span class="c1">#          Arnaud Joly, Jacob Schreiber</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">ABCMeta</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="k">import</span> <span class="n">abstractmethod</span>

<span class="kn">from</span> <span class="nn">.base</span> <span class="k">import</span> <span class="n">BaseEnsemble</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">ClassifierMixin</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..externals</span> <span class="k">import</span> <span class="n">six</span>

<span class="kn">from</span> <span class="nn">._gradient_boosting</span> <span class="k">import</span> <span class="n">predict_stages</span>
<span class="kn">from</span> <span class="nn">._gradient_boosting</span> <span class="k">import</span> <span class="n">predict_stage</span>
<span class="kn">from</span> <span class="nn">._gradient_boosting</span> <span class="k">import</span> <span class="n">_random_sample_mask</span>

<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">scipy</span> <span class="k">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="k">import</span> <span class="n">csc_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="k">import</span> <span class="n">csr_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="k">import</span> <span class="n">issparse</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="k">import</span> <span class="n">expit</span>

<span class="kn">from</span> <span class="nn">time</span> <span class="k">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">..tree.tree</span> <span class="k">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">..tree._tree</span> <span class="k">import</span> <span class="n">DTYPE</span>
<span class="kn">from</span> <span class="nn">..tree._tree</span> <span class="k">import</span> <span class="n">TREE_LEAF</span>

<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">column_or_1d</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_consistent_length</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="k">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">..utils.stats</span> <span class="k">import</span> <span class="n">_weighted_percentile</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="k">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="k">import</span> <span class="n">NotFittedError</span>


<span class="k">class</span> <span class="nc">QuantileEstimator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An estimator predicting the alpha-quantile of the training targets.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`alpha` must be in (0, 1.0) but was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantile</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">scoreatpercentile</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">quantile</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;quantile&#39;</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">quantile</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span> <span class="nc">MeanEstimator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An estimator predicting the mean of the training targets.&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span> <span class="nc">LogOddsEstimator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An estimator predicting the log odds ratio.&quot;&quot;&quot;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># pre-cond: pos, neg are encoded as 1, 0</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">neg</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">pos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">neg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">neg</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">pos</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;y contains non binary labels.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prior</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pos</span> <span class="o">/</span> <span class="n">neg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;prior&#39;</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prior</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span> <span class="nc">ScaledLogOddsEstimator</span><span class="p">(</span><span class="n">LogOddsEstimator</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Log odds ratio scaled by 0.5 -- for exponential loss. &quot;&quot;&quot;</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="mf">0.5</span>


<span class="k">class</span> <span class="nc">PriorProbabilityEstimator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An estimator predicting the probability of each</span>
<span class="sd">    class in the training data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">class_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priors</span> <span class="o">=</span> <span class="n">class_counts</span> <span class="o">/</span> <span class="n">class_counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;priors&#39;</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">priors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span><span class="p">[:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">priors</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span> <span class="nc">ZeroEstimator</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An estimator that simply predicts zero. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">issubdtype</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="c1"># classification</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># regression</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;n_classes&#39;</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>


<span class="k">class</span> <span class="nc">LossFunction</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="nb">object</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Abstract base class for various loss functions.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    K : int</span>
<span class="sd">        The number of regression trees to be induced;</span>
<span class="sd">        1 for regression and binary classification;</span>
<span class="sd">        ``n_classes`` for multi-class classification.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">is_multi_class</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">n_classes</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Default ``init`` estimator for loss function. &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the loss of prediction ``pred`` and ``y``. &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the negative gradient.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ---------</span>
<span class="sd">        y : np.ndarray, shape=(n,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        y_pred : np.ndarray, shape=(n,):</span>
<span class="sd">            The predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">update_terminal_regions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the terminal regions (=leaves) of the given tree and</span>
<span class="sd">        updates the current predictions of the model. Traverses tree</span>
<span class="sd">        and invokes template method `_update_terminal_region`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tree : tree.Tree</span>
<span class="sd">            The tree object.</span>
<span class="sd">        X : ndarray, shape=(n, m)</span>
<span class="sd">            The data array.</span>
<span class="sd">        y : ndarray, shape=(n,)</span>
<span class="sd">            The target labels.</span>
<span class="sd">        residual : ndarray, shape=(n,)</span>
<span class="sd">            The residuals (usually the negative gradient).</span>
<span class="sd">        y_pred : ndarray, shape=(n,)</span>
<span class="sd">            The predictions.</span>
<span class="sd">        sample_weight : ndarray, shape=(n,)</span>
<span class="sd">            The weight of each sample.</span>
<span class="sd">        sample_mask : ndarray, shape=(n,)</span>
<span class="sd">            The sample mask to be used.</span>
<span class="sd">        learning_rate : float, default=0.1</span>
<span class="sd">            learning rate shrinks the contribution of each tree by</span>
<span class="sd">             ``learning_rate``.</span>
<span class="sd">        k : int, default 0</span>
<span class="sd">            The index of the estimator being updated.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># compute leaf for each sample in ``X``.</span>
        <span class="n">terminal_regions</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># mask all which are not in sample mask.</span>
        <span class="n">masked_terminal_regions</span> <span class="o">=</span> <span class="n">terminal_regions</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">masked_terminal_regions</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># update each leaf (= perform line search)</span>
        <span class="k">for</span> <span class="n">leaf</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">children_left</span> <span class="o">==</span> <span class="n">TREE_LEAF</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_terminal_region</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">masked_terminal_regions</span><span class="p">,</span>
                                         <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span>
                                         <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="c1"># update predictions (both in-bag and out-of-bag)</span>
        <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">learning_rate</span>
                         <span class="o">*</span> <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_regions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method for updating terminal regions (=leaves). &quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">RegressionLossFunction</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">LossFunction</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for regression loss functions. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;``n_classes`` must be 1 for regression but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">n_classes</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LeastSquaresError</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loss function for least squares (LS) estimation.</span>
<span class="sd">    Terminal regions need not to be updated for least squares. &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MeanEstimator</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_terminal_regions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                                <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Least squares does not need to update terminal regions.</span>

<span class="sd">        But it has to update the predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># update predictions</span>
        <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="k">pass</span>


<span class="k">class</span> <span class="nc">LeastAbsoluteError</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loss function for least absolute deviation (LAD) regression. &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">QuantileEstimator</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;1.0 if y - pred &gt; 0.0 else -1.0&quot;&quot;&quot;</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">return</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">pred</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;LAD updates terminal regions to median estimates. &quot;&quot;&quot;</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HuberLossFunction</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Huber loss function for robust regression.</span>

<span class="sd">    M-Regression proposed in Friedman 2001.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HuberLossFunction</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">QuantileEstimator</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">pred</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
        <span class="k">if</span> <span class="n">gamma</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">gamma</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">scoreatpercentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gamma</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

        <span class="n">gamma_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">gamma</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sq_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>
            <span class="n">lin_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">])</span> <span class="o">-</span> <span class="n">gamma</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">sq_loss</span> <span class="o">+</span> <span class="n">lin_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sq_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>
            <span class="n">lin_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">*</span>
                              <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">])</span> <span class="o">-</span> <span class="n">gamma</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">sq_loss</span> <span class="o">+</span> <span class="n">lin_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">pred</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">scoreatpercentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">),</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">gamma_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">gamma</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">residual</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="n">gamma_mask</span><span class="p">]</span>
        <span class="n">residual</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">gamma_mask</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="k">return</span> <span class="n">residual</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">median</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="n">diff_minus_median</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">-</span> <span class="n">median</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">median</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">diff_minus_median</span><span class="p">)</span> <span class="o">*</span>
            <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">diff_minus_median</span><span class="p">),</span> <span class="n">gamma</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">QuantileLossFunction</span><span class="p">(</span><span class="n">RegressionLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loss function for quantile regression.</span>

<span class="sd">    Quantile regression allows to estimate the percentiles</span>
<span class="sd">    of the conditional distribution of the target.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">QuantileLossFunction</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">percentile</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="mf">100.0</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">QuantileEstimator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">pred</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="n">pred</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span>
                    <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">-</span>
                    <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span> <span class="o">*</span> <span class="n">diff</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]))</span> <span class="o">/</span>
                    <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="n">pred</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span> <span class="o">-</span> <span class="p">((</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="o">~</span><span class="n">mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">pred</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">val</span> <span class="o">=</span> <span class="n">_weighted_percentile</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">percentile</span><span class="p">)</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>


<span class="k">class</span> <span class="nc">ClassificationLossFunction</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">LossFunction</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Base class for classification loss functions. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_score_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method to convert scores to probabilities.</span>

<span class="sd">         the does not support probabilities raises AttributeError.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_score_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Template method to convert scores to decisions.</span>

<span class="sd">        Returns int arrays.</span>
<span class="sd">        &quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">BinomialDeviance</span><span class="p">(</span><span class="n">ClassificationLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Binomial deviance loss function for binary classification.</span>

<span class="sd">    Binary classification is a special case; here, we only need to</span>
<span class="sd">    fit one tree instead of ``n_classes`` trees.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0:s}</span><span class="s2"> requires 2 classes.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="c1"># we only need to fit one tree for binary clf.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BinomialDeviance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">LogOddsEstimator</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the deviance (= 2 * negative log-likelihood). &quot;&quot;&quot;</span>
        <span class="c1"># logaddexp(0, v) == log(1.0 + exp(v))</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">*</span> <span class="n">pred</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">*</span> <span class="n">pred</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">pred</span><span class="p">))))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the residual (= negative gradient). &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="n">expit</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make a single Newton-Raphson step.</span>

<span class="sd">        our node estimate is given by:</span>

<span class="sd">            sum(w * (y - prob)) / sum(w * prob * (1 - prob))</span>

<span class="sd">        we take advantage that: y - prob = residual</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">residual</span><span class="p">)</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">residual</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span> <span class="o">+</span> <span class="n">residual</span><span class="p">))</span>

        <span class="c1"># prevents overflow and division by zero</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denominator</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-150</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">def</span> <span class="nf">_score_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">score</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">score</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">proba</span>

    <span class="k">def</span> <span class="nf">_score_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_to_proba</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultinomialDeviance</span><span class="p">(</span><span class="n">ClassificationLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multinomial deviance loss function for multi-class classification.</span>

<span class="sd">    For multi-class classification we need to fit ``n_classes`` trees at</span>
<span class="sd">    each stage.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">is_multi_class</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0:s}</span><span class="s2"> requires more than 2 classes.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultinomialDeviance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">PriorProbabilityEstimator</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># create one-hot label encoding</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="n">Y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="n">k</span>

        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">*</span> <span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
                          <span class="n">logsumexp</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">*</span> <span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
                          <span class="n">logsumexp</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute negative gradient for the ``k``-th class. &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">pred</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">-</span>
                                        <span class="n">logsumexp</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make a single Newton-Raphson step. &quot;&quot;&quot;</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">residual</span><span class="p">)</span>
        <span class="n">numerator</span> <span class="o">*=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span>

        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">residual</span><span class="p">)</span> <span class="o">*</span>
                             <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">y</span> <span class="o">+</span> <span class="n">residual</span><span class="p">))</span>

        <span class="c1"># prevents overflow and division by zero</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denominator</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-150</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">def</span> <span class="nf">_score_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">score</span> <span class="o">-</span> <span class="p">(</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])))</span>

    <span class="k">def</span> <span class="nf">_score_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_to_proba</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ExponentialLoss</span><span class="p">(</span><span class="n">ClassificationLossFunction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Exponential loss function for binary classification.</span>

<span class="sd">    Same loss as AdaBoost.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0:s}</span><span class="s2"> requires 2 classes.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="c1"># we only need to fit one tree for binary clf.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ScaledLogOddsEstimator</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">*</span> <span class="n">pred</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">pred</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">negative_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
        <span class="n">y_</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mf">2.</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_</span> <span class="o">*</span> <span class="n">pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_update_terminal_region</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">terminal_regions</span><span class="p">,</span> <span class="n">leaf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                <span class="n">residual</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">):</span>
        <span class="n">terminal_region</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">terminal_regions</span> <span class="o">==</span> <span class="n">leaf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">terminal_region</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">y_</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">y</span> <span class="o">-</span> <span class="mf">1.</span>

        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y_</span> <span class="o">*</span> <span class="n">pred</span><span class="p">))</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y_</span> <span class="o">*</span> <span class="n">pred</span><span class="p">))</span>

        <span class="c1"># prevents overflow and division by zero</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">denominator</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-150</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="n">leaf</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">def</span> <span class="nf">_score_to_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">score</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">score</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
        <span class="n">proba</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">proba</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">proba</span>

    <span class="k">def</span> <span class="nf">_score_to_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">score</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>


<span class="n">LOSS_FUNCTIONS</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;ls&#39;</span><span class="p">:</span> <span class="n">LeastSquaresError</span><span class="p">,</span>
                  <span class="s1">&#39;lad&#39;</span><span class="p">:</span> <span class="n">LeastAbsoluteError</span><span class="p">,</span>
                  <span class="s1">&#39;huber&#39;</span><span class="p">:</span> <span class="n">HuberLossFunction</span><span class="p">,</span>
                  <span class="s1">&#39;quantile&#39;</span><span class="p">:</span> <span class="n">QuantileLossFunction</span><span class="p">,</span>
                  <span class="s1">&#39;deviance&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>    <span class="c1"># for both, multinomial and binomial</span>
                  <span class="s1">&#39;exponential&#39;</span><span class="p">:</span> <span class="n">ExponentialLoss</span><span class="p">,</span>
                  <span class="p">}</span>


<span class="n">INIT_ESTIMATORS</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;zero&#39;</span><span class="p">:</span> <span class="n">ZeroEstimator</span><span class="p">}</span>


<span class="k">class</span> <span class="nc">VerboseReporter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reports verbose output to stdout.</span>

<span class="sd">    If ``verbose==1`` output is printed once in a while (when iteration mod</span>
<span class="sd">    verbose_mod is zero).; if larger than 1 then output is printed for</span>
<span class="sd">    each update.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">est</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="c1"># header fields and line format str</span>
        <span class="n">header_fields</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Iter&#39;</span><span class="p">,</span> <span class="s1">&#39;Train Loss&#39;</span><span class="p">]</span>
        <span class="n">verbose_fmt</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">{iter:&gt;10d}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{train_score:&gt;16.4f}</span><span class="s1">&#39;</span><span class="p">]</span>
        <span class="c1"># do oob?</span>
        <span class="k">if</span> <span class="n">est</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">header_fields</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;OOB Improve&#39;</span><span class="p">)</span>
            <span class="n">verbose_fmt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{oob_impr:&gt;16.4f}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">header_fields</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;Remaining Time&#39;</span><span class="p">)</span>
        <span class="n">verbose_fmt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{remaining_time:&gt;16s}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># print the header line</span>
        <span class="nb">print</span><span class="p">((</span><span class="s1">&#39;</span><span class="si">%10s</span><span class="s1"> &#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="si">%16s</span><span class="s1"> &#39;</span> <span class="o">*</span>
               <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">header_fields</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">header_fields</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_fmt</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">verbose_fmt</span><span class="p">)</span>
        <span class="c1"># plot verbose info each time i % verbose_mod == 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">begin_at_stage</span> <span class="o">=</span> <span class="n">begin_at_stage</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">est</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update reporter with new iteration. &quot;&quot;&quot;</span>
        <span class="n">do_oob</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span>
        <span class="c1"># we need to take into account if we fit additional estimators.</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_at_stage</span>  <span class="c1"># iteration relative to the start iter</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">oob_impr</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">do_oob</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">remaining_time</span> <span class="o">=</span> <span class="p">((</span><span class="n">est</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span>
                              <span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">remaining_time</span> <span class="o">&gt;</span> <span class="mi">60</span><span class="p">:</span>
                <span class="n">remaining_time</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0:.2f}</span><span class="s1">m&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">remaining_time</span> <span class="o">/</span> <span class="mf">60.0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">remaining_time</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0:.2f}</span><span class="s1">s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">remaining_time</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose_fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">iter</span><span class="o">=</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                                          <span class="n">train_score</span><span class="o">=</span><span class="n">est</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">j</span><span class="p">],</span>
                                          <span class="n">oob_impr</span><span class="o">=</span><span class="n">oob_impr</span><span class="p">,</span>
                                          <span class="n">remaining_time</span><span class="o">=</span><span class="n">remaining_time</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
                <span class="c1"># adjust verbose frequency (powers of 10)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">verbose_mod</span> <span class="o">*=</span> <span class="mi">10</span>


<span class="k">class</span> <span class="nc">BaseGradientBoosting</span><span class="p">(</span><span class="n">six</span><span class="o">.</span><span class="n">with_metaclass</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">,</span> <span class="n">BaseEnsemble</span><span class="p">)):</span>
    <span class="sd">&quot;&quot;&quot;Abstract base class for Gradient Boosting. &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="p">,</span> <span class="n">min_impurity_split</span><span class="p">,</span>
                 <span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="p">,</span> <span class="n">max_features</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">presort</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n_estimators</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">=</span> <span class="n">subsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">=</span> <span class="n">presort</span>

    <span class="k">def</span> <span class="nf">_fit_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                   <span class="n">random_state</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">,</span> <span class="n">X_csc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">X_csr</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit another stage of ``n_classes_`` trees to the boosting model. &quot;&quot;&quot;</span>

        <span class="k">assert</span> <span class="n">sample_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">bool</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span>
        <span class="n">original_y</span> <span class="o">=</span> <span class="n">y</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">is_multi_class</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">original_y</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="n">residual</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">negative_gradient</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span>
                                              <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

            <span class="c1"># induce regression tree on residuals</span>
            <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span>
                <span class="n">criterion</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
                <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">,</span>
                <span class="n">max_depth</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">,</span>
                <span class="n">min_samples_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">,</span>
                <span class="n">min_samples_leaf</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span><span class="p">,</span>
                <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
                <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
                <span class="n">min_impurity_split</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span><span class="p">,</span>
                <span class="n">max_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span>
                <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                <span class="n">presort</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">presort</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="c1"># no inplace multiplication!</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">sample_mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">X_csc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_csc</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                         <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                         <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">)</span>

            <span class="c1"># update tree leaves</span>
            <span class="k">if</span> <span class="n">X_csr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">update_terminal_regions</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="n">X_csr</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                             <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">.</span><span class="n">update_terminal_regions</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">residual</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                             <span class="n">sample_weight</span><span class="p">,</span> <span class="n">sample_mask</span><span class="p">,</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

            <span class="c1"># add tree to ensemble</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tree</span>

        <span class="k">return</span> <span class="n">y_pred</span>

    <span class="k">def</span> <span class="nf">_check_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check validity of parameters and raise ValueError if not valid. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_estimators must be greater than 0 but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;learning_rate must be greater than 0 but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_SUPPORTED_LOSS</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">LOSS_FUNCTIONS</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Loss &#39;</span><span class="si">{0:s}</span><span class="s2">&#39; not supported. &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;deviance&#39;</span><span class="p">:</span>
            <span class="n">loss_class</span> <span class="o">=</span> <span class="p">(</span><span class="n">MultinomialDeviance</span>
                          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span>
                          <span class="k">else</span> <span class="n">BinomialDeviance</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss_class</span> <span class="o">=</span> <span class="n">LOSS_FUNCTIONS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;quantile&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span> <span class="o">=</span> <span class="n">loss_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;subsample must be in (0,1] but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">INIT_ESTIMATORS</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;init=&quot;</span><span class="si">%s</span><span class="s1">&quot; is not supported&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="s1">&#39;fit&#39;</span><span class="p">)</span>
                        <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="s1">&#39;predict&#39;</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;init=</span><span class="si">%r</span><span class="s2"> must be valid BaseEstimator &quot;</span>
                                     <span class="s2">&quot;and support both fit and &quot;</span>
                                     <span class="s2">&quot;predict&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be in (0.0, 1.0) but &quot;</span>
                             <span class="s2">&quot;was </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="c1"># if is_classification</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># is regression</span>
                    <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;log2&quot;</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid value for max_features: </span><span class="si">%r</span><span class="s2">. &quot;</span>
                                 <span class="s2">&quot;Allowed string values are &#39;auto&#39;, &#39;sqrt&#39; &quot;</span>
                                 <span class="s2">&quot;or &#39;log2&#39;.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)):</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># float</span>
            <span class="k">if</span> <span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">&lt;=</span> <span class="mf">1.</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">*</span>
                                       <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_features must be in (0, n_features]&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_features_</span> <span class="o">=</span> <span class="n">max_features</span>

    <span class="k">def</span> <span class="nf">_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize model state and allocate model state data structures. &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">init_estimator</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="n">INIT_ESTIMATORS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">]()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">),</span>
                                    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="c1"># do oob?</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">),</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_clear_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Clear the state of the gradient boosting model. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">object</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;train_score_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;init_&#39;</span><span class="p">):</span>
            <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span>

    <span class="k">def</span> <span class="nf">_resize_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add additional ``n_estimators`` entries to all attributes. &quot;&quot;&quot;</span>
        <span class="c1"># self.n_estimators is the number of additional est to fit</span>
        <span class="n">total_n_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span>
        <span class="k">if</span> <span class="n">total_n_estimators</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;resize with smaller n_estimators </span><span class="si">%d</span><span class="s1"> &lt; </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span>
                             <span class="p">(</span><span class="n">total_n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">total_n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">total_n_estimators</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">)):</span>
            <span class="c1"># if do oob resize arrays or create new if not available</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">total_n_estimators</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">total_n_estimators</span><span class="p">,),</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">,</span> <span class="p">[]))</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">_check_initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check that the estimator is initialized, raising an error if not.&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;estimators_&#39;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;Attribute n_features was deprecated in version 0.19 and &quot;</span>
                <span class="s2">&quot;will be removed in 0.21.&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">n_features</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit the gradient boosting model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = [n_samples, n_features]</span>
<span class="sd">            Training vectors, where n_samples is the number of samples</span>
<span class="sd">            and n_features is the number of features.</span>

<span class="sd">        y : array-like, shape = [n_samples]</span>
<span class="sd">            Target values (integers in classification, real numbers in</span>
<span class="sd">            regression)</span>
<span class="sd">            For classification, labels must correspond to classes.</span>

<span class="sd">        sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        monitor : callable, optional</span>
<span class="sd">            The monitor is called after each iteration with the current</span>
<span class="sd">            iteration, a reference to the estimator and the local variables of</span>
<span class="sd">            ``_fit_stages`` as keyword arguments ``callable(i, self,</span>
<span class="sd">            locals())``. If the callable returns ``True`` the fitting procedure</span>
<span class="sd">            is stopped. The monitor can be used for various things such as</span>
<span class="sd">            computing held-out estimates, early stopping, model introspect, and</span>
<span class="sd">            snapshoting.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if not warmstart - clear the estimator state</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_clear_state</span><span class="p">()</span>

        <span class="c1"># Check input</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="s1">&#39;coo&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>
        <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">warn</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_params</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_initialized</span><span class="p">():</span>
            <span class="c1"># init state</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_state</span><span class="p">()</span>

            <span class="c1"># fit initial model - FIXME make sample_weight optional</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

            <span class="c1"># init predictions</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">begin_at_stage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># add more estimators to fitted model</span>
            <span class="c1"># invariant: warm_start = True</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                                 <span class="s1">&#39;estimators_.shape[0]=</span><span class="si">%d</span><span class="s1"> when &#39;</span>
                                 <span class="s1">&#39;warm_start==True&#39;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="n">begin_at_stage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_resize_state</span><span class="p">()</span>

        <span class="n">X_idx_sorted</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">presort</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">presort</span>
        <span class="c1"># Allow presort to be &#39;auto&#39;, which means True if the dataset is dense,</span>
        <span class="c1"># otherwise it will be False.</span>
        <span class="k">if</span> <span class="n">presort</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span> <span class="ow">and</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">presort</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="n">presort</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span><span class="p">:</span>
            <span class="n">presort</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">presort</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Presorting is not supported for sparse matrices.&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">X_idx_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asfortranarray</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                                 <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># fit the boosting stages</span>
        <span class="n">n_stages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_stages</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                                    <span class="n">begin_at_stage</span><span class="p">,</span> <span class="n">monitor</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">)</span>
        <span class="c1"># change shape of arrays after fit (early-stopping or additional ests)</span>
        <span class="k">if</span> <span class="n">n_stages</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;oob_improvement_&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[:</span><span class="n">n_stages</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span>
                    <span class="n">begin_at_stage</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iteratively fits the stages.</span>

<span class="sd">        For each stage it computes the progress (OOB, train score)</span>
<span class="sd">        and delegates to ``_fit_stage``.</span>
<span class="sd">        Returns the number of stages fit; might differ from ``n_estimators``</span>
<span class="sd">        due to early stopping.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">do_oob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">&lt;</span> <span class="mf">1.0</span>
        <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
        <span class="n">n_inbag</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subsample</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
        <span class="n">loss_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span>

        <span class="c1"># Set min_weight_leaf from min_weight_fraction_leaf</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">!=</span> <span class="mf">0.</span> <span class="ow">and</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">min_weight_leaf</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">*</span>
                               <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_weight_leaf</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">verbose_reporter</span> <span class="o">=</span> <span class="n">VerboseReporter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
            <span class="n">verbose_reporter</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_at_stage</span><span class="p">)</span>

        <span class="n">X_csc</span> <span class="o">=</span> <span class="n">csc_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">X_csr</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># perform boosting iterations</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">begin_at_stage</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">begin_at_stage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">):</span>

            <span class="c1"># subsampling</span>
            <span class="k">if</span> <span class="n">do_oob</span><span class="p">:</span>
                <span class="n">sample_mask</span> <span class="o">=</span> <span class="n">_random_sample_mask</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_inbag</span><span class="p">,</span>
                                                  <span class="n">random_state</span><span class="p">)</span>
                <span class="c1"># OOB score before adding this stage</span>
                <span class="n">old_oob_score</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                      <span class="n">y_pred</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                      <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">])</span>

            <span class="c1"># fit next stage of trees</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit_stage</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                     <span class="n">sample_mask</span><span class="p">,</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">,</span>
                                     <span class="n">X_csc</span><span class="p">,</span> <span class="n">X_csr</span><span class="p">)</span>

            <span class="c1"># track deviance (= loss)</span>
            <span class="k">if</span> <span class="n">do_oob</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">],</span>
                                             <span class="n">y_pred</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">],</span>
                                             <span class="n">sample_weight</span><span class="p">[</span><span class="n">sample_mask</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_improvement_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">old_oob_score</span> <span class="o">-</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                          <span class="n">y_pred</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">],</span>
                                          <span class="n">sample_weight</span><span class="p">[</span><span class="o">~</span><span class="n">sample_mask</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># no need to fancy index w/ no subsampling</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_score_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">verbose_reporter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">monitor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">monitor</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="nb">locals</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">early_stopping</span><span class="p">:</span>
                    <span class="k">break</span>
        <span class="k">return</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_make_estimator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># we don&#39;t need _make_estimator</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check input and compute prediction of ``init``. &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;X.shape[1] should be </span><span class="si">{0:d}</span><span class="s2">, not </span><span class="si">{1:d}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">score</span>

    <span class="k">def</span> <span class="nf">_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># for use in inner loop, not raveling the output in single-class case,</span>
        <span class="c1"># not doing input validation.</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">predict_stages</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">score</span>


    <span class="k">def</span> <span class="nf">_staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each iteration.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : generator of array, shape = [n_samples, k]</span>
<span class="sd">            The decision function of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">            Regression and binary classification are special cases with</span>
<span class="sd">            ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>  <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">predict_stage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">score</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the feature importances (the higher, the more important the</span>
<span class="sd">           feature).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_importances_ : array, shape = [n_features]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>

        <span class="n">total_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">stage</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">stage_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span>
                            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">stage</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
            <span class="n">total_sum</span> <span class="o">+=</span> <span class="n">stage_sum</span>

        <span class="n">importances</span> <span class="o">=</span> <span class="n">total_sum</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">importances</span>

    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="s1">&#39;O&#39;</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="c1"># Default implementation</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>

<span class="sd">        .. versionadded:: 0.17</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">            be converted to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array_like, shape = [n_samples, n_estimators, n_classes]</span>
<span class="sd">            For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">            return the index of the leaf x ends up in each estimator.</span>
<span class="sd">            In the case of binary classification n_classes is 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_initialized</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># n_classes will be equal to 1 in the binary classification or the</span>
        <span class="c1"># regression case.</span>
        <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_estimators</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
                <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
                <span class="n">leaves</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">leaves</span>


<span class="k">class</span> <span class="nc">GradientBoostingClassifier</span><span class="p">(</span><span class="n">BaseGradientBoosting</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient Boosting for classification.</span>

<span class="sd">    GB builds an additive model in a</span>
<span class="sd">    forward stage-wise fashion; it allows for the optimization of</span>
<span class="sd">    arbitrary differentiable loss functions. In each stage ``n_classes_``</span>
<span class="sd">    regression trees are fit on the negative gradient of the</span>
<span class="sd">    binomial or multinomial deviance loss function. Binary classification</span>
<span class="sd">    is a special case where only a single regression tree is induced.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;gradient_boosting&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : {&#39;deviance&#39;, &#39;exponential&#39;}, optional (default=&#39;deviance&#39;)</span>
<span class="sd">        loss function to be optimized. &#39;deviance&#39; refers to</span>
<span class="sd">        deviance (= logistic regression) for classification</span>
<span class="sd">        with probabilistic outputs. For loss &#39;exponential&#39; gradient</span>
<span class="sd">        boosting recovers the AdaBoost algorithm.</span>

<span class="sd">    learning_rate : float, optional (default=0.1)</span>
<span class="sd">        learning rate shrinks the contribution of each tree by `learning_rate`.</span>
<span class="sd">        There is a trade-off between learning_rate and n_estimators.</span>

<span class="sd">    n_estimators : int (default=100)</span>
<span class="sd">        The number of boosting stages to perform. Gradient boosting</span>
<span class="sd">        is fairly robust to over-fitting so a large number usually</span>
<span class="sd">        results in better performance.</span>

<span class="sd">    max_depth : integer, optional (default=3)</span>
<span class="sd">        maximum depth of the individual regression estimators. The maximum</span>
<span class="sd">        depth limits the number of nodes in the tree. Tune this parameter</span>
<span class="sd">        for best performance; the best value depends on the interaction</span>
<span class="sd">        of the input variables.</span>

<span class="sd">    criterion : string, optional (default=&quot;friedman_mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;friedman_mse&quot; for the mean squared error with improvement</span>
<span class="sd">        score by Friedman, &quot;mse&quot; for mean squared error, and &quot;mae&quot; for</span>
<span class="sd">        the mean absolute error. The default value of &quot;friedman_mse&quot; is</span>
<span class="sd">        generally the best as it can provide a better approximation in</span>
<span class="sd">        some cases.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a percentage and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for percentages.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node:</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a percentage and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for percentages.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    subsample : float, optional (default=1.0)</span>
<span class="sd">        The fraction of samples to be used for fitting the individual base</span>
<span class="sd">        learners. If smaller than 1.0 this results in Stochastic Gradient</span>
<span class="sd">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span>
<span class="sd">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=None)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a percentage and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Choosing `max_features &lt; n_features` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    init : BaseEstimator, None, optional (default=None)</span>
<span class="sd">        An estimator object that is used to compute the initial</span>
<span class="sd">        predictions. ``init`` has to provide ``fit`` and ``predict``.</span>
<span class="sd">        If None it uses ``loss.init_estimator``.</span>

<span class="sd">    verbose : int, default: 0</span>
<span class="sd">        Enable verbose output. If 1 then it prints progress and performance</span>
<span class="sd">        once in a while (the more trees the lower the frequency). If greater</span>
<span class="sd">        than 1 then it prints progress and performance for every tree.</span>

<span class="sd">    warm_start : bool, default: False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just erase the</span>
<span class="sd">        previous solution.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    presort : bool or &#39;auto&#39;, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Whether to presort the data to speed up the finding of best splits in</span>
<span class="sd">        fitting. Auto mode by default will use presorting on dense data and</span>
<span class="sd">        default to normal sorting on sparse data. Setting presort to true on</span>
<span class="sd">        sparse data will raise an error.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *presort* parameter.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_importances_ : array, shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    oob_improvement_ : array, shape = [n_estimators]</span>
<span class="sd">        The improvement in loss (= deviance) on the out-of-bag samples</span>
<span class="sd">        relative to the previous iteration.</span>
<span class="sd">        ``oob_improvement_[0]`` is the improvement in</span>
<span class="sd">        loss of the first stage over the ``init`` estimator.</span>

<span class="sd">    train_score_ : array, shape = [n_estimators]</span>
<span class="sd">        The i-th score ``train_score_[i]`` is the deviance (= loss) of the</span>
<span class="sd">        model at iteration ``i`` on the in-bag sample.</span>
<span class="sd">        If ``subsample == 1`` this is the deviance on the training data.</span>

<span class="sd">    loss_ : LossFunction</span>
<span class="sd">        The concrete ``LossFunction`` object.</span>

<span class="sd">    init : BaseEstimator</span>
<span class="sd">        The estimator that provides the initial predictions.</span>
<span class="sd">        Set via the ``init`` argument or ``loss.init_estimator``.</span>

<span class="sd">    estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, ``loss_.K``]</span>
<span class="sd">        The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary</span>
<span class="sd">        classification, otherwise n_classes.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.tree.DecisionTreeClassifier, RandomForestClassifier</span>
<span class="sd">    AdaBoostClassifier</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>

<span class="sd">    J. Friedman, Stochastic Gradient Boosting, 1999</span>

<span class="sd">    T. Hastie, R. Tibshirani and J. Friedman.</span>
<span class="sd">    Elements of Statistical Learning Ed. 2, Springer, 2009.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_SUPPORTED_LOSS</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span> <span class="s1">&#39;exponential&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;deviance&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">subsample</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">presort</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="n">presort</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>

<div class="viewcode-block" id="GradientBoostingClassifier.decision_function"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.decision_function">[docs]</a>    <span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the decision function of ``X``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : array, shape = [n_samples, n_classes] or [n_samples]</span>
<span class="sd">            The decision function of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">            Regression and binary classification produce an array of shape</span>
<span class="sd">            [n_samples].</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>  <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">score</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">score</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">score</span></div>

<div class="viewcode-block" id="GradientBoostingClassifier.staged_decision_function"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.staged_decision_function">[docs]</a>    <span class="k">def</span> <span class="nf">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each iteration.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        score : generator of array, shape = [n_samples, k]</span>
<span class="sd">            The decision function of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">            Regression and binary classification are special cases with</span>
<span class="sd">            ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">dec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># no yield from in Python2.X</span>
            <span class="k">yield</span> <span class="n">dec</span></div>

<div class="viewcode-block" id="GradientBoostingClassifier.predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples]</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">decisions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_score_to_decision</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">decisions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientBoostingClassifier.staged_predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.staged_predict">[docs]</a>    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class at each stage for X.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of array of shape = [n_samples]</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">decisions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_score_to_decision</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">decisions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientBoostingClassifier.predict_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AttributeError</span>
<span class="sd">            If the ``loss`` does not support probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : array of shape = [n_samples]</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_score_to_proba</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
            <span class="k">raise</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;loss=</span><span class="si">%r</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientBoostingClassifier.predict_log_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.predict_log_proba">[docs]</a>    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        AttributeError</span>
<span class="sd">            If the ``loss`` does not support probabilities.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : array of shape = [n_samples]</span>
<span class="sd">            The class log-probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute `classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span></div>

<div class="viewcode-block" id="GradientBoostingClassifier.staged_predict_proba"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingclassifier.html#ibex.sklearn.ensemble.GradientBoostingClassifier.staged_predict_proba">[docs]</a>    <span class="k">def</span> <span class="nf">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict class probabilities at each stage for X.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of array of shape = [n_samples]</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">score</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
                <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_score_to_proba</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
            <span class="k">raise</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s1">&#39;loss=</span><span class="si">%r</span><span class="s1"> does not support predict_proba&#39;</span> <span class="o">%</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">GradientBoostingRegressor</span><span class="p">(</span><span class="n">BaseGradientBoosting</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gradient Boosting for regression.</span>

<span class="sd">    GB builds an additive model in a forward stage-wise fashion;</span>
<span class="sd">    it allows for the optimization of arbitrary differentiable loss functions.</span>
<span class="sd">    In each stage a regression tree is fit on the negative gradient of the</span>
<span class="sd">    given loss function.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;gradient_boosting&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : {&#39;ls&#39;, &#39;lad&#39;, &#39;huber&#39;, &#39;quantile&#39;}, optional (default=&#39;ls&#39;)</span>
<span class="sd">        loss function to be optimized. &#39;ls&#39; refers to least squares</span>
<span class="sd">        regression. &#39;lad&#39; (least absolute deviation) is a highly robust</span>
<span class="sd">        loss function solely based on order information of the input</span>
<span class="sd">        variables. &#39;huber&#39; is a combination of the two. &#39;quantile&#39;</span>
<span class="sd">        allows quantile regression (use `alpha` to specify the quantile).</span>

<span class="sd">    learning_rate : float, optional (default=0.1)</span>
<span class="sd">        learning rate shrinks the contribution of each tree by `learning_rate`.</span>
<span class="sd">        There is a trade-off between learning_rate and n_estimators.</span>

<span class="sd">    n_estimators : int (default=100)</span>
<span class="sd">        The number of boosting stages to perform. Gradient boosting</span>
<span class="sd">        is fairly robust to over-fitting so a large number usually</span>
<span class="sd">        results in better performance.</span>

<span class="sd">    max_depth : integer, optional (default=3)</span>
<span class="sd">        maximum depth of the individual regression estimators. The maximum</span>
<span class="sd">        depth limits the number of nodes in the tree. Tune this parameter</span>
<span class="sd">        for best performance; the best value depends on the interaction</span>
<span class="sd">        of the input variables.</span>

<span class="sd">    criterion : string, optional (default=&quot;friedman_mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;friedman_mse&quot; for the mean squared error with improvement</span>
<span class="sd">        score by Friedman, &quot;mse&quot; for mean squared error, and &quot;mae&quot; for</span>
<span class="sd">        the mean absolute error. The default value of &quot;friedman_mse&quot; is</span>
<span class="sd">        generally the best as it can provide a better approximation in</span>
<span class="sd">        some cases.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a percentage and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for percentages.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node:</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a percentage and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for percentages.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    subsample : float, optional (default=1.0)</span>
<span class="sd">        The fraction of samples to be used for fitting the individual base</span>
<span class="sd">        learners. If smaller than 1.0 this results in Stochastic Gradient</span>
<span class="sd">        Boosting. `subsample` interacts with the parameter `n_estimators`.</span>
<span class="sd">        Choosing `subsample &lt; 1.0` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=None)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a percentage and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Choosing `max_features &lt; n_features` leads to a reduction of variance</span>
<span class="sd">        and an increase in bias.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_split : float,</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19 and will be removed in 0.21.</span>
<span class="sd">           Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    alpha : float (default=0.9)</span>
<span class="sd">        The alpha-quantile of the huber loss function and the quantile</span>
<span class="sd">        loss function. Only if ``loss=&#39;huber&#39;`` or ``loss=&#39;quantile&#39;``.</span>

<span class="sd">    init : BaseEstimator, None, optional (default=None)</span>
<span class="sd">        An estimator object that is used to compute the initial</span>
<span class="sd">        predictions. ``init`` has to provide ``fit`` and ``predict``.</span>
<span class="sd">        If None it uses ``loss.init_estimator``.</span>

<span class="sd">    verbose : int, default: 0</span>
<span class="sd">        Enable verbose output. If 1 then it prints progress and performance</span>
<span class="sd">        once in a while (the more trees the lower the frequency). If greater</span>
<span class="sd">        than 1 then it prints progress and performance for every tree.</span>

<span class="sd">    warm_start : bool, default: False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just erase the</span>
<span class="sd">        previous solution.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    presort : bool or &#39;auto&#39;, optional (default=&#39;auto&#39;)</span>
<span class="sd">        Whether to presort the data to speed up the finding of best splits in</span>
<span class="sd">        fitting. Auto mode by default will use presorting on dense data and</span>
<span class="sd">        default to normal sorting on sparse data. Setting presort to true on</span>
<span class="sd">        sparse data will raise an error.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           optional parameter *presort*.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    feature_importances_ : array, shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    oob_improvement_ : array, shape = [n_estimators]</span>
<span class="sd">        The improvement in loss (= deviance) on the out-of-bag samples</span>
<span class="sd">        relative to the previous iteration.</span>
<span class="sd">        ``oob_improvement_[0]`` is the improvement in</span>
<span class="sd">        loss of the first stage over the ``init`` estimator.</span>

<span class="sd">    train_score_ : array, shape = [n_estimators]</span>
<span class="sd">        The i-th score ``train_score_[i]`` is the deviance (= loss) of the</span>
<span class="sd">        model at iteration ``i`` on the in-bag sample.</span>
<span class="sd">        If ``subsample == 1`` this is the deviance on the training data.</span>

<span class="sd">    loss_ : LossFunction</span>
<span class="sd">        The concrete ``LossFunction`` object.</span>

<span class="sd">    init : BaseEstimator</span>
<span class="sd">        The estimator that provides the initial predictions.</span>
<span class="sd">        Set via the ``init`` argument or ``loss.init_estimator``.</span>

<span class="sd">    estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data and</span>
<span class="sd">    ``max_features=n_features``, if the improvement of the criterion is</span>
<span class="sd">    identical for several splits enumerated during the search of the best</span>
<span class="sd">    split. To obtain a deterministic behaviour during fitting,</span>
<span class="sd">    ``random_state`` has to be fixed.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    DecisionTreeRegressor, RandomForestRegressor</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    J. Friedman, Greedy Function Approximation: A Gradient Boosting</span>
<span class="sd">    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</span>

<span class="sd">    J. Friedman, Stochastic Gradient Boosting, 1999</span>

<span class="sd">    T. Hastie, R. Tibshirani and J. Friedman.</span>
<span class="sd">    Elements of Statistical Learning Ed. 2, Springer, 2009.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_SUPPORTED_LOSS</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ls&#39;</span><span class="p">,</span> <span class="s1">&#39;lad&#39;</span><span class="p">,</span> <span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;quantile&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ls&#39;</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">subsample</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;friedman_mse&#39;</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">presort</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="n">subsample</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
            <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="n">min_impurity_decrease</span><span class="p">,</span>
            <span class="n">min_impurity_split</span><span class="o">=</span><span class="n">min_impurity_split</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="n">presort</span><span class="p">)</span>

<div class="viewcode-block" id="GradientBoostingRegressor.predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingregressor.html#ibex.sklearn.ensemble.GradientBoostingRegressor.predict">[docs]</a>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : array of shape = [n_samples]</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>  <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>

<div class="viewcode-block" id="GradientBoostingRegressor.staged_predict"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingregressor.html#ibex.sklearn.ensemble.GradientBoostingRegressor.staged_predict">[docs]</a>    <span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict regression target at each stage for X.</span>

<span class="sd">        This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">        after each stage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, it will be converted to</span>
<span class="sd">            ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">            to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : generator of array of shape = [n_samples]</span>
<span class="sd">            The predicted value of the input samples.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">y</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>

<div class="viewcode-block" id="GradientBoostingRegressor.apply"><a class="viewcode-back" href="../../../api_ibex_sklearn_ensemble_gradientboostingregressor.html#ibex.sklearn.ensemble.GradientBoostingRegressor.apply">[docs]</a>    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>

<span class="sd">        .. versionadded:: 0.17</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">            be converted to a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : array_like, shape = [n_samples, n_estimators]</span>
<span class="sd">            For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">            return the index of the leaf x ends up in each estimator.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">leaves</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">GradientBoostingRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">leaves</span> <span class="o">=</span> <span class="n">leaves</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">leaves</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../index.html">
              <img class="logo" src="../../../_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="../../../index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>