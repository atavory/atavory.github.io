
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>ElasticNet &#8212; ibex latest documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     'latest',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="_static/logo.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="elasticnet">
<h1><code class="docutils literal"><span class="pre">ElasticNet</span></code><a class="headerlink" href="#elasticnet" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="ibex.sklearn.linear_model.ElasticNet">
<em class="property">class </em><code class="descclassname">ibex.sklearn.linear_model.</code><code class="descname">ElasticNet</code><span class="sig-paren">(</span><em>alpha=1.0</em>, <em>l1_ratio=0.5</em>, <em>fit_intercept=True</em>, <em>normalize=False</em>, <em>precompute=False</em>, <em>max_iter=1000</em>, <em>copy_X=True</em>, <em>tol=0.0001</em>, <em>warm_start=False</em>, <em>positive=False</em>, <em>random_state=None</em>, <em>selection='cyclic'</em><span class="sig-paren">)</span><a class="headerlink" href="#ibex.sklearn.linear_model.ElasticNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">sklearn.linear_model.coordinate_descent.ElasticNet</span></code>, <code class="xref py py-class docutils literal"><span class="pre">ibex._base.FrameMixin</span></code></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The documentation following is of the class wrapped by this class. There
are some changes, in particular:</p>
<blockquote class="last">
<div><ul class="simple">
<li>A parameter <code class="docutils literal"><span class="pre">X</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></code></a>.</li>
<li>A parameter <code class="docutils literal"><span class="pre">y</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html#pandas.Series" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.Series</span></code></a>.</li>
</ul>
</div></blockquote>
</div>
<p>Linear regression with combined L1 and L2 priors as regularizer.</p>
<blockquote>
<div><p>Minimizes the objective function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2</span><span class="n">_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">L2</span>
</pre></div>
</div>
<p>where:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package while
alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio
= 1 is the lasso penalty. Currently, l1_ratio &lt;= 0.01 is not reliable,
unless you supply your own sequence of alpha.</p>
<p>Read more in the <a class="reference external" href="http://scikit-learn.org/stable/modules/linear_model.html#elastic-net" title="(in scikit-learn v0.19.1)"><span class="xref std std-ref">User Guide</span></a>.</p>
<dl class="docutils">
<dt>alpha <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>Constant that multiplies the penalty terms. Defaults to 1.0.
See the notes for the exact mathematical meaning of this
parameter.``alpha = 0`` is equivalent to an ordinary least square,
solved by the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object. For numerical
reasons, using <code class="docutils literal"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the <code class="docutils literal"><span class="pre">Lasso</span></code> object is not advised.
Given this, you should use the <code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code> object.</dd>
<dt>l1_ratio <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>The ElasticNet mixing parameter, with <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;=</span> <span class="pre">1</span></code>. For
<code class="docutils literal"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code> the penalty is an L2 penalty. <code class="docutils literal"><span class="pre">For</span> <span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it
is an L1 penalty.  For <code class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a
combination of L1 and L2.</dd>
<dt>fit_intercept <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the intercept should be estimated or not. If <code class="docutils literal"><span class="pre">False</span></code>, the
data is assumed to be already centered.</dd>
<dt>normalize <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default False</span></dt>
<dd>This parameter is ignored when <code class="docutils literal"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="(in scikit-learn v0.19.1)"><code class="xref py py-class docutils literal"><span class="pre">sklearn.preprocessing.StandardScaler</span></code></a> before calling <code class="docutils literal"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal"><span class="pre">normalize=False</span></code>.</dd>
<dt>precompute <span class="classifier-delimiter">:</span> <span class="classifier">True | False | array-like</span></dt>
<dd>Whether to use a precomputed Gram matrix to speed up
calculations. The Gram matrix can also be passed as argument.
For sparse input this option is always <code class="docutils literal"><span class="pre">True</span></code> to preserve sparsity.</dd>
<dt>max_iter <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The maximum number of iterations</dd>
<dt>copy_X <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional, default True</span></dt>
<dd>If <code class="docutils literal"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</dd>
<dt>tol <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
<dd>The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal"><span class="pre">tol</span></code>.</dd>
<dt>warm_start <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.</dd>
<dt>positive <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional</span></dt>
<dd>When set to <code class="docutils literal"><span class="pre">True</span></code>, forces the coefficients to be positive.</dd>
<dt>random_state <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default None</span></dt>
<dd>The seed of the pseudo random number generator that selects a random
feature to update.  If int, random_state is the seed used by the random
number generator; If RandomState instance, random_state is the random
number generator; If None, the random number generator is the
RandomState instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">selection</span></code> ==
‘random’.</dd>
<dt>selection <span class="classifier-delimiter">:</span> <span class="classifier">str, default ‘cyclic’</span></dt>
<dd>If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</dd>
</dl>
<dl class="docutils">
<dt><a href="#id1"><span class="problematic" id="id2">coef_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape (n_features,) | (n_targets, n_features)</span></dt>
<dd>parameter vector (w in the cost function formula)</dd>
<dt><a href="#id3"><span class="problematic" id="id4">sparse_coef_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features)</span></dt>
<dd><code class="docutils literal"><span class="pre">sparse_coef_</span></code> is a readonly property derived from <code class="docutils literal"><span class="pre">coef_</span></code></dd>
<dt><a href="#id5"><span class="problematic" id="id6">intercept_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">float | array, shape (n_targets,)</span></dt>
<dd>independent term in decision function.</dd>
<dt><a href="#id7"><span class="problematic" id="id8">n_iter_</span></a> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape (n_targets,)</span></dt>
<dd>number of iterations run by the coordinate descent solver to reach
the specified tolerance.</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">ElasticNet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_regression</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,</span>
<span class="go">      max_iter=1000, normalize=False, positive=False, precompute=False,</span>
<span class="go">      random_state=0, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> 
<span class="go">[ 18.83816048  64.55968825]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span> 
<span class="go">1.45126075617</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span> 
<span class="go">[ 1.45126076]</span>
</pre></div>
</div>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>SGDRegressor: implements elastic net regression with incremental training.
SGDClassifier: implements logistic regression with elastic net penalty</p>
<blockquote>
<div>(<code class="docutils literal"><span class="pre">SGDClassifier(loss=&quot;log&quot;,</span> <span class="pre">penalty=&quot;elasticnet&quot;)</span></code>).</div></blockquote>
</div></blockquote>
<dl class="method">
<dt id="ibex.sklearn.linear_model.ElasticNet.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>check_input=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/sklearn/linear_model/coordinate_descent.html#ElasticNet.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#ibex.sklearn.linear_model.ElasticNet.fit" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The documentation following is of the class wrapped by this class. There
are some changes, in particular:</p>
<blockquote class="last">
<div><ul class="simple">
<li>A parameter <code class="docutils literal"><span class="pre">X</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></code></a>.</li>
<li>A parameter <code class="docutils literal"><span class="pre">y</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html#pandas.Series" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.Series</span></code></a>.</li>
</ul>
</div></blockquote>
</div>
<p>Fit model with coordinate descent.</p>
<blockquote>
<div><dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or scipy.sparse matrix, (n_samples, n_features)</span></dt>
<dd>Data</dd>
<dt>y <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape (n_samples,) or (n_samples, n_targets)</span></dt>
<dd>Target. Will be cast to X’s dtype if necessary</dd>
<dt>check_input <span class="classifier-delimiter">:</span> <span class="classifier">boolean, (default=True)</span></dt>
<dd>Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</dd>
</dl>
<p>Coordinate descent is an algorithm that considers each column of
data at a time hence it will automatically convert the X input
as a Fortran-contiguous numpy array if necessary.</p>
<p>To avoid memory re-allocation it is advised to allocate the
initial data in memory directly using that format.</p>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="ibex.sklearn.linear_model.ElasticNet.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#ibex.sklearn.linear_model.ElasticNet.predict" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The documentation following is of the class wrapped by this class. There
are some changes, in particular:</p>
<blockquote class="last">
<div><ul class="simple">
<li>A parameter <code class="docutils literal"><span class="pre">X</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></code></a>.</li>
<li>A parameter <code class="docutils literal"><span class="pre">y</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html#pandas.Series" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.Series</span></code></a>.</li>
</ul>
</div></blockquote>
</div>
<p>Predict using the linear model</p>
<blockquote>
<div><dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">{array-like, sparse matrix}, shape = (n_samples, n_features)</span></dt>
<dd>Samples.</dd>
</dl>
<dl class="docutils">
<dt>C <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = (n_samples,)</span></dt>
<dd>Returns predicted values.</dd>
</dl>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="ibex.sklearn.linear_model.ElasticNet.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#ibex.sklearn.linear_model.ElasticNet.score" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The documentation following is of the class wrapped by this class. There
are some changes, in particular:</p>
<blockquote class="last">
<div><ul class="simple">
<li>A parameter <code class="docutils literal"><span class="pre">X</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html#pandas.DataFrame" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.DataFrame</span></code></a>.</li>
<li>A parameter <code class="docutils literal"><span class="pre">y</span></code> denotes a <a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html#pandas.Series" title="(in pandas v0.21.0)"><code class="xref py py-class docutils literal"><span class="pre">pandas.Series</span></code></a>.</li>
</ul>
</div></blockquote>
</div>
<p>Returns the coefficient of determination R^2 of the prediction.</p>
<blockquote>
<div><p>The coefficient R^2 is defined as (1 - u/v), where u is the residual
sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
sum of squares ((y_true - y_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<dl class="docutils">
<dt>X <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_samples, n_features)</span></dt>
<dd>Test samples.</dd>
<dt>y <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = (n_samples) or (n_samples, n_outputs)</span></dt>
<dd>True values for X.</dd>
<dt>sample_weight <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_samples], optional</span></dt>
<dd>Sample weights.</dd>
</dl>
<dl class="docutils">
<dt>score <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
<dd>R^2 of self.predict(X) wrt. y.</dd>
</dl>
</div></blockquote>
</dd></dl>

</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/logo.jpeg" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Ibex</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="frame_adapter.html">Adapting Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="input_verification_and_output_processing.html">Verification and Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="function_transformer.html">Transforming</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipelines.html">Pipelining</a></li>
<li class="toctree-l1"><a class="reference internal" href="feature_union.html">Uniting Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="sklearn.html"><code class="docutils literal"><span class="pre">sklearn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorflow.html"><code class="docutils literal"><span class="pre">tensorflow</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="xgboost.html"><code class="docutils literal"><span class="pre">xgboost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="extending.html">Extending</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/api_ibex_sklearn_linear_model_elasticnet.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Ami Tavory, Shahar Azulay, Tali Raveh-Sadka.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/api_ibex_sklearn_linear_model_elasticnet.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/atavory/ibex" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>