<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Principal Component Cardinality via Normalized Maximum Likelihood</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/impressConsole.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="preso.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body class="impress-not-supported"><div id="impress" data-transition-duration="800"><div class="step step-level-1" step="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="0" data-y="0" data-z="0"><h1 id="minimum-description-length-br-rarr-br-principal-components-cardinality">Minimum Description Length <br/> &rarr; <br/> Principal Components' Cardinality</h1><img src="logo.png" height="200px" class="center"></img><h2 id="ami-tavory-br-facebook-research-core-data-science">Ami Tavory <br/> Facebook Research, Core Data Science</h2><div class="notes"><p>Presenter console active</p></div></div><div class="step step-level-1" step="1" data-x="-20" data-y="-50" data-scale="0.01" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0"><h1 id="outline">Outline</h1><ul><li><span class="underline">Introduction</span></li><li>MDL</li><li>MDL Reduction for PCA</li><li>Results</li><li>Conclusions</li></ul></div><div class="step step-level-1" step="2" data-x="-20" data-y="-50" data-scale="0.0001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0"><h1 id="svd-br-singular-value-decomposition">SVD <br/> (Singular Value Decomposition)</h1><img src="svd.png"></img><ul><li><span class="math ">\(X = X_{n \times m}\)</span></li><li><span class="math ">\(U\)</span>, V unitary</li><li><span class="math ">\(\Sigma = \mathrm{diag}\left(\sigma_1, ..., \sigma_n\right)\)</span> eigenvalue matrix</li></ul></div><div class="step step-level-1" step="3" data-x="-20" data-y="-49.85" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="pca-br-principal-components-analysis-approximation">PCA <br/> (Principal Components Analysis) Approximation</h1><img src="pca_orig_1.png"></img></div><div class="step step-level-1" step="4" data-x="-20" data-y="-49.85" data-rotate-y="90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="pca_orig_2.png"></img></div><div class="step step-level-1" step="5" data-y="-49.75" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-x="-20" data-z="0"><h1 id="focus-only-on-dimensions">Focus Only on Dimensions</h1><img src="pca_reduced_1.png"></img></div><div class="step step-level-1" step="6" data-x="-20.0" data-y="-49.75" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="pca_reduced_2.png"></img></div><div class="step step-level-1" step="7" data-x="-20.0" data-y="-49.65" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="which-k-should-we-choose">Which <span class="math ">\(k\)</span> Should We Choose?</h1><img src="pca_reduced_how_much.png"></img></div><div class="step step-level-1" step="8" data-x="-20.0" data-y="-49.55" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="minimize-reconstruction-error">Minimize Reconstruction Error?</h1></div><div class="step step-level-1" step="9" data-x="-20.1" data-y="-49.5" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><p class="substep">(Eckart-Young-Mirsky) For any <span class="math ">\(k\)</span></p><div class="math-block substep">$$\begin{align}\min_{W, V} \left| X - W_k V_k^T \right|_2^2  = \sum_{i = k + 1}^m\left[ \sigma_i^2\right].\end{align}$$</div></div><div class="step step-level-1" step="10" data-x="-20.200000000000003" data-y="-49.45" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="explained_var_ratio.png"></img></div><div class="step step-level-1" step="11" data-x="-20.300000000000004" data-y="-49.400000000000006" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="over_the_top.png"></img></div><div class="step step-level-1" step="12" data-x="-20.300000000000004" data-y="-49.300000000000004" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="find-knee-elbow">Find Knee/Elbow?</h1><img src="explained_var_ratio_kneed.png"></img></div><div class="step step-level-1" step="13" data-x="-20.300000000000004" data-y="-49.2" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="utilize-eigenvalue-properties">Utilize Eigenvalue Properties?</h1><img src="explained_var_ratio_kaiser.png"></img></div><div class="step step-level-1" step="14" data-x="-20.370000000000005" data-y="-49.27" data-z="-0.07" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="drawbacks">Drawbacks</h1><ul><li>Not very well established theoretically</li><li>Known to be problematic in some cases</li></ul></div><div class="step step-level-1" step="15" data-x="-20.370000000000005" data-y="-49.27" data-rotate-y="0" data-scale="100.0001" data-rotate-x="0" data-rotate-z="0" data-z="-0.14"><h1 id="id1">Outline</h1><ul><li>Introduction</li><li><span class="underline">MDL</span></li><li>MDL Reduction for PCA</li><li>Results</li><li>Conclusions</li></ul></div><div class="step step-level-1" step="16" data-x="-20.670000000000005" data-y="-48.970000000000006" data-scale="0.0001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="-0.21000000000000002"><h1 id="minimum-description-length-principle">Minimum Description Length Principle</h1><img src="Occam.png" height="200px" class="center"></img><p>The best hypothesis for a given set of data, is the one that leads to the best compression of the data (Rissanen).</p></div><div class="step step-level-1" step="17" data-x="-20.670000000000005" data-y="-48.82000000000001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="0.0001" data-z="-0.28"><h1 id="application-to-pca-cardinality">Application to PCA Cardinality</h1><img src="sender_receiver.png"></img></div><div class="step step-level-1" step="18" data-x="-20.640000000000004" data-y="-48.77000000000001" data-z="-0.23000000000000004" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><img src="pca_delta.png"></img></div><div class="step step-level-1" step="19" data-x="-20.640000000000004" data-y="-48.74000000000001" data-z="-0.18000000000000005" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><img src="sender_receiver_improved.png"></img></div><div class="step step-level-1" step="20" data-x="-20.640000000000004" data-y="-48.44000000000001" data-z="-0.18000000000000005" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="inherent-tradeoff">Inherent Tradeoff</h1><img src="pca_reduced_1_comm.png"></img></div><div class="step step-level-1" step="21" data-x="-20.640000000000004" data-y="-48.44000000000001" data-z="-0.18000000000000005" data-rotate-y="90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><img src="pca_reduced_2_comm.png"></img></div><div class="step step-level-1" step="22" data-x="-20.640000000000004" data-y="-47.44000000000001" data-z="-0.18000000000000005" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="how-to-find-the-description-length">How to Find the Description Length?</h1><ul><li>Use Information Theory (Shannon) and Complexity Theory (Kolmogorov).</li><li>If we can impose a density <span class="math ">\(f(X)\)</span> on an R.V. <span class="math ">\(X\)</span>, then <span class="math ">\(-\log\left(f(X)\right)\)</span> is (effectively) the answer.</li><li>Transformed escription length problem &rarr; distribution problem.</li></ul></div><div class="step step-level-1" step="23" data-x="-20.640000000000004" data-y="-47.41000000000001" data-z="-0.13000000000000006" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><ul><li>But how should we choose a distribution for a model?</li></ul></div><div class="step step-level-1" step="24" data-x="-20.640000000000004" data-y="-47.26000000000001" data-z="-0.13000000000000006" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="normalized-maximum-likelihood">Normalized Maximum Likelihood</h1><ul><li>Let <span class="math ">\(X\)</span> be distributed according to a model parameterized by <span class="math ">\(\Phi\)</span>.</li><li>Let <span class="math ">\(\hat{\Phi}\left(X\right)\)</span> be the ML (maximum likelihood) estimate of <span class="math ">\(\Phi\)</span> for <span class="math ">\(X\)</span>.</li></ul></div><div class="step step-level-1" step="25" data-x="-20.640000000000004" data-y="-47.210000000000015" data-z="-0.08000000000000006" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><ul><li>The NML (normalized maximum likelihood) distribution is<div class="math-block ">$$\begin{align}{f\left( X \;;\; \hat{\Phi}\left(X\right)\right) \over \int f\left( Y \;;\; \hat{\Phi}\left(Y\right)\right) dY }\end{align}$$</div></li><li>This is the "modern form" of MDL (Shtarkov, Rissanen)<ul><li>Prequential (Dawid) optimality properties</li></ul></li></ul></div><div class="step step-level-1" step="26" data-x="-20.640000000000004" data-y="-47.158000000000015" data-z="-0.030000000000000054" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><p>Inutitively, <span class="math ">\(f\left( X \;;\; \hat{\Phi}\left(X\right)\right)\)</span> an immediate choice,
but self-referential, cannot be used for transmission</p><blockquote><img src="self_referential.png"></img></blockquote></div><div class="step step-level-1" step="27" data-x="-20.640000000000004" data-y="-47.106000000000016" data-z="0.01999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><div class="math-block ">$$\begin{align}{f\left( X \;;\; \hat{\Phi}\left(X\right)\right) \over \int f\left( Y \;;\; \hat{\Phi}\left(Y\right)\right) dY }\end{align}$$</div><p>also has a pleasing bias-variance tradeoff.</p></div><div class="step step-level-1" step="28" data-x="-20.640000000000004" data-y="-46.95600000000002" data-z="0.01999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="pca-as-a-generative-model">PCA as a Generative Model</h1><blockquote><img src="plate1.png" height="400px" class="center"></img></blockquote><p><span class="math ">\(\Phi \equiv k, \Gamma\)</span></p></div><div class="step step-level-1" step="29" data-x="-40.64" data-y="-96.95600000000002" data-scale="0.01" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0.01999999999999995"><h1 id="id2">Outline</h1><ul><li>Introduction</li><li>MDL</li><li><span class="underline">MDL Reduction for PCA</span></li><li>Results</li><li>Conclusions</li></ul></div></div><div id="hovercraft-help"><table><tr><th>Space</th><td>Forward</td></tr><tr><th>Right, Down, Page Down</th><td>Next slide</td></tr><tr><th>Left, Up, Page Up</th><td>Previous slide</td></tr><tr><th>G</th><td>Go to slide number</td></tr><tr><th>P</th><td>Open presenter console</td></tr><tr><th>H</th><td>Toggle this help</td></tr></table></div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/impressConsole.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>