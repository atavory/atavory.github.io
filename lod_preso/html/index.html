<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Principal Component Cardinality via Normalized Maximum Likelihood</title><meta charset="UTF-8"></meta><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/impressConsole.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="preso.css" media="screen,projection"></link><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        TeX : { extensions : ['color.js'] }
      });
    </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body class="impress-not-supported"><div id="impress" data-transition-duration="800"><div class="step step-level-1" step="0" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="1" data-x="0" data-y="0" data-z="0"><h1 id="minimum-description-length-br-rarr-br-principal-components-cardinality">Minimum Description Length <br/> &rarr; <br/> Principal Components' Cardinality</h1><img src="logo.png" height="200px" class="center"></img><h2 id="ami-tavory-br-facebook-research-core-data-science">Ami Tavory <br/> Facebook Research, Core Data Science</h2><div class="notes"><p>Presenter console active</p></div></div><div class="step step-level-1" step="1" data-x="-20" data-y="-50" data-scale="0.01" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0"><h1 id="outline">Outline</h1><ul><li><span class="underline">Introduction</span></li><li>MDL</li><li>MDL Reduction for PCA</li><li>Results and Conclusions</li></ul></div><div class="step step-level-1" step="2" data-x="-20" data-y="-50" data-scale="0.0001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0"><h1 id="svd-br-singular-value-decomposition">SVD <br/> (Singular Value Decomposition)</h1><img src="svd.png"></img><ul><li><span class="math ">\(X = X_{n \times m}\)</span></li><li><span class="math ">\(U\)</span>, V unitary</li><li><span class="math ">\(\Sigma = \mathrm{diag}\left(\sigma_1, ..., \sigma_n\right)\)</span> eigenvalue matrix</li></ul></div><div class="step step-level-1" step="3" data-x="-20" data-y="-49.85" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="pca-br-principal-components-analysis-approximation">PCA <br/> (Principal Components Analysis) Approximation</h1><img src="pca_orig_1.png"></img></div><div class="step step-level-1" step="4" data-y="-49.75" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-x="-20" data-z="0"><h1 id="focus-only-on-dimensions">Focus Only on Dimensions</h1><img src="pca_reduced_1.png"></img></div><div class="step step-level-1" step="5" data-x="-20.0" data-y="-49.75" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="pca_reduced_2.png"></img></div><div class="step step-level-1" step="6" data-x="-20.0" data-y="-49.65" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="which-k-should-we-choose">Which <span class="math ">\(k\)</span> Should We Choose?</h1><img src="pca_reduced_how_much.png"></img></div><div class="step step-level-1" step="7" data-x="-20.0" data-y="-49.55" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="minimize-reconstruction-error">Minimize Reconstruction Error?</h1></div><div class="step step-level-1" step="8" data-x="-20.1" data-y="-49.5" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><p class="substep">(Eckart-Young-Mirsky) For any <span class="math ">\(k\)</span></p><div class="math-block substep">$$\begin{align}\min_{W, V} \left| X - W_k V_k^T \right|_2^2  = \sum_{i = k + 1}^m\left[ \sigma_i^2\right].\end{align}$$</div></div><div class="step step-level-1" step="9" data-x="-20.200000000000003" data-y="-49.44" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="explained_var_ratio.png" height="500px"></img></div><div class="step step-level-1" step="10" data-x="-20.300000000000004" data-y="-49.39" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><img src="over_the_top.png"></img></div><div class="step step-level-1" step="11" data-x="-20.300000000000004" data-y="-49.29" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="find-knee-elbow">Find Knee/Elbow?</h1><img src="explained_var_ratio_kneed.png" height="500px"></img></div><div class="step step-level-1" step="12" data-x="-20.300000000000004" data-y="-49.19" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001" data-z="0"><h1 id="utilize-eigenvalue-properties">Utilize Eigenvalue Properties?</h1><img src="explained_var_ratio_kaiser.png" height="500px"></img></div><div class="step step-level-1" step="13" data-x="-20.370000000000005" data-y="-49.26" data-z="-0.07" data-rotate-y="-90" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="drawbacks">Drawbacks</h1><ul><li>Not very well established theoretically</li><li>Known to be problematic in some cases</li></ul></div><div class="step step-level-1" step="14" data-x="-20.370000000000005" data-y="-49.26" data-rotate-y="0" data-scale="100.0001" data-rotate-x="0" data-rotate-z="0" data-z="-0.14"><h1 id="id1">Outline</h1><ul><li>Introduction</li><li><span class="underline">MDL</span></li><li>MDL Reduction for PCA</li><li>Results and Conclusions</li></ul></div><div class="step step-level-1" step="15" data-x="-20.670000000000005" data-y="-48.96" data-scale="0.0001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="-0.21000000000000002"><h1 id="minimum-description-length-principle">Minimum Description Length Principle</h1><img src="Occam.png" height="200px" class="center"></img><p>The best hypothesis for a given set of data, is the one that leads to the best compression of the data (Rissanen).</p></div><div class="step step-level-1" step="16" data-x="-20.670000000000005" data-y="-48.81" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="0.0001" data-z="-0.28"><h1 id="application-to-pca-cardinality">Application to PCA Cardinality</h1><img src="sender_receiver.png"></img></div><div class="step step-level-1" step="17" data-x="-20.640000000000004" data-y="-48.760000000000005" data-z="-0.23000000000000004" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><img src="pca_delta.png"></img></div><div class="step step-level-1" step="18" data-x="-20.640000000000004" data-y="-48.730000000000004" data-z="-0.18000000000000005" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><img src="sender_receiver_improved.png"></img></div><div class="step step-level-1" step="19" data-x="-20.640000000000004" data-y="-48.43000000000001" data-z="-0.18000000000000005" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="inherent-tradeoff">Inherent Tradeoff</h1><img src="pca_reduced_1_comm.png"></img></div><div class="step step-level-1" step="20" data-x="-20.440000000000005" data-y="-48.43000000000001" data-z="-0.18000000000000005" data-rotate-y="45" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><img src="pca_reduced_2_comm.png"></img></div><div class="step step-level-1" step="21" data-x="-20.640000000000004" data-y="-47.43000000000001" data-z="-0.18000000000000005" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="how-to-find-the-description-length">How to Find the Description Length?</h1><ul><li>Use Information Theory (Shannon) and Complexity Theory (Kolmogorov).</li><li>If we can impose a density <span class="math ">\(f(X)\)</span> on an R.V. <span class="math ">\(X\)</span>, then <span class="math ">\(-\log\left(f(X)\right)\)</span> is (effectively) the answer.</li><li>Description length problem &rarr; distribution problem.</li></ul></div><div class="step step-level-1" step="22" data-x="-20.640000000000004" data-y="-47.400000000000006" data-z="-0.13000000000000006" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><ul><li>But how should we choose a distribution for a model?</li></ul></div><div class="step step-level-1" step="23" data-x="-20.640000000000004" data-y="-47.25000000000001" data-z="-0.13000000000000006" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="normalized-maximum-likelihood">Normalized Maximum Likelihood</h1><ul><li>Let <span class="math ">\(X\)</span> be distributed according to a model parameterized by <span class="math ">\(\Phi\)</span>.</li><li>Let <span class="math ">\(\hat{\Phi}\left(X\right)\)</span> be the ML (maximum likelihood) estimate of <span class="math ">\(\Phi\)</span> for <span class="math ">\(X\)</span>.</li></ul></div><div class="step step-level-1" step="24" data-x="-20.640000000000004" data-y="-47.20000000000001" data-z="-0.08000000000000006" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><ul><li>The NML (normalized maximum likelihood) distribution is<div class="math-block ">$$\begin{align}{f\left( X \;;\; \hat{\Phi}\left(X\right)\right) \over \int f\left( Y \;;\; \hat{\Phi}\left(Y\right)\right) dY }\end{align}$$</div></li><li>This is the "modern form" of MDL (Shtarkov, Rissanen)<ul><li>Prequential (Dawid) optimality properties</li></ul></li></ul></div><div class="step step-level-1" step="25" data-x="-20.640000000000004" data-y="-47.14800000000001" data-z="-0.030000000000000054" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><p>Inutitively, <span class="math ">\(f\left( X \;;\; \hat{\Phi}\left(X\right)\right)\)</span> an immediate choice,
but self-referential, cannot be used for transmission</p><blockquote><img src="self_referential.png"></img></blockquote></div><div class="step step-level-1" step="26" data-x="-20.640000000000004" data-y="-47.09600000000001" data-z="0.01999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><div class="math-block ">$$\begin{align}{f\left( X \;;\; \hat{\Phi}\left(X\right)\right) \over \int f\left( Y \;;\; \hat{\Phi}\left(Y\right)\right) dY }\end{align}$$</div><p>also has a pleasing bias-variance tradeoff.</p></div><div class="step step-level-1" step="27" data-x="-20.640000000000004" data-y="-46.94600000000001" data-z="0.01999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="pca-as-a-generative-model">PCA As a Generative Model</h1><blockquote><img src="plate1.png" height="400px" class="center"></img></blockquote><p><span class="math ">\(\Phi \equiv k, \Upsilon\)</span></p></div><div class="step step-level-1" step="28" data-x="-40.64" data-y="-96.94600000000001" data-scale="0.01" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0.01999999999999995"><h1 id="id2">Outline</h1><ul><li>Introduction</li><li>MDL</li><li><span class="underline">MDL Reduction for PCA</span></li><li>Results and Conclusions</li></ul></div><div class="step step-level-1" step="29" data-x="-40.64" data-y="-96.94600000000001" data-scale="0.0001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0.01999999999999995"><h1 id="mathematical-difficulty-of-calculating-the-nml">Mathematical Difficulty of Calculating the NML</h1><img src="plate1.png" height="400px" class="center"></img><div class="math-block ">$$\begin{align}{f\left( X \;;\; \hat{k}\left(X\right), \hat{\Upsilon}\left(X\right)\right) \over \int f\left( Y \;;\; \hat{k}\left(Y\right), \hat{\Upsilon}\left(Y\right)\right) dY }\end{align}$$</div></div><div class="step step-level-1" step="30" data-x="-40.64" data-y="-96.894" data-z="0.06999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><ul><li>Numerator easy (Eckart-Young-Mirsky)</li><li>Denominator very difficult</li></ul></div><div class="step step-level-1" step="31" data-x="-40.64" data-y="-96.744" data-z="0.06999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="comparison-with-linear-regression">Comparison with Linear Regression</h1><img src="generative_pca_linreg.png" height="400px" class="center"></img><ul><li>Right process has a few more fixed parameters</li><li>Sufficient for analytical solution for its denominator (Rissanen)</li></ul></div><div class="step step-level-1" step="32" data-x="-40.64" data-y="-96.594" data-z="0.06999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><h1 id="eliminating-denominator-terms">Eliminating Denominator Terms</h1><p>For a parameter set <span class="math ">\(\Phi = A \bigcup \{b\}\)</span>, consider</p><img src="opt_by.png" height="400px" class="center"></img><div class="math-block ">$$\begin{align}\int f\left( X \;;\; \hat{A}\left(X\right), \hat{b}\left(X\right)\right) dX\end{align}$$</div></div><div class="step step-level-1" step="33" data-x="-40.77" data-y="-96.464" data-z="0.06999999999999995" data-rotate-y="0" data-rotate-z="0" data-rotate-x="0" data-scale="0.0001"><h1 id="lower-bound">Lower Bound</h1><img src="lower_bound.png"></img><div class="math-block ">$$\begin{align}b = \arg \min_{b'} f\left( X \;;\; \hat{A}\left(X\right), b'\right)\end{align}$$</div></div><div class="step step-level-1" step="34" data-x="-40.510000000000005" data-y="-96.724" data-z="0.06999999999999995" data-rotate-y="0" data-rotate-z="0" data-rotate-x="0" data-scale="0.0001"><h1 id="upper-bound">Upper Bound</h1><img src="upper_bound.png"></img></div><div class="step step-level-1" step="35" data-x="-40.510000000000005" data-y="-96.524" data-z="0.21999999999999995" data-rotate-y="0" data-rotate-x="0" data-rotate-z="0" data-scale="0.0001"><p>Bounded by expressions of the form</p><blockquote><div class="math-block ">$$\begin{align}\int f\left( X \;;\; \hat{A}\left(X\right), b\right) dX\end{align}$$</div></blockquote><p>(for fixed <span class="math ">\(b\)</span>) ~ linear regression</p></div><div class="step step-level-1" step="36" data-x="-40.61000000000001" data-y="-96.304" data-z="0.21999999999999995" data-rotate-y="0" data-rotate-z="0" data-rotate-x="0" data-scale="0.0001"><h1 id="reducing-pca-to-linear-regression">Reducing PCA to Linear Regression</h1><img src="plate3.png" height="450px"></img><p>Most analysis involves properties of quantized unary matrices.</p></div><div class="step step-level-1" step="37" data-x="20000" data-y="70000" data-rotate-y="0" data-scale="100.0001" data-rotate-x="0" data-rotate-z="0" data-z="0.21999999999999995"><h1 id="id3">Outline</h1><ul><li>Introduction</li><li>MDL</li><li>MDL Reduction for PCA</li><li><span class="underline">Results and Conclusions</span></li></ul></div><div class="step step-level-1" step="38" data-x="70000" data-y="160000" data-rotate-y="0" data-scale="101.0001" data-rotate-x="0" data-rotate-z="0" data-z="0.21999999999999995"><h1 id="main-result">Main Result</h1><p>Let <span class="math ">\(s\left(X \;;\; k\right)\)</span> be the stochastic complexity of a <span class="math ">\(k\)</span>-dimensional PCA reduction of <span class="math ">\(X\)</span>.</p><div class="math-block ">$$\begin{align}s(X \;;\; k)
\simeq
\left( nm - kn \right) \ln\left(\sum_{i = k + 1}\left[\lambda_i^2\right]\right)
+ nk \ln\left( \left| X^T X\right|_2^2 \right)
+
\\
(mn - kn - 1) \ln\left( mn \over mn - kn \right)
-
(nk + 1) \ln\left(nk\right)
+ \Delta s
,\end{align}$$</div><p>where</p><div class="math-block ">$$\begin{align}0 &amp;\leq \Delta s \leq mk \ln\left(2 \over m \epsilon\right)
.\end{align}$$</div></div><div class="step step-level-1" step="39" data-x="70000" data-y="260000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="102.0001" data-z="0.21999999999999995"><h1 id="numerical-experiments">Numerical Experiments</h1><ul><li>Historical Dow-Jones Industrial Average</li><li>Sets:<ul><li>Orig - all 30 assets</li><li>Lin10 - 10 assets, 20 random linear combination + noise</li><li>Lin50 - 5 assets, 25 random linear combination + noise</li></ul></li></ul></div><div class="step step-level-1" step="40" data-x="70000" data-y="360000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="103.0001" data-z="0.21999999999999995"><h1 id="explained-variance">Explained Variance</h1><img src="explained_variance.png" height="500px" class="center"></img></div><div class="step step-level-1" step="41" data-x="200000" data-y="360000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="104.0001" data-z="0.21999999999999995"><h1 id="knee-results">Knee Results</h1><img src="knee_sim.png" height="500px" class="center"></img></div><div class="step step-level-1" step="42" data-x="330000" data-y="360000" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-scale="105.0001" data-z="0.21999999999999995"><h1 id="mdl-results">MDL Results</h1><img src="mdl_sim.png" height="500px" class="center"></img></div><div class="step step-level-1" step="43" data-x="780000" data-y="710000" data-scale="1105.0001" data-rotate-x="0" data-rotate-y="0" data-rotate-z="0" data-z="0.21999999999999995"><h1 id="main-points">Main Points</h1><ul><li>MDL - approach to unsupervised model selection<ul><li>Theoretically justified</li></ul></li><li>NML is difficult to calculate<ul><li>NML for embeddings, e.g.</li></ul></li><li>Saw a technique for NML reduction</li><li>Applied technique to PCA</li></ul></div></div><div id="hovercraft-help"><table><tr><th>Space</th><td>Forward</td></tr><tr><th>Right, Down, Page Down</th><td>Next slide</td></tr><tr><th>Left, Up, Page Up</th><td>Previous slide</td></tr><tr><th>G</th><td>Go to slide number</td></tr><tr><th>P</th><td>Open presenter console</td></tr><tr><th>H</th><td>Toggle this help</td></tr></table></div><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/impressConsole.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>